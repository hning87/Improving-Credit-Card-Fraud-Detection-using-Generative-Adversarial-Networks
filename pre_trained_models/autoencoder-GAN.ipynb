{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "import os\n",
    "from keras.layers import Input, Embedding, multiply, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, g_model, d_model):\n",
    "        self.z = latent_dim\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.generator = g_model\n",
    "        self.discriminator = d_model\n",
    "\n",
    "        self.train_G = train_G(self.generator, self.discriminator)\n",
    "        self.loss_D, self.loss_G = [], []\n",
    "\n",
    "    def train(self, data, batch_size=128, steps_per_epoch=100):\n",
    "\n",
    "        for epoch in range(steps_per_epoch):\n",
    "            # Select a random batch of transactions data\n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            real_data = data[idx]\n",
    "\n",
    "            # generate a batch of new data\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            fake_data = self.generator.predict(noise)\n",
    "\n",
    "            # Train D\n",
    "            loss_real = self.discriminator.train_on_batch(real_data, np.ones(batch_size))\n",
    "            loss_fake = self.discriminator.train_on_batch(fake_data, np.zeros(batch_size))\n",
    "            self.loss_D.append(0.5 * np.add(loss_fake, loss_real))\n",
    "\n",
    "            # Train G\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            loss_G = self.train_G.train_on_batch(noise, np.ones(batch_size))\n",
    "            self.loss_G.append(loss_G)\n",
    "\n",
    "            if (epoch + 1) * 10 % steps_per_epoch == 0:\n",
    "                print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f, acc: %.2f%%] [Loss_G: %f]' %\n",
    "                  (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], 100*self.loss_D[-1][1], loss_G))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "df_raw['Amount'] = np.log10(df_raw['Amount'].values + 1)\n",
    "df_raw['Time'] = (df_raw['Time'].values/3600)\n",
    "df_fraud = df_raw[df_raw['Class'] == 1]\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_raw, train_size=0.8, random_state=42, stratify=df_raw[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "x_train = df_train.drop(target, axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target, axis=1)\n",
    "y_test = df_test[target]\n",
    "\n",
    "\n",
    "# %% --------------------------------------- Set Seeds -----------------------------------------------------------------\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "weight_init = glorot_normal(seed=SEED)\n",
    "\n",
    "# %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n",
    "latent_dim = 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.1, beta_2=0.9)\n",
    "trainRatio = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 227845 samples, validate on 56962 samples\n",
      "Epoch 1/30\n",
      "227845/227845 [==============================] - 5s 24us/step - loss: 0.4911 - val_loss: 0.3555\n",
      "Epoch 2/30\n",
      "227845/227845 [==============================] - 5s 24us/step - loss: 0.3011 - val_loss: 0.2957\n",
      "Epoch 3/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.2364 - val_loss: 0.2083\n",
      "Epoch 4/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.2008 - val_loss: 0.2059\n",
      "Epoch 5/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1709 - val_loss: 0.2165\n",
      "Epoch 6/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.1506 - val_loss: 0.1623\n",
      "Epoch 7/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.1391 - val_loss: 0.1694\n",
      "Epoch 8/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1315 - val_loss: 0.1182\n",
      "Epoch 9/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1257 - val_loss: 0.1504\n",
      "Epoch 10/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1207 - val_loss: 0.1356\n",
      "Epoch 11/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1165 - val_loss: 0.1348\n",
      "Epoch 12/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.1127 - val_loss: 0.1583\n",
      "Epoch 13/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.1094 - val_loss: 0.1095\n",
      "Epoch 14/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.1063 - val_loss: 0.1170\n",
      "Epoch 15/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.1029 - val_loss: 0.0998\n",
      "Epoch 16/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0998 - val_loss: 0.1009\n",
      "Epoch 17/30\n",
      "227845/227845 [==============================] - 5s 24us/step - loss: 0.0973 - val_loss: 0.1263\n",
      "Epoch 18/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0950 - val_loss: 0.0932\n",
      "Epoch 19/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0926 - val_loss: 0.1022\n",
      "Epoch 20/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0907 - val_loss: 0.1026\n",
      "Epoch 21/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0889 - val_loss: 0.1419\n",
      "Epoch 22/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0872 - val_loss: 0.1218\n",
      "Epoch 23/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0857 - val_loss: 0.1171\n",
      "Epoch 24/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0842 - val_loss: 0.0881\n",
      "Epoch 25/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0828 - val_loss: 0.0889\n",
      "Epoch 26/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0815 - val_loss: 0.0859\n",
      "Epoch 27/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0803 - val_loss: 0.0837\n",
      "Epoch 28/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.0790 - val_loss: 0.0693\n",
      "Epoch 29/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.0778 - val_loss: 0.0843\n",
      "Epoch 30/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0767 - val_loss: 0.0827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e638fab2e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Encoder\n",
    "def encoder():\n",
    "    data = Input(shape=(data_dim,))\n",
    "    x = Dense(256, kernel_initializer=weight_init)(data)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    #    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    #    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = Dense(64, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    #    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    encodered = Dense(latent_dim)(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=data, outputs=encodered)\n",
    "    return model\n",
    "\n",
    "def decoder():\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "\n",
    "    x = Dense(64, kernel_initializer=weight_init)(noise)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Dense(256, kernel_initializer=weight_init)(x)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    # tanh is removed since we are not dealing with normalized image data\n",
    "    generated = Dense(data_dim, kernel_initializer=weight_init)(x)\n",
    "\n",
    "    generator = Model(inputs=noise, outputs=generated)\n",
    "    return generator\n",
    "\n",
    "# Build Autoencoder\n",
    "def train_AE(encoder, decoder):\n",
    "    feature = Input(shape=(data_dim,))\n",
    "    latent = encoder(feature)\n",
    "    rec_feature = decoder(latent)\n",
    "    model = Model(feature, rec_feature)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    return model\n",
    "\n",
    "# Train Autoencoder\n",
    "en = encoder()\n",
    "de = decoder()\n",
    "ae = train_AE(en, de)\n",
    "\n",
    "ae.fit(x_train, x_train,\n",
    "       epochs=30,\n",
    "       batch_size=128,\n",
    "       shuffle=True,\n",
    "       validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_fraud, train_size=0.8, random_state=42, stratify=df_fraud[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "x_train = df_train.drop(target, axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target, axis=1)\n",
    "y_test = df_test[target]\n",
    "\n",
    "latent_dim = 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.1, beta_2=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 32)                51168     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 51,201\n",
      "Trainable params: 51,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 30)                51166     \n",
      "=================================================================\n",
      "Total params: 51,166\n",
      "Trainable params: 51,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "EPOCH #  1 --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (10 / 100): [Loss_D_real: 1.854686, Loss_D_fake: 0.322197, acc: 60.55%] [Loss_G: 1.464881]\n",
      "Steps (20 / 100): [Loss_D_real: 0.689074, Loss_D_fake: 0.478994, acc: 78.52%] [Loss_G: 1.215435]\n",
      "Steps (30 / 100): [Loss_D_real: 0.404704, Loss_D_fake: 0.573806, acc: 77.73%] [Loss_G: 1.022084]\n",
      "Steps (40 / 100): [Loss_D_real: 0.204999, Loss_D_fake: 0.592633, acc: 79.69%] [Loss_G: 1.057282]\n",
      "Steps (50 / 100): [Loss_D_real: 0.230878, Loss_D_fake: 0.564651, acc: 79.69%] [Loss_G: 1.117832]\n",
      "Steps (60 / 100): [Loss_D_real: 0.313420, Loss_D_fake: 0.513291, acc: 83.59%] [Loss_G: 1.095918]\n",
      "Steps (70 / 100): [Loss_D_real: 0.400042, Loss_D_fake: 0.463009, acc: 83.20%] [Loss_G: 1.316952]\n",
      "Steps (80 / 100): [Loss_D_real: 0.387818, Loss_D_fake: 0.454032, acc: 83.98%] [Loss_G: 1.331147]\n",
      "Steps (90 / 100): [Loss_D_real: 0.431270, Loss_D_fake: 0.528597, acc: 82.42%] [Loss_G: 1.126544]\n",
      "Steps (100 / 100): [Loss_D_real: 0.431223, Loss_D_fake: 0.621392, acc: 73.83%] [Loss_G: 1.063085]\n",
      "EPOCH #  2 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.480855, Loss_D_fake: 0.656504, acc: 74.61%] [Loss_G: 0.934815]\n",
      "Steps (20 / 100): [Loss_D_real: 0.574273, Loss_D_fake: 0.698775, acc: 62.89%] [Loss_G: 0.946818]\n",
      "Steps (30 / 100): [Loss_D_real: 0.526741, Loss_D_fake: 0.664299, acc: 69.14%] [Loss_G: 0.840181]\n",
      "Steps (40 / 100): [Loss_D_real: 0.662767, Loss_D_fake: 0.696485, acc: 62.89%] [Loss_G: 0.828028]\n",
      "Steps (50 / 100): [Loss_D_real: 0.554833, Loss_D_fake: 0.726489, acc: 61.33%] [Loss_G: 0.782766]\n",
      "Steps (60 / 100): [Loss_D_real: 0.570436, Loss_D_fake: 0.780664, acc: 55.47%] [Loss_G: 0.762307]\n",
      "Steps (70 / 100): [Loss_D_real: 0.547199, Loss_D_fake: 0.767559, acc: 57.03%] [Loss_G: 0.755035]\n",
      "Steps (80 / 100): [Loss_D_real: 0.664404, Loss_D_fake: 0.817505, acc: 49.22%] [Loss_G: 0.769882]\n",
      "Steps (90 / 100): [Loss_D_real: 0.776118, Loss_D_fake: 0.793837, acc: 44.14%] [Loss_G: 0.712957]\n",
      "Steps (100 / 100): [Loss_D_real: 0.700213, Loss_D_fake: 0.823913, acc: 46.88%] [Loss_G: 0.733293]\n",
      "EPOCH #  3 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.730998, Loss_D_fake: 0.837253, acc: 38.67%] [Loss_G: 0.745497]\n",
      "Steps (20 / 100): [Loss_D_real: 0.755999, Loss_D_fake: 0.791342, acc: 42.97%] [Loss_G: 0.749745]\n",
      "Steps (30 / 100): [Loss_D_real: 0.838758, Loss_D_fake: 0.801048, acc: 37.50%] [Loss_G: 0.755547]\n",
      "Steps (40 / 100): [Loss_D_real: 0.758647, Loss_D_fake: 0.807530, acc: 40.23%] [Loss_G: 0.755489]\n",
      "Steps (50 / 100): [Loss_D_real: 0.779037, Loss_D_fake: 0.787550, acc: 39.84%] [Loss_G: 0.790228]\n",
      "Steps (60 / 100): [Loss_D_real: 0.783328, Loss_D_fake: 0.803630, acc: 34.77%] [Loss_G: 0.813116]\n",
      "Steps (70 / 100): [Loss_D_real: 0.793971, Loss_D_fake: 0.739004, acc: 37.50%] [Loss_G: 0.871122]\n",
      "Steps (80 / 100): [Loss_D_real: 0.806571, Loss_D_fake: 0.751234, acc: 29.30%] [Loss_G: 0.839270]\n",
      "Steps (90 / 100): [Loss_D_real: 0.820908, Loss_D_fake: 0.692585, acc: 42.58%] [Loss_G: 0.884432]\n",
      "Steps (100 / 100): [Loss_D_real: 0.799650, Loss_D_fake: 0.687844, acc: 41.41%] [Loss_G: 0.932693]\n",
      "EPOCH #  4 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.848790, Loss_D_fake: 0.725036, acc: 35.94%] [Loss_G: 0.898022]\n",
      "Steps (20 / 100): [Loss_D_real: 0.852451, Loss_D_fake: 0.737314, acc: 29.30%] [Loss_G: 0.876061]\n",
      "Steps (30 / 100): [Loss_D_real: 0.787709, Loss_D_fake: 0.704762, acc: 40.62%] [Loss_G: 0.887460]\n",
      "Steps (40 / 100): [Loss_D_real: 0.791786, Loss_D_fake: 0.657609, acc: 47.27%] [Loss_G: 0.933159]\n",
      "Steps (50 / 100): [Loss_D_real: 0.828878, Loss_D_fake: 0.710171, acc: 37.50%] [Loss_G: 0.897653]\n",
      "Steps (60 / 100): [Loss_D_real: 0.803347, Loss_D_fake: 0.660134, acc: 44.92%] [Loss_G: 0.940145]\n",
      "Steps (70 / 100): [Loss_D_real: 0.750838, Loss_D_fake: 0.624259, acc: 57.42%] [Loss_G: 0.993902]\n",
      "Steps (80 / 100): [Loss_D_real: 0.675980, Loss_D_fake: 0.571680, acc: 70.70%] [Loss_G: 1.135259]\n",
      "Steps (90 / 100): [Loss_D_real: 0.681173, Loss_D_fake: 0.590138, acc: 64.06%] [Loss_G: 1.058188]\n",
      "Steps (100 / 100): [Loss_D_real: 0.832044, Loss_D_fake: 0.768514, acc: 34.38%] [Loss_G: 0.888393]\n",
      "EPOCH #  5 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.853676, Loss_D_fake: 0.839376, acc: 28.91%] [Loss_G: 0.756542]\n",
      "Steps (20 / 100): [Loss_D_real: 0.753813, Loss_D_fake: 0.797096, acc: 32.81%] [Loss_G: 0.784061]\n",
      "Steps (30 / 100): [Loss_D_real: 0.743303, Loss_D_fake: 0.755781, acc: 40.23%] [Loss_G: 0.884917]\n",
      "Steps (40 / 100): [Loss_D_real: 0.722438, Loss_D_fake: 0.712047, acc: 48.05%] [Loss_G: 0.945411]\n",
      "Steps (50 / 100): [Loss_D_real: 0.714401, Loss_D_fake: 0.700483, acc: 50.00%] [Loss_G: 0.938817]\n",
      "Steps (60 / 100): [Loss_D_real: 0.725443, Loss_D_fake: 0.715139, acc: 48.83%] [Loss_G: 0.945810]\n",
      "Steps (70 / 100): [Loss_D_real: 0.693902, Loss_D_fake: 0.701026, acc: 51.56%] [Loss_G: 0.974155]\n",
      "Steps (80 / 100): [Loss_D_real: 0.681513, Loss_D_fake: 0.684301, acc: 53.12%] [Loss_G: 1.022557]\n",
      "Steps (90 / 100): [Loss_D_real: 0.709974, Loss_D_fake: 0.718179, acc: 53.12%] [Loss_G: 0.998103]\n",
      "Steps (100 / 100): [Loss_D_real: 0.671254, Loss_D_fake: 0.693972, acc: 57.81%] [Loss_G: 1.015799]\n",
      "EPOCH #  6 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.667185, Loss_D_fake: 0.790812, acc: 48.83%] [Loss_G: 0.865145]\n",
      "Steps (20 / 100): [Loss_D_real: 0.692198, Loss_D_fake: 0.868558, acc: 41.02%] [Loss_G: 0.781471]\n",
      "Steps (30 / 100): [Loss_D_real: 0.677930, Loss_D_fake: 0.789454, acc: 42.19%] [Loss_G: 0.817202]\n",
      "Steps (40 / 100): [Loss_D_real: 0.643583, Loss_D_fake: 0.806810, acc: 48.05%] [Loss_G: 0.838989]\n",
      "Steps (50 / 100): [Loss_D_real: 0.635726, Loss_D_fake: 0.787814, acc: 51.56%] [Loss_G: 0.857738]\n",
      "Steps (60 / 100): [Loss_D_real: 0.738121, Loss_D_fake: 0.912560, acc: 39.06%] [Loss_G: 0.779231]\n",
      "Steps (70 / 100): [Loss_D_real: 0.687815, Loss_D_fake: 0.826691, acc: 47.66%] [Loss_G: 0.834272]\n",
      "Steps (80 / 100): [Loss_D_real: 0.617321, Loss_D_fake: 0.705714, acc: 58.98%] [Loss_G: 0.942081]\n",
      "Steps (90 / 100): [Loss_D_real: 0.620806, Loss_D_fake: 0.660588, acc: 66.41%] [Loss_G: 0.954017]\n",
      "Steps (100 / 100): [Loss_D_real: 0.619629, Loss_D_fake: 0.705806, acc: 60.94%] [Loss_G: 0.907066]\n",
      "EPOCH #  7 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.673574, Loss_D_fake: 0.724578, acc: 53.52%] [Loss_G: 0.903534]\n",
      "Steps (20 / 100): [Loss_D_real: 0.690428, Loss_D_fake: 0.868061, acc: 44.53%] [Loss_G: 0.800766]\n",
      "Steps (30 / 100): [Loss_D_real: 0.649305, Loss_D_fake: 0.874092, acc: 40.62%] [Loss_G: 0.703718]\n",
      "Steps (40 / 100): [Loss_D_real: 0.592377, Loss_D_fake: 0.856039, acc: 38.67%] [Loss_G: 0.729543]\n",
      "Steps (50 / 100): [Loss_D_real: 0.681554, Loss_D_fake: 0.814015, acc: 34.38%] [Loss_G: 0.764354]\n",
      "Steps (60 / 100): [Loss_D_real: 0.630116, Loss_D_fake: 0.763624, acc: 49.22%] [Loss_G: 0.850474]\n",
      "Steps (70 / 100): [Loss_D_real: 0.640124, Loss_D_fake: 0.695461, acc: 59.77%] [Loss_G: 0.915615]\n",
      "Steps (80 / 100): [Loss_D_real: 0.595626, Loss_D_fake: 0.635863, acc: 64.84%] [Loss_G: 0.936207]\n",
      "Steps (90 / 100): [Loss_D_real: 0.629328, Loss_D_fake: 0.671573, acc: 62.11%] [Loss_G: 0.926751]\n",
      "Steps (100 / 100): [Loss_D_real: 0.616634, Loss_D_fake: 0.767931, acc: 49.61%] [Loss_G: 0.826165]\n",
      "EPOCH #  8 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.659925, Loss_D_fake: 0.882866, acc: 40.23%] [Loss_G: 0.760113]\n",
      "Steps (20 / 100): [Loss_D_real: 0.632006, Loss_D_fake: 0.800598, acc: 49.22%] [Loss_G: 0.732861]\n",
      "Steps (30 / 100): [Loss_D_real: 0.678212, Loss_D_fake: 0.808812, acc: 42.97%] [Loss_G: 0.794900]\n",
      "Steps (40 / 100): [Loss_D_real: 0.659421, Loss_D_fake: 0.765443, acc: 49.22%] [Loss_G: 0.839644]\n",
      "Steps (50 / 100): [Loss_D_real: 0.636904, Loss_D_fake: 0.746488, acc: 55.08%] [Loss_G: 0.827870]\n",
      "Steps (60 / 100): [Loss_D_real: 0.673349, Loss_D_fake: 0.784414, acc: 46.88%] [Loss_G: 0.802638]\n",
      "Steps (70 / 100): [Loss_D_real: 0.619229, Loss_D_fake: 0.733885, acc: 53.12%] [Loss_G: 0.824580]\n",
      "Steps (80 / 100): [Loss_D_real: 0.644947, Loss_D_fake: 0.737896, acc: 57.03%] [Loss_G: 0.875733]\n",
      "Steps (90 / 100): [Loss_D_real: 0.614115, Loss_D_fake: 0.686920, acc: 64.06%] [Loss_G: 0.860661]\n",
      "Steps (100 / 100): [Loss_D_real: 0.622248, Loss_D_fake: 0.658484, acc: 60.94%] [Loss_G: 0.907967]\n",
      "EPOCH #  9 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.576370, Loss_D_fake: 0.689348, acc: 61.33%] [Loss_G: 0.895490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (20 / 100): [Loss_D_real: 0.635499, Loss_D_fake: 0.701926, acc: 59.38%] [Loss_G: 0.860522]\n",
      "Steps (30 / 100): [Loss_D_real: 0.656951, Loss_D_fake: 0.766852, acc: 53.91%] [Loss_G: 0.803601]\n",
      "Steps (40 / 100): [Loss_D_real: 0.676729, Loss_D_fake: 0.791411, acc: 45.70%] [Loss_G: 0.840218]\n",
      "Steps (50 / 100): [Loss_D_real: 0.678889, Loss_D_fake: 0.704970, acc: 55.08%] [Loss_G: 0.827372]\n",
      "Steps (60 / 100): [Loss_D_real: 0.675134, Loss_D_fake: 0.669629, acc: 62.50%] [Loss_G: 0.920371]\n",
      "Steps (70 / 100): [Loss_D_real: 0.692258, Loss_D_fake: 0.656257, acc: 57.03%] [Loss_G: 0.919382]\n",
      "Steps (80 / 100): [Loss_D_real: 0.660077, Loss_D_fake: 0.690421, acc: 56.64%] [Loss_G: 0.894136]\n",
      "Steps (90 / 100): [Loss_D_real: 0.692053, Loss_D_fake: 0.775650, acc: 49.61%] [Loss_G: 0.806598]\n",
      "Steps (100 / 100): [Loss_D_real: 0.769649, Loss_D_fake: 0.799149, acc: 42.58%] [Loss_G: 0.767125]\n",
      "EPOCH #  10 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.686837, Loss_D_fake: 0.786363, acc: 44.53%] [Loss_G: 0.775573]\n",
      "Steps (20 / 100): [Loss_D_real: 0.675308, Loss_D_fake: 0.684043, acc: 56.25%] [Loss_G: 0.870984]\n",
      "Steps (30 / 100): [Loss_D_real: 0.684124, Loss_D_fake: 0.652246, acc: 60.55%] [Loss_G: 0.958061]\n",
      "Steps (40 / 100): [Loss_D_real: 0.735189, Loss_D_fake: 0.733634, acc: 47.66%] [Loss_G: 0.869592]\n",
      "Steps (50 / 100): [Loss_D_real: 0.779776, Loss_D_fake: 0.825268, acc: 40.23%] [Loss_G: 0.863068]\n",
      "Steps (60 / 100): [Loss_D_real: 0.791160, Loss_D_fake: 0.750160, acc: 39.06%] [Loss_G: 0.807976]\n",
      "Steps (70 / 100): [Loss_D_real: 0.760636, Loss_D_fake: 0.714570, acc: 47.27%] [Loss_G: 0.856380]\n",
      "Steps (80 / 100): [Loss_D_real: 0.738171, Loss_D_fake: 0.695150, acc: 53.52%] [Loss_G: 0.896773]\n",
      "Steps (90 / 100): [Loss_D_real: 0.668511, Loss_D_fake: 0.760310, acc: 54.30%] [Loss_G: 0.819776]\n",
      "Steps (100 / 100): [Loss_D_real: 0.757757, Loss_D_fake: 0.792322, acc: 42.97%] [Loss_G: 0.786461]\n",
      "EPOCH #  11 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.755858, Loss_D_fake: 0.804773, acc: 42.97%] [Loss_G: 0.805411]\n",
      "Steps (20 / 100): [Loss_D_real: 0.759372, Loss_D_fake: 0.743687, acc: 46.09%] [Loss_G: 0.823525]\n",
      "Steps (30 / 100): [Loss_D_real: 0.788321, Loss_D_fake: 0.761660, acc: 40.23%] [Loss_G: 0.851681]\n",
      "Steps (40 / 100): [Loss_D_real: 0.781000, Loss_D_fake: 0.752930, acc: 40.23%] [Loss_G: 0.855036]\n",
      "Steps (50 / 100): [Loss_D_real: 0.807666, Loss_D_fake: 0.720415, acc: 39.06%] [Loss_G: 0.865360]\n",
      "Steps (60 / 100): [Loss_D_real: 0.750904, Loss_D_fake: 0.690643, acc: 46.88%] [Loss_G: 0.850496]\n",
      "Steps (70 / 100): [Loss_D_real: 0.755908, Loss_D_fake: 0.746198, acc: 41.41%] [Loss_G: 0.887836]\n",
      "Steps (80 / 100): [Loss_D_real: 0.845506, Loss_D_fake: 0.724066, acc: 38.28%] [Loss_G: 0.873994]\n",
      "Steps (90 / 100): [Loss_D_real: 0.861204, Loss_D_fake: 0.739339, acc: 37.11%] [Loss_G: 0.862976]\n",
      "Steps (100 / 100): [Loss_D_real: 0.815640, Loss_D_fake: 0.643433, acc: 50.00%] [Loss_G: 0.905504]\n",
      "EPOCH #  12 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.809482, Loss_D_fake: 0.693593, acc: 45.70%] [Loss_G: 0.860729]\n",
      "Steps (20 / 100): [Loss_D_real: 0.881441, Loss_D_fake: 0.804077, acc: 25.78%] [Loss_G: 0.819349]\n",
      "Steps (30 / 100): [Loss_D_real: 0.863323, Loss_D_fake: 0.709667, acc: 32.81%] [Loss_G: 0.872023]\n",
      "Steps (40 / 100): [Loss_D_real: 0.823514, Loss_D_fake: 0.650454, acc: 42.19%] [Loss_G: 0.970003]\n",
      "Steps (50 / 100): [Loss_D_real: 0.787996, Loss_D_fake: 0.676419, acc: 46.48%] [Loss_G: 0.877592]\n",
      "Steps (60 / 100): [Loss_D_real: 0.913990, Loss_D_fake: 0.716814, acc: 32.42%] [Loss_G: 0.878937]\n",
      "Steps (70 / 100): [Loss_D_real: 0.857856, Loss_D_fake: 0.696922, acc: 37.89%] [Loss_G: 0.891424]\n",
      "Steps (80 / 100): [Loss_D_real: 0.832396, Loss_D_fake: 0.697945, acc: 40.23%] [Loss_G: 0.952772]\n",
      "Steps (90 / 100): [Loss_D_real: 0.802029, Loss_D_fake: 0.708296, acc: 40.23%] [Loss_G: 0.885381]\n",
      "Steps (100 / 100): [Loss_D_real: 0.861122, Loss_D_fake: 0.721344, acc: 29.30%] [Loss_G: 0.834062]\n",
      "EPOCH #  13 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.857763, Loss_D_fake: 0.682186, acc: 38.28%] [Loss_G: 0.892204]\n",
      "Steps (20 / 100): [Loss_D_real: 0.868015, Loss_D_fake: 0.647055, acc: 39.84%] [Loss_G: 0.935522]\n",
      "Steps (30 / 100): [Loss_D_real: 0.856323, Loss_D_fake: 0.642235, acc: 41.80%] [Loss_G: 0.916057]\n",
      "Steps (40 / 100): [Loss_D_real: 0.866454, Loss_D_fake: 0.710568, acc: 33.20%] [Loss_G: 0.855385]\n",
      "Steps (50 / 100): [Loss_D_real: 0.821381, Loss_D_fake: 0.678482, acc: 36.72%] [Loss_G: 0.908708]\n",
      "Steps (60 / 100): [Loss_D_real: 0.762455, Loss_D_fake: 0.731408, acc: 35.55%] [Loss_G: 0.867204]\n",
      "Steps (70 / 100): [Loss_D_real: 0.839462, Loss_D_fake: 0.676199, acc: 35.94%] [Loss_G: 0.882078]\n",
      "Steps (80 / 100): [Loss_D_real: 0.834298, Loss_D_fake: 0.678120, acc: 36.33%] [Loss_G: 0.860855]\n",
      "Steps (90 / 100): [Loss_D_real: 0.822724, Loss_D_fake: 0.638058, acc: 48.05%] [Loss_G: 0.916921]\n",
      "Steps (100 / 100): [Loss_D_real: 0.814054, Loss_D_fake: 0.617547, acc: 50.39%] [Loss_G: 0.959348]\n",
      "EPOCH #  14 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.822078, Loss_D_fake: 0.709029, acc: 38.28%] [Loss_G: 0.874399]\n",
      "Steps (20 / 100): [Loss_D_real: 0.827108, Loss_D_fake: 0.721003, acc: 33.20%] [Loss_G: 0.858675]\n",
      "Steps (30 / 100): [Loss_D_real: 0.784913, Loss_D_fake: 0.651810, acc: 41.80%] [Loss_G: 0.920951]\n",
      "Steps (40 / 100): [Loss_D_real: 0.793054, Loss_D_fake: 0.673423, acc: 41.80%] [Loss_G: 0.897360]\n",
      "Steps (50 / 100): [Loss_D_real: 0.809923, Loss_D_fake: 0.693798, acc: 37.50%] [Loss_G: 0.869655]\n",
      "Steps (60 / 100): [Loss_D_real: 0.770966, Loss_D_fake: 0.734911, acc: 32.03%] [Loss_G: 0.868110]\n",
      "Steps (70 / 100): [Loss_D_real: 0.835180, Loss_D_fake: 0.687950, acc: 37.89%] [Loss_G: 0.829857]\n",
      "Steps (80 / 100): [Loss_D_real: 0.780090, Loss_D_fake: 0.692183, acc: 40.23%] [Loss_G: 0.851772]\n",
      "Steps (90 / 100): [Loss_D_real: 0.743477, Loss_D_fake: 0.664406, acc: 49.61%] [Loss_G: 0.867198]\n",
      "Steps (100 / 100): [Loss_D_real: 0.791470, Loss_D_fake: 0.737402, acc: 31.25%] [Loss_G: 0.859264]\n",
      "EPOCH #  15 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.735568, Loss_D_fake: 0.711085, acc: 44.92%] [Loss_G: 0.831993]\n",
      "Steps (20 / 100): [Loss_D_real: 0.736324, Loss_D_fake: 0.643546, acc: 52.34%] [Loss_G: 0.894171]\n",
      "Steps (30 / 100): [Loss_D_real: 0.741929, Loss_D_fake: 0.716514, acc: 44.92%] [Loss_G: 0.838458]\n",
      "Steps (40 / 100): [Loss_D_real: 0.803626, Loss_D_fake: 0.684261, acc: 41.80%] [Loss_G: 0.814537]\n",
      "Steps (50 / 100): [Loss_D_real: 0.860123, Loss_D_fake: 0.707151, acc: 37.11%] [Loss_G: 0.854173]\n",
      "Steps (60 / 100): [Loss_D_real: 0.853040, Loss_D_fake: 0.691493, acc: 35.94%] [Loss_G: 0.876026]\n",
      "Steps (70 / 100): [Loss_D_real: 0.798008, Loss_D_fake: 0.646343, acc: 42.97%] [Loss_G: 0.870008]\n",
      "Steps (80 / 100): [Loss_D_real: 0.748923, Loss_D_fake: 0.644101, acc: 47.66%] [Loss_G: 0.884674]\n",
      "Steps (90 / 100): [Loss_D_real: 0.708678, Loss_D_fake: 0.678989, acc: 48.44%] [Loss_G: 0.847508]\n",
      "Steps (100 / 100): [Loss_D_real: 0.727521, Loss_D_fake: 0.734187, acc: 38.67%] [Loss_G: 0.801263]\n",
      "EPOCH #  16 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.757186, Loss_D_fake: 0.762406, acc: 35.55%] [Loss_G: 0.795807]\n",
      "Steps (20 / 100): [Loss_D_real: 0.755677, Loss_D_fake: 0.753434, acc: 33.59%] [Loss_G: 0.793462]\n",
      "Steps (30 / 100): [Loss_D_real: 0.737256, Loss_D_fake: 0.660558, acc: 52.34%] [Loss_G: 0.863510]\n",
      "Steps (40 / 100): [Loss_D_real: 0.745548, Loss_D_fake: 0.673625, acc: 41.02%] [Loss_G: 0.864934]\n",
      "Steps (50 / 100): [Loss_D_real: 0.796904, Loss_D_fake: 0.668332, acc: 44.53%] [Loss_G: 0.916118]\n",
      "Steps (60 / 100): [Loss_D_real: 0.750243, Loss_D_fake: 0.697071, acc: 37.50%] [Loss_G: 0.861467]\n",
      "Steps (70 / 100): [Loss_D_real: 0.720554, Loss_D_fake: 0.671085, acc: 44.92%] [Loss_G: 0.845733]\n",
      "Steps (80 / 100): [Loss_D_real: 0.722565, Loss_D_fake: 0.687681, acc: 47.66%] [Loss_G: 0.858784]\n",
      "Steps (90 / 100): [Loss_D_real: 0.772454, Loss_D_fake: 0.698042, acc: 36.72%] [Loss_G: 0.805563]\n",
      "Steps (100 / 100): [Loss_D_real: 0.713784, Loss_D_fake: 0.705323, acc: 42.19%] [Loss_G: 0.813527]\n",
      "EPOCH #  17 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.737864, Loss_D_fake: 0.704998, acc: 45.70%] [Loss_G: 0.841566]\n",
      "Steps (20 / 100): [Loss_D_real: 0.747791, Loss_D_fake: 0.684978, acc: 46.88%] [Loss_G: 0.832070]\n",
      "Steps (30 / 100): [Loss_D_real: 0.722303, Loss_D_fake: 0.707437, acc: 39.45%] [Loss_G: 0.825197]\n",
      "Steps (40 / 100): [Loss_D_real: 0.751006, Loss_D_fake: 0.647340, acc: 50.39%] [Loss_G: 0.883450]\n",
      "Steps (50 / 100): [Loss_D_real: 0.681653, Loss_D_fake: 0.648672, acc: 55.86%] [Loss_G: 0.864352]\n",
      "Steps (60 / 100): [Loss_D_real: 0.706286, Loss_D_fake: 0.762422, acc: 41.02%] [Loss_G: 0.772904]\n",
      "Steps (70 / 100): [Loss_D_real: 0.718140, Loss_D_fake: 0.733680, acc: 39.06%] [Loss_G: 0.784136]\n",
      "Steps (80 / 100): [Loss_D_real: 0.737021, Loss_D_fake: 0.731492, acc: 38.67%] [Loss_G: 0.820556]\n",
      "Steps (90 / 100): [Loss_D_real: 0.740837, Loss_D_fake: 0.676387, acc: 43.36%] [Loss_G: 0.857782]\n",
      "Steps (100 / 100): [Loss_D_real: 0.722612, Loss_D_fake: 0.661737, acc: 48.83%] [Loss_G: 0.881852]\n",
      "EPOCH #  18 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.693837, Loss_D_fake: 0.718604, acc: 46.09%] [Loss_G: 0.824833]\n",
      "Steps (20 / 100): [Loss_D_real: 0.753962, Loss_D_fake: 0.717372, acc: 44.92%] [Loss_G: 0.840739]\n",
      "Steps (30 / 100): [Loss_D_real: 0.754400, Loss_D_fake: 0.689214, acc: 44.53%] [Loss_G: 0.819294]\n",
      "Steps (40 / 100): [Loss_D_real: 0.726472, Loss_D_fake: 0.667410, acc: 54.69%] [Loss_G: 0.874419]\n",
      "Steps (50 / 100): [Loss_D_real: 0.728482, Loss_D_fake: 0.696357, acc: 44.53%] [Loss_G: 0.842898]\n",
      "Steps (60 / 100): [Loss_D_real: 0.755835, Loss_D_fake: 0.689459, acc: 43.36%] [Loss_G: 0.825580]\n",
      "Steps (70 / 100): [Loss_D_real: 0.720426, Loss_D_fake: 0.710807, acc: 38.67%] [Loss_G: 0.791801]\n",
      "Steps (80 / 100): [Loss_D_real: 0.701718, Loss_D_fake: 0.733303, acc: 43.75%] [Loss_G: 0.794898]\n",
      "Steps (90 / 100): [Loss_D_real: 0.669993, Loss_D_fake: 0.685120, acc: 53.52%] [Loss_G: 0.837462]\n",
      "Steps (100 / 100): [Loss_D_real: 0.691901, Loss_D_fake: 0.638556, acc: 60.94%] [Loss_G: 0.860275]\n",
      "EPOCH #  19 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.680863, Loss_D_fake: 0.680407, acc: 51.56%] [Loss_G: 0.868714]\n",
      "Steps (20 / 100): [Loss_D_real: 0.782343, Loss_D_fake: 0.722187, acc: 37.11%] [Loss_G: 0.825203]\n",
      "Steps (30 / 100): [Loss_D_real: 0.762423, Loss_D_fake: 0.707601, acc: 40.62%] [Loss_G: 0.789248]\n",
      "Steps (40 / 100): [Loss_D_real: 0.704891, Loss_D_fake: 0.665252, acc: 53.12%] [Loss_G: 0.827338]\n",
      "Steps (50 / 100): [Loss_D_real: 0.672375, Loss_D_fake: 0.665974, acc: 59.38%] [Loss_G: 0.829195]\n",
      "Steps (60 / 100): [Loss_D_real: 0.693672, Loss_D_fake: 0.738626, acc: 41.02%] [Loss_G: 0.793926]\n",
      "Steps (70 / 100): [Loss_D_real: 0.755253, Loss_D_fake: 0.707350, acc: 39.06%] [Loss_G: 0.809599]\n",
      "Steps (80 / 100): [Loss_D_real: 0.709771, Loss_D_fake: 0.715036, acc: 42.58%] [Loss_G: 0.829287]\n",
      "Steps (90 / 100): [Loss_D_real: 0.722075, Loss_D_fake: 0.680669, acc: 46.88%] [Loss_G: 0.846984]\n",
      "Steps (100 / 100): [Loss_D_real: 0.742702, Loss_D_fake: 0.630040, acc: 55.86%] [Loss_G: 0.884318]\n",
      "EPOCH #  20 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.716066, Loss_D_fake: 0.651318, acc: 51.95%] [Loss_G: 0.879457]\n",
      "Steps (20 / 100): [Loss_D_real: 0.697014, Loss_D_fake: 0.676336, acc: 53.12%] [Loss_G: 0.848317]\n",
      "Steps (30 / 100): [Loss_D_real: 0.701185, Loss_D_fake: 0.680654, acc: 53.52%] [Loss_G: 0.846797]\n",
      "Steps (40 / 100): [Loss_D_real: 0.721335, Loss_D_fake: 0.654703, acc: 48.83%] [Loss_G: 0.849003]\n",
      "Steps (50 / 100): [Loss_D_real: 0.749277, Loss_D_fake: 0.709937, acc: 41.80%] [Loss_G: 0.814637]\n",
      "Steps (60 / 100): [Loss_D_real: 0.741866, Loss_D_fake: 0.677873, acc: 44.14%] [Loss_G: 0.814589]\n",
      "Steps (70 / 100): [Loss_D_real: 0.716230, Loss_D_fake: 0.650329, acc: 53.91%] [Loss_G: 0.883314]\n",
      "Steps (80 / 100): [Loss_D_real: 0.634377, Loss_D_fake: 0.653342, acc: 60.55%] [Loss_G: 0.894452]\n",
      "Steps (90 / 100): [Loss_D_real: 0.714941, Loss_D_fake: 0.671652, acc: 51.17%] [Loss_G: 0.860422]\n",
      "Steps (100 / 100): [Loss_D_real: 0.698666, Loss_D_fake: 0.696949, acc: 52.34%] [Loss_G: 0.825170]\n"
     ]
    }
   ],
   "source": [
    "def discriminator(encoder):\n",
    "\n",
    "    data = Input(shape=(data_dim,))\n",
    "    x = encoder(data)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid', kernel_initializer=weight_init)(x)\n",
    "\n",
    "    model = Model(inputs=data, outputs=out)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generator(decoder):\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "\n",
    "    generated_feature = decoder(noise)\n",
    "    model = Model(inputs=noise, outputs=generated_feature)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_G(generator, discriminator):\n",
    "    # Freeze the discriminator when training generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "D = discriminator(en)\n",
    "G = generator(de)\n",
    "\n",
    "D.summary()\n",
    "G.summary()\n",
    "\n",
    "x_train = df_train.drop(target, axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target, axis=1)\n",
    "y_test = df_test[target]\n",
    "\n",
    "gan = GAN(g_model=G, d_model=D)\n",
    "\n",
    "EPOCHS = 20\n",
    "X_train = x_train.to_numpy()\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH # ', epoch + 1, '-' * 50)\n",
    "    gan.train(X_train, batch_size=128, steps_per_epoch=100)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        gan.generator.save('gan_pre_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "generator = tf.keras.models.load_model('gan_pre_generator.h5')\n",
    "\n",
    "noise = np.random.normal(0, 1, size=(1000, 32))\n",
    "fake_data = generator.predict(noise)\n",
    "decoded_feature = pd.DataFrame(fake_data)\n",
    "\n",
    "def boxplot_compare(df1, df2,title):\n",
    "  fig, ax = plt.subplots(figsize=(16,10))\n",
    "  bp1 = df1.boxplot(color='green', showfliers=False)\n",
    "  bp2 = df2.boxplot(color='purple', showfliers=False)\n",
    "\n",
    "  patch1 = mpatches.Patch(color='green', label='Original')\n",
    "  patch2 = mpatches.Patch(color='purple', label='GAN with Pre-Train')\n",
    "  plt.legend(handles=[patch1, patch2], prop={'size': 16})\n",
    "  ax.set_title(title)\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df['Time'] = (df['Time'].values / 3600)\n",
    "df['Amount'] = np.log10(df['Amount'].values + 1)\n",
    "\n",
    "feature = df.loc[:, df.columns != 'Class']\n",
    "true = df.loc[df['Class'] == 0]\n",
    "fraud = df.loc[df['Class'] == 1]\n",
    "true = true.loc[:, df.columns != 'Class']\n",
    "fraud = fraud.loc[:, df.columns != 'Class']\n",
    "\n",
    "boxplot_compare(fraud, df_gen_1000, 'Original Data Distribution versus GAN with Pre-Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_raw, train_size=0.8, random_state=42, stratify=df_raw[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test  = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "x_train = df_train.drop(target,axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target,axis=1)\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(generator, n_data):\n",
    "    noise = np.random.normal(0, 1, size=(n_data, latent_dim))\n",
    "    gen = generator.predict(noise)\n",
    "    x_train_gen = np.concatenate((x_train, gen))\n",
    "    y_gen = np.array(gen.shape[0] * [1])\n",
    "    y_train_gen = np.concatenate((y_train, y_gen))\n",
    "    return gen, x_train_gen, y_train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 more fraud\n",
    "gen_1000, x_train_gen_1000, y_train_gen_1000 = gen_data(generator, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.472897</td>\n",
       "      <td>0.269123</td>\n",
       "      <td>-0.895008</td>\n",
       "      <td>0.614541</td>\n",
       "      <td>0.310112</td>\n",
       "      <td>-0.540636</td>\n",
       "      <td>-0.202127</td>\n",
       "      <td>-0.892870</td>\n",
       "      <td>0.885713</td>\n",
       "      <td>-0.793311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245588</td>\n",
       "      <td>-0.024109</td>\n",
       "      <td>-0.614912</td>\n",
       "      <td>0.032546</td>\n",
       "      <td>0.213735</td>\n",
       "      <td>-0.103700</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>0.175866</td>\n",
       "      <td>-0.186939</td>\n",
       "      <td>1.879387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.328333</td>\n",
       "      <td>1.315495</td>\n",
       "      <td>1.126839</td>\n",
       "      <td>1.394587</td>\n",
       "      <td>1.504691</td>\n",
       "      <td>1.180996</td>\n",
       "      <td>1.295504</td>\n",
       "      <td>0.933658</td>\n",
       "      <td>0.981721</td>\n",
       "      <td>1.337022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555606</td>\n",
       "      <td>0.537005</td>\n",
       "      <td>0.516557</td>\n",
       "      <td>0.523011</td>\n",
       "      <td>0.559640</td>\n",
       "      <td>0.597365</td>\n",
       "      <td>0.407212</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.426948</td>\n",
       "      <td>0.964702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.338640</td>\n",
       "      <td>-4.212586</td>\n",
       "      <td>-5.153825</td>\n",
       "      <td>-3.832330</td>\n",
       "      <td>-4.291127</td>\n",
       "      <td>-4.835877</td>\n",
       "      <td>-4.112792</td>\n",
       "      <td>-4.009332</td>\n",
       "      <td>-2.067336</td>\n",
       "      <td>-5.832733</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.849974</td>\n",
       "      <td>-1.723314</td>\n",
       "      <td>-2.298334</td>\n",
       "      <td>-2.460097</td>\n",
       "      <td>-1.730210</td>\n",
       "      <td>-2.369094</td>\n",
       "      <td>-1.319195</td>\n",
       "      <td>-0.881849</td>\n",
       "      <td>-1.729712</td>\n",
       "      <td>-1.208726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.283927</td>\n",
       "      <td>-0.615794</td>\n",
       "      <td>-1.560161</td>\n",
       "      <td>-0.270948</td>\n",
       "      <td>-0.675545</td>\n",
       "      <td>-1.282441</td>\n",
       "      <td>-1.084698</td>\n",
       "      <td>-1.509457</td>\n",
       "      <td>0.196102</td>\n",
       "      <td>-1.710471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127216</td>\n",
       "      <td>-0.396885</td>\n",
       "      <td>-0.942913</td>\n",
       "      <td>-0.283761</td>\n",
       "      <td>-0.143362</td>\n",
       "      <td>-0.489774</td>\n",
       "      <td>-0.260062</td>\n",
       "      <td>-0.056408</td>\n",
       "      <td>-0.461286</td>\n",
       "      <td>1.207530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.090736</td>\n",
       "      <td>0.344032</td>\n",
       "      <td>-0.829663</td>\n",
       "      <td>0.631187</td>\n",
       "      <td>0.293893</td>\n",
       "      <td>-0.492750</td>\n",
       "      <td>-0.188531</td>\n",
       "      <td>-0.871711</td>\n",
       "      <td>0.895862</td>\n",
       "      <td>-0.735797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225387</td>\n",
       "      <td>-0.005805</td>\n",
       "      <td>-0.577185</td>\n",
       "      <td>0.061818</td>\n",
       "      <td>0.187524</td>\n",
       "      <td>-0.101438</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.166223</td>\n",
       "      <td>-0.166284</td>\n",
       "      <td>1.870305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.620511</td>\n",
       "      <td>1.157255</td>\n",
       "      <td>-0.139510</td>\n",
       "      <td>1.535365</td>\n",
       "      <td>1.274335</td>\n",
       "      <td>0.238183</td>\n",
       "      <td>0.631072</td>\n",
       "      <td>-0.228222</td>\n",
       "      <td>1.594603</td>\n",
       "      <td>0.139798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584330</td>\n",
       "      <td>0.329476</td>\n",
       "      <td>-0.274662</td>\n",
       "      <td>0.401767</td>\n",
       "      <td>0.569205</td>\n",
       "      <td>0.300653</td>\n",
       "      <td>0.261738</td>\n",
       "      <td>0.413273</td>\n",
       "      <td>0.092329</td>\n",
       "      <td>2.511708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72.969955</td>\n",
       "      <td>4.020152</td>\n",
       "      <td>2.146783</td>\n",
       "      <td>5.107396</td>\n",
       "      <td>5.496337</td>\n",
       "      <td>2.724861</td>\n",
       "      <td>3.646155</td>\n",
       "      <td>2.861658</td>\n",
       "      <td>4.134928</td>\n",
       "      <td>2.978698</td>\n",
       "      <td>...</td>\n",
       "      <td>2.356845</td>\n",
       "      <td>1.589298</td>\n",
       "      <td>0.706371</td>\n",
       "      <td>1.304573</td>\n",
       "      <td>1.891358</td>\n",
       "      <td>1.605636</td>\n",
       "      <td>1.357738</td>\n",
       "      <td>1.377679</td>\n",
       "      <td>1.037881</td>\n",
       "      <td>4.959672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     26.472897     0.269123    -0.895008     0.614541     0.310112   \n",
       "std      13.328333     1.315495     1.126839     1.394587     1.504691   \n",
       "min      -2.338640    -4.212586    -5.153825    -3.832330    -4.291127   \n",
       "25%      16.283927    -0.615794    -1.560161    -0.270948    -0.675545   \n",
       "50%      25.090736     0.344032    -0.829663     0.631187     0.293893   \n",
       "75%      34.620511     1.157255    -0.139510     1.535365     1.274335   \n",
       "max      72.969955     4.020152     2.146783     5.107396     5.496337   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -0.540636    -0.202127    -0.892870     0.885713    -0.793311  ...   \n",
       "std       1.180996     1.295504     0.933658     0.981721     1.337022  ...   \n",
       "min      -4.835877    -4.112792    -4.009332    -2.067336    -5.832733  ...   \n",
       "25%      -1.282441    -1.084698    -1.509457     0.196102    -1.710471  ...   \n",
       "50%      -0.492750    -0.188531    -0.871711     0.895862    -0.735797  ...   \n",
       "75%       0.238183     0.631072    -0.228222     1.594603     0.139798  ...   \n",
       "max       2.724861     3.646155     2.861658     4.134928     2.978698  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.245588    -0.024109    -0.614912     0.032546     0.213735   \n",
       "std       0.555606     0.537005     0.516557     0.523011     0.559640   \n",
       "min      -1.849974    -1.723314    -2.298334    -2.460097    -1.730210   \n",
       "25%      -0.127216    -0.396885    -0.942913    -0.283761    -0.143362   \n",
       "50%       0.225387    -0.005805    -0.577185     0.061818     0.187524   \n",
       "75%       0.584330     0.329476    -0.274662     0.401767     0.569205   \n",
       "max       2.356845     1.589298     0.706371     1.304573     1.891358   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.103700    -0.001885     0.175866    -0.186939     1.879387  \n",
       "std       0.597365     0.407212     0.339657     0.426948     0.964702  \n",
       "min      -2.369094    -1.319195    -0.881849    -1.729712    -1.208726  \n",
       "25%      -0.489774    -0.260062    -0.056408    -0.461286     1.207530  \n",
       "50%      -0.101438     0.000217     0.166223    -0.166284     1.870305  \n",
       "75%       0.300653     0.261738     0.413273     0.092329     2.511708  \n",
       "max       1.605636     1.357738     1.377679     1.037881     4.959672  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen_1000 = pd.DataFrame(data=gen_1000, index=None, columns=x_train.columns)\n",
    "df_gen_1000.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.008938</td>\n",
       "      <td>-4.707808</td>\n",
       "      <td>3.588729</td>\n",
       "      <td>-7.068378</td>\n",
       "      <td>4.592975</td>\n",
       "      <td>-3.101629</td>\n",
       "      <td>-1.387192</td>\n",
       "      <td>-5.539909</td>\n",
       "      <td>0.587920</td>\n",
       "      <td>-2.589654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358018</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.051318</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>-0.109108</td>\n",
       "      <td>0.019602</td>\n",
       "      <td>0.047827</td>\n",
       "      <td>0.155933</td>\n",
       "      <td>0.077212</td>\n",
       "      <td>1.228674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.347935</td>\n",
       "      <td>6.841390</td>\n",
       "      <td>4.309436</td>\n",
       "      <td>7.166449</td>\n",
       "      <td>2.883467</td>\n",
       "      <td>5.406586</td>\n",
       "      <td>1.864770</td>\n",
       "      <td>7.316745</td>\n",
       "      <td>6.676697</td>\n",
       "      <td>2.495584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384017</td>\n",
       "      <td>3.750615</td>\n",
       "      <td>1.457801</td>\n",
       "      <td>1.681228</td>\n",
       "      <td>0.509477</td>\n",
       "      <td>0.826820</td>\n",
       "      <td>0.467046</td>\n",
       "      <td>1.358987</td>\n",
       "      <td>0.555106</td>\n",
       "      <td>0.965996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.239444</td>\n",
       "      <td>-30.552380</td>\n",
       "      <td>-8.402154</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>-1.313275</td>\n",
       "      <td>-22.105532</td>\n",
       "      <td>-5.773192</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.128186</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-19.254328</td>\n",
       "      <td>-2.028024</td>\n",
       "      <td>-4.781606</td>\n",
       "      <td>-1.152671</td>\n",
       "      <td>-7.263482</td>\n",
       "      <td>-1.869290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.500278</td>\n",
       "      <td>-5.996596</td>\n",
       "      <td>1.229209</td>\n",
       "      <td>-8.436924</td>\n",
       "      <td>2.419178</td>\n",
       "      <td>-4.741036</td>\n",
       "      <td>-2.504633</td>\n",
       "      <td>-7.765017</td>\n",
       "      <td>-0.135707</td>\n",
       "      <td>-3.828323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181718</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>-0.515338</td>\n",
       "      <td>-0.330293</td>\n",
       "      <td>-0.445282</td>\n",
       "      <td>-0.312004</td>\n",
       "      <td>-0.253693</td>\n",
       "      <td>-0.025894</td>\n",
       "      <td>-0.096541</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.393056</td>\n",
       "      <td>-2.272114</td>\n",
       "      <td>2.662472</td>\n",
       "      <td>-5.133485</td>\n",
       "      <td>4.258196</td>\n",
       "      <td>-1.522962</td>\n",
       "      <td>-1.421577</td>\n",
       "      <td>-2.926216</td>\n",
       "      <td>0.642565</td>\n",
       "      <td>-2.230097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280862</td>\n",
       "      <td>0.576441</td>\n",
       "      <td>0.073696</td>\n",
       "      <td>-0.057241</td>\n",
       "      <td>-0.060269</td>\n",
       "      <td>0.088371</td>\n",
       "      <td>-0.003464</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>0.147380</td>\n",
       "      <td>1.007318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.912917</td>\n",
       "      <td>-0.410418</td>\n",
       "      <td>4.737900</td>\n",
       "      <td>-2.302626</td>\n",
       "      <td>6.390866</td>\n",
       "      <td>0.240184</td>\n",
       "      <td>-0.361122</td>\n",
       "      <td>-0.900824</td>\n",
       "      <td>1.743587</td>\n",
       "      <td>-0.825345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783528</td>\n",
       "      <td>1.204214</td>\n",
       "      <td>0.615344</td>\n",
       "      <td>0.307132</td>\n",
       "      <td>0.274014</td>\n",
       "      <td>0.441670</td>\n",
       "      <td>0.393148</td>\n",
       "      <td>0.779267</td>\n",
       "      <td>0.372389</td>\n",
       "      <td>2.028937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.318889</td>\n",
       "      <td>2.132386</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>2.250210</td>\n",
       "      <td>12.114672</td>\n",
       "      <td>11.095089</td>\n",
       "      <td>6.474115</td>\n",
       "      <td>5.802537</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>...</td>\n",
       "      <td>11.059004</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>5.466230</td>\n",
       "      <td>0.994110</td>\n",
       "      <td>2.208209</td>\n",
       "      <td>2.745261</td>\n",
       "      <td>3.052358</td>\n",
       "      <td>1.779364</td>\n",
       "      <td>3.327741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time          V1          V2          V3          V4          V5  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean    23.008938   -4.707808    3.588729   -7.068378    4.592975   -3.101629   \n",
       "std     13.347935    6.841390    4.309436    7.166449    2.883467    5.406586   \n",
       "min      1.239444  -30.552380   -8.402154  -31.103685   -1.313275  -22.105532   \n",
       "25%     11.500278   -5.996596    1.229209   -8.436924    2.419178   -4.741036   \n",
       "50%     21.393056   -2.272114    2.662472   -5.133485    4.258196   -1.522962   \n",
       "75%     35.912917   -0.410418    4.737900   -2.302626    6.390866    0.240184   \n",
       "max     47.318889    2.132386   22.057729    2.250210   12.114672   11.095089   \n",
       "\n",
       "               V6          V7          V8          V9  ...         V20  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  ...  394.000000   \n",
       "mean    -1.387192   -5.539909    0.587920   -2.589654  ...    0.358018   \n",
       "std      1.864770    7.316745    6.676697    2.495584  ...    1.384017   \n",
       "min     -5.773192  -43.557242  -41.044261  -13.434066  ...   -4.128186   \n",
       "25%     -2.504633   -7.765017   -0.135707   -3.828323  ...   -0.181718   \n",
       "50%     -1.421577   -2.926216    0.642565   -2.230097  ...    0.280862   \n",
       "75%     -0.361122   -0.900824    1.743587   -0.825345  ...    0.783528   \n",
       "max      6.474115    5.802537   20.007208    3.353525  ...   11.059004   \n",
       "\n",
       "              V21         V22         V23         V24         V25         V26  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean     0.628814    0.051318   -0.062790   -0.109108    0.019602    0.047827   \n",
       "std      3.750615    1.457801    1.681228    0.509477    0.826820    0.467046   \n",
       "min    -22.797604   -8.887017  -19.254328   -2.028024   -4.781606   -1.152671   \n",
       "25%      0.040122   -0.515338   -0.330293   -0.445282   -0.312004   -0.253693   \n",
       "50%      0.576441    0.073696   -0.057241   -0.060269    0.088371   -0.003464   \n",
       "75%      1.204214    0.615344    0.307132    0.274014    0.441670    0.393148   \n",
       "max     27.202839    8.361985    5.466230    0.994110    2.208209    2.745261   \n",
       "\n",
       "              V27         V28      Amount  \n",
       "count  394.000000  394.000000  394.000000  \n",
       "mean     0.155933    0.077212    1.228674  \n",
       "std      1.358987    0.555106    0.965996  \n",
       "min     -7.263482   -1.869290    0.000000  \n",
       "25%     -0.025894   -0.096541    0.301030  \n",
       "50%      0.394926    0.147380    1.007318  \n",
       "75%      0.779267    0.372389    2.028937  \n",
       "max      3.052358    1.779364    3.327741  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_fraud = x_train[y_train==1]\n",
    "x_train_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:86: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# with GPU\n",
    "ros = RandomOverSampler()\n",
    "x, y = ros.fit_sample(x_train_gen_1000, y_train_gen_1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with GPU\n",
    "ros = RandomOverSampler()\n",
    "def XGBC_model_predit(x, y):   \n",
    "    x, y = ros.fit_sample(x, y)\n",
    "    clf_xgb_os = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
    "              gamma=0.2, gpu_id=0, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.02, max_delta_step=0,\n",
    "              max_depth=10, min_child_weight=2,\n",
    "              monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
    "              n_estimators=800, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "              tree_method='gpu_hist', validate_parameters=1)\n",
    "    \n",
    "    clf_xgb_os.fit(x, y)  \n",
    "    y_pred_gen_os = clf_xgb_os.predict(x_test.to_numpy())\n",
    "    return y_pred_gen_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(y_test, y_pred):\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1 score: ', f1_score(y_test, y_pred))\n",
    "    print('ROC AUC score: ',  roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "import scikitplot as skplt\n",
    "def plot_cm(y_test, y_pred, title):\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, y_pred,figsize=(8,8))\n",
    "    plt.title('Confusion Matrix ' + title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:86: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995435553526912\n",
      "Precision:  0.8829787234042553\n",
      "Recall:  0.8469387755102041\n",
      "F1 score:  0.8645833333333334\n",
      "ROC AUC score:  0.9233726657517257\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_1000 = XGBC_model_predit(x_train_gen_1000, y_train_gen_1000)\n",
    "check_performance(y_test, y_pred_gen_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_1000, 'GAN with Pre-Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
