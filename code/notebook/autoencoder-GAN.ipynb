{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "import os\n",
    "from keras.layers import Input, Embedding, multiply, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, g_model, d_model):\n",
    "        self.z = latent_dim\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.generator = g_model\n",
    "        self.discriminator = d_model\n",
    "\n",
    "        self.train_G = train_G(self.generator, self.discriminator)\n",
    "        self.loss_D, self.loss_G = [], []\n",
    "\n",
    "    def train(self, data, batch_size=128, steps_per_epoch=100):\n",
    "\n",
    "        for epoch in range(steps_per_epoch):\n",
    "            # Select a random batch of transactions data\n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            real_data = data[idx]\n",
    "\n",
    "            # generate a batch of new data\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            fake_data = self.generator.predict(noise)\n",
    "\n",
    "            # Train D\n",
    "            loss_real = self.discriminator.train_on_batch(real_data, np.ones(batch_size))\n",
    "            loss_fake = self.discriminator.train_on_batch(fake_data, np.zeros(batch_size))\n",
    "            self.loss_D.append(0.5 * np.add(loss_fake, loss_real))\n",
    "\n",
    "            # Train G\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            loss_G = self.train_G.train_on_batch(noise, np.ones(batch_size))\n",
    "            self.loss_G.append(loss_G)\n",
    "\n",
    "            if (epoch + 1) * 10 % steps_per_epoch == 0:\n",
    "                print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f, acc: %.2f%%] [Loss_G: %f]' %\n",
    "                  (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], 100*self.loss_D[-1][1], loss_G))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('creditcard.csv')\n",
    "df_raw['Amount'] = np.log10(df_raw['Amount'].values + 1)\n",
    "df_raw['Time'] = (df_raw['Time'].values/3600)\n",
    "df_fraud = df_raw[df_raw['Class'] == 1]\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_raw, train_size=0.8, random_state=42, stratify=df_raw[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "x_train = df_train.drop(target, axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target, axis=1)\n",
    "y_test = df_test[target]\n",
    "\n",
    "\n",
    "# %% --------------------------------------- Set Seeds -----------------------------------------------------------------\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "weight_init = glorot_normal(seed=SEED)\n",
    "\n",
    "# %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n",
    "latent_dim = 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.1, beta_2=0.9)\n",
    "trainRatio = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program Files\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 227845 samples, validate on 56962 samples\n",
      "Epoch 1/30\n",
      "227845/227845 [==============================] - 5s 24us/step - loss: 0.4911 - val_loss: 0.3555\n",
      "Epoch 2/30\n",
      "227845/227845 [==============================] - 5s 24us/step - loss: 0.3011 - val_loss: 0.2957\n",
      "Epoch 3/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.2364 - val_loss: 0.2083\n",
      "Epoch 4/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.2008 - val_loss: 0.2059\n",
      "Epoch 5/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1709 - val_loss: 0.2165\n",
      "Epoch 6/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.1506 - val_loss: 0.1623\n",
      "Epoch 7/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.1391 - val_loss: 0.1694\n",
      "Epoch 8/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1315 - val_loss: 0.1182\n",
      "Epoch 9/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1257 - val_loss: 0.1504\n",
      "Epoch 10/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1207 - val_loss: 0.1356\n",
      "Epoch 11/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.1165 - val_loss: 0.1348\n",
      "Epoch 12/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.1127 - val_loss: 0.1583\n",
      "Epoch 13/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.1094 - val_loss: 0.1095\n",
      "Epoch 14/30\n",
      "227845/227845 [==============================] - 6s 25us/step - loss: 0.1063 - val_loss: 0.1170\n",
      "Epoch 15/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.1029 - val_loss: 0.0998\n",
      "Epoch 16/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0998 - val_loss: 0.1009\n",
      "Epoch 17/30\n",
      "227845/227845 [==============================] - 5s 24us/step - loss: 0.0973 - val_loss: 0.1263\n",
      "Epoch 18/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0950 - val_loss: 0.0932\n",
      "Epoch 19/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0926 - val_loss: 0.1022\n",
      "Epoch 20/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0907 - val_loss: 0.1026\n",
      "Epoch 21/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0889 - val_loss: 0.1419\n",
      "Epoch 22/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0872 - val_loss: 0.1218\n",
      "Epoch 23/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0857 - val_loss: 0.1171\n",
      "Epoch 24/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0842 - val_loss: 0.0881\n",
      "Epoch 25/30\n",
      "227845/227845 [==============================] - 5s 21us/step - loss: 0.0828 - val_loss: 0.0889\n",
      "Epoch 26/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0815 - val_loss: 0.0859\n",
      "Epoch 27/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0803 - val_loss: 0.0837\n",
      "Epoch 28/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.0790 - val_loss: 0.0693\n",
      "Epoch 29/30\n",
      "227845/227845 [==============================] - 5s 23us/step - loss: 0.0778 - val_loss: 0.0843\n",
      "Epoch 30/30\n",
      "227845/227845 [==============================] - 5s 22us/step - loss: 0.0767 - val_loss: 0.0827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e638fab2e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Encoder\n",
    "def encoder():\n",
    "    data = Input(shape=(data_dim,))\n",
    "    x = Dense(256, kernel_initializer=weight_init)(data)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    #    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    #    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = Dense(64, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    #    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    encodered = Dense(latent_dim)(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=data, outputs=encodered)\n",
    "    return model\n",
    "\n",
    "def decoder():\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "\n",
    "    x = Dense(64, kernel_initializer=weight_init)(noise)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Dense(256, kernel_initializer=weight_init)(x)\n",
    "    #     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    # tanh is removed since we are not dealing with normalized image data\n",
    "    generated = Dense(data_dim, kernel_initializer=weight_init)(x)\n",
    "\n",
    "    generator = Model(inputs=noise, outputs=generated)\n",
    "    return generator\n",
    "\n",
    "# Build Autoencoder\n",
    "def train_AE(encoder, decoder):\n",
    "    feature = Input(shape=(data_dim,))\n",
    "    latent = encoder(feature)\n",
    "    rec_feature = decoder(latent)\n",
    "    model = Model(feature, rec_feature)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    return model\n",
    "\n",
    "# Train Autoencoder\n",
    "en = encoder()\n",
    "de = decoder()\n",
    "ae = train_AE(en, de)\n",
    "\n",
    "ae.fit(x_train, x_train,\n",
    "       epochs=30,\n",
    "       batch_size=128,\n",
    "       shuffle=True,\n",
    "       validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_fraud, train_size=0.8, random_state=42, stratify=df_fraud[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "x_train = df_train.drop(target, axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target, axis=1)\n",
    "y_test = df_test[target]\n",
    "\n",
    "latent_dim = 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.1, beta_2=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 32)                51168     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 51,201\n",
      "Trainable params: 51,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 30)                51166     \n",
      "=================================================================\n",
      "Total params: 51,166\n",
      "Trainable params: 51,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "EPOCH #  1 --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (10 / 100): [Loss_D_real: 1.854686, Loss_D_fake: 0.322197, acc: 60.55%] [Loss_G: 1.464881]\n",
      "Steps (20 / 100): [Loss_D_real: 0.689074, Loss_D_fake: 0.478994, acc: 78.52%] [Loss_G: 1.215435]\n",
      "Steps (30 / 100): [Loss_D_real: 0.404704, Loss_D_fake: 0.573806, acc: 77.73%] [Loss_G: 1.022084]\n",
      "Steps (40 / 100): [Loss_D_real: 0.204999, Loss_D_fake: 0.592633, acc: 79.69%] [Loss_G: 1.057282]\n",
      "Steps (50 / 100): [Loss_D_real: 0.230878, Loss_D_fake: 0.564651, acc: 79.69%] [Loss_G: 1.117832]\n",
      "Steps (60 / 100): [Loss_D_real: 0.313420, Loss_D_fake: 0.513291, acc: 83.59%] [Loss_G: 1.095918]\n",
      "Steps (70 / 100): [Loss_D_real: 0.400042, Loss_D_fake: 0.463009, acc: 83.20%] [Loss_G: 1.316952]\n",
      "Steps (80 / 100): [Loss_D_real: 0.387818, Loss_D_fake: 0.454032, acc: 83.98%] [Loss_G: 1.331147]\n",
      "Steps (90 / 100): [Loss_D_real: 0.431270, Loss_D_fake: 0.528597, acc: 82.42%] [Loss_G: 1.126544]\n",
      "Steps (100 / 100): [Loss_D_real: 0.431223, Loss_D_fake: 0.621392, acc: 73.83%] [Loss_G: 1.063085]\n",
      "EPOCH #  2 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.480855, Loss_D_fake: 0.656504, acc: 74.61%] [Loss_G: 0.934815]\n",
      "Steps (20 / 100): [Loss_D_real: 0.574273, Loss_D_fake: 0.698775, acc: 62.89%] [Loss_G: 0.946818]\n",
      "Steps (30 / 100): [Loss_D_real: 0.526741, Loss_D_fake: 0.664299, acc: 69.14%] [Loss_G: 0.840181]\n",
      "Steps (40 / 100): [Loss_D_real: 0.662767, Loss_D_fake: 0.696485, acc: 62.89%] [Loss_G: 0.828028]\n",
      "Steps (50 / 100): [Loss_D_real: 0.554833, Loss_D_fake: 0.726489, acc: 61.33%] [Loss_G: 0.782766]\n",
      "Steps (60 / 100): [Loss_D_real: 0.570436, Loss_D_fake: 0.780664, acc: 55.47%] [Loss_G: 0.762307]\n",
      "Steps (70 / 100): [Loss_D_real: 0.547199, Loss_D_fake: 0.767559, acc: 57.03%] [Loss_G: 0.755035]\n",
      "Steps (80 / 100): [Loss_D_real: 0.664404, Loss_D_fake: 0.817505, acc: 49.22%] [Loss_G: 0.769882]\n",
      "Steps (90 / 100): [Loss_D_real: 0.776118, Loss_D_fake: 0.793837, acc: 44.14%] [Loss_G: 0.712957]\n",
      "Steps (100 / 100): [Loss_D_real: 0.700213, Loss_D_fake: 0.823913, acc: 46.88%] [Loss_G: 0.733293]\n",
      "EPOCH #  3 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.730998, Loss_D_fake: 0.837253, acc: 38.67%] [Loss_G: 0.745497]\n",
      "Steps (20 / 100): [Loss_D_real: 0.755999, Loss_D_fake: 0.791342, acc: 42.97%] [Loss_G: 0.749745]\n",
      "Steps (30 / 100): [Loss_D_real: 0.838758, Loss_D_fake: 0.801048, acc: 37.50%] [Loss_G: 0.755547]\n",
      "Steps (40 / 100): [Loss_D_real: 0.758647, Loss_D_fake: 0.807530, acc: 40.23%] [Loss_G: 0.755489]\n",
      "Steps (50 / 100): [Loss_D_real: 0.779037, Loss_D_fake: 0.787550, acc: 39.84%] [Loss_G: 0.790228]\n",
      "Steps (60 / 100): [Loss_D_real: 0.783328, Loss_D_fake: 0.803630, acc: 34.77%] [Loss_G: 0.813116]\n",
      "Steps (70 / 100): [Loss_D_real: 0.793971, Loss_D_fake: 0.739004, acc: 37.50%] [Loss_G: 0.871122]\n",
      "Steps (80 / 100): [Loss_D_real: 0.806571, Loss_D_fake: 0.751234, acc: 29.30%] [Loss_G: 0.839270]\n",
      "Steps (90 / 100): [Loss_D_real: 0.820908, Loss_D_fake: 0.692585, acc: 42.58%] [Loss_G: 0.884432]\n",
      "Steps (100 / 100): [Loss_D_real: 0.799650, Loss_D_fake: 0.687844, acc: 41.41%] [Loss_G: 0.932693]\n",
      "EPOCH #  4 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.848790, Loss_D_fake: 0.725036, acc: 35.94%] [Loss_G: 0.898022]\n",
      "Steps (20 / 100): [Loss_D_real: 0.852451, Loss_D_fake: 0.737314, acc: 29.30%] [Loss_G: 0.876061]\n",
      "Steps (30 / 100): [Loss_D_real: 0.787709, Loss_D_fake: 0.704762, acc: 40.62%] [Loss_G: 0.887460]\n",
      "Steps (40 / 100): [Loss_D_real: 0.791786, Loss_D_fake: 0.657609, acc: 47.27%] [Loss_G: 0.933159]\n",
      "Steps (50 / 100): [Loss_D_real: 0.828878, Loss_D_fake: 0.710171, acc: 37.50%] [Loss_G: 0.897653]\n",
      "Steps (60 / 100): [Loss_D_real: 0.803347, Loss_D_fake: 0.660134, acc: 44.92%] [Loss_G: 0.940145]\n",
      "Steps (70 / 100): [Loss_D_real: 0.750838, Loss_D_fake: 0.624259, acc: 57.42%] [Loss_G: 0.993902]\n",
      "Steps (80 / 100): [Loss_D_real: 0.675980, Loss_D_fake: 0.571680, acc: 70.70%] [Loss_G: 1.135259]\n",
      "Steps (90 / 100): [Loss_D_real: 0.681173, Loss_D_fake: 0.590138, acc: 64.06%] [Loss_G: 1.058188]\n",
      "Steps (100 / 100): [Loss_D_real: 0.832044, Loss_D_fake: 0.768514, acc: 34.38%] [Loss_G: 0.888393]\n",
      "EPOCH #  5 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.853676, Loss_D_fake: 0.839376, acc: 28.91%] [Loss_G: 0.756542]\n",
      "Steps (20 / 100): [Loss_D_real: 0.753813, Loss_D_fake: 0.797096, acc: 32.81%] [Loss_G: 0.784061]\n",
      "Steps (30 / 100): [Loss_D_real: 0.743303, Loss_D_fake: 0.755781, acc: 40.23%] [Loss_G: 0.884917]\n",
      "Steps (40 / 100): [Loss_D_real: 0.722438, Loss_D_fake: 0.712047, acc: 48.05%] [Loss_G: 0.945411]\n",
      "Steps (50 / 100): [Loss_D_real: 0.714401, Loss_D_fake: 0.700483, acc: 50.00%] [Loss_G: 0.938817]\n",
      "Steps (60 / 100): [Loss_D_real: 0.725443, Loss_D_fake: 0.715139, acc: 48.83%] [Loss_G: 0.945810]\n",
      "Steps (70 / 100): [Loss_D_real: 0.693902, Loss_D_fake: 0.701026, acc: 51.56%] [Loss_G: 0.974155]\n",
      "Steps (80 / 100): [Loss_D_real: 0.681513, Loss_D_fake: 0.684301, acc: 53.12%] [Loss_G: 1.022557]\n",
      "Steps (90 / 100): [Loss_D_real: 0.709974, Loss_D_fake: 0.718179, acc: 53.12%] [Loss_G: 0.998103]\n",
      "Steps (100 / 100): [Loss_D_real: 0.671254, Loss_D_fake: 0.693972, acc: 57.81%] [Loss_G: 1.015799]\n",
      "EPOCH #  6 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.667185, Loss_D_fake: 0.790812, acc: 48.83%] [Loss_G: 0.865145]\n",
      "Steps (20 / 100): [Loss_D_real: 0.692198, Loss_D_fake: 0.868558, acc: 41.02%] [Loss_G: 0.781471]\n",
      "Steps (30 / 100): [Loss_D_real: 0.677930, Loss_D_fake: 0.789454, acc: 42.19%] [Loss_G: 0.817202]\n",
      "Steps (40 / 100): [Loss_D_real: 0.643583, Loss_D_fake: 0.806810, acc: 48.05%] [Loss_G: 0.838989]\n",
      "Steps (50 / 100): [Loss_D_real: 0.635726, Loss_D_fake: 0.787814, acc: 51.56%] [Loss_G: 0.857738]\n",
      "Steps (60 / 100): [Loss_D_real: 0.738121, Loss_D_fake: 0.912560, acc: 39.06%] [Loss_G: 0.779231]\n",
      "Steps (70 / 100): [Loss_D_real: 0.687815, Loss_D_fake: 0.826691, acc: 47.66%] [Loss_G: 0.834272]\n",
      "Steps (80 / 100): [Loss_D_real: 0.617321, Loss_D_fake: 0.705714, acc: 58.98%] [Loss_G: 0.942081]\n",
      "Steps (90 / 100): [Loss_D_real: 0.620806, Loss_D_fake: 0.660588, acc: 66.41%] [Loss_G: 0.954017]\n",
      "Steps (100 / 100): [Loss_D_real: 0.619629, Loss_D_fake: 0.705806, acc: 60.94%] [Loss_G: 0.907066]\n",
      "EPOCH #  7 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.673574, Loss_D_fake: 0.724578, acc: 53.52%] [Loss_G: 0.903534]\n",
      "Steps (20 / 100): [Loss_D_real: 0.690428, Loss_D_fake: 0.868061, acc: 44.53%] [Loss_G: 0.800766]\n",
      "Steps (30 / 100): [Loss_D_real: 0.649305, Loss_D_fake: 0.874092, acc: 40.62%] [Loss_G: 0.703718]\n",
      "Steps (40 / 100): [Loss_D_real: 0.592377, Loss_D_fake: 0.856039, acc: 38.67%] [Loss_G: 0.729543]\n",
      "Steps (50 / 100): [Loss_D_real: 0.681554, Loss_D_fake: 0.814015, acc: 34.38%] [Loss_G: 0.764354]\n",
      "Steps (60 / 100): [Loss_D_real: 0.630116, Loss_D_fake: 0.763624, acc: 49.22%] [Loss_G: 0.850474]\n",
      "Steps (70 / 100): [Loss_D_real: 0.640124, Loss_D_fake: 0.695461, acc: 59.77%] [Loss_G: 0.915615]\n",
      "Steps (80 / 100): [Loss_D_real: 0.595626, Loss_D_fake: 0.635863, acc: 64.84%] [Loss_G: 0.936207]\n",
      "Steps (90 / 100): [Loss_D_real: 0.629328, Loss_D_fake: 0.671573, acc: 62.11%] [Loss_G: 0.926751]\n",
      "Steps (100 / 100): [Loss_D_real: 0.616634, Loss_D_fake: 0.767931, acc: 49.61%] [Loss_G: 0.826165]\n",
      "EPOCH #  8 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.659925, Loss_D_fake: 0.882866, acc: 40.23%] [Loss_G: 0.760113]\n",
      "Steps (20 / 100): [Loss_D_real: 0.632006, Loss_D_fake: 0.800598, acc: 49.22%] [Loss_G: 0.732861]\n",
      "Steps (30 / 100): [Loss_D_real: 0.678212, Loss_D_fake: 0.808812, acc: 42.97%] [Loss_G: 0.794900]\n",
      "Steps (40 / 100): [Loss_D_real: 0.659421, Loss_D_fake: 0.765443, acc: 49.22%] [Loss_G: 0.839644]\n",
      "Steps (50 / 100): [Loss_D_real: 0.636904, Loss_D_fake: 0.746488, acc: 55.08%] [Loss_G: 0.827870]\n",
      "Steps (60 / 100): [Loss_D_real: 0.673349, Loss_D_fake: 0.784414, acc: 46.88%] [Loss_G: 0.802638]\n",
      "Steps (70 / 100): [Loss_D_real: 0.619229, Loss_D_fake: 0.733885, acc: 53.12%] [Loss_G: 0.824580]\n",
      "Steps (80 / 100): [Loss_D_real: 0.644947, Loss_D_fake: 0.737896, acc: 57.03%] [Loss_G: 0.875733]\n",
      "Steps (90 / 100): [Loss_D_real: 0.614115, Loss_D_fake: 0.686920, acc: 64.06%] [Loss_G: 0.860661]\n",
      "Steps (100 / 100): [Loss_D_real: 0.622248, Loss_D_fake: 0.658484, acc: 60.94%] [Loss_G: 0.907967]\n",
      "EPOCH #  9 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.576370, Loss_D_fake: 0.689348, acc: 61.33%] [Loss_G: 0.895490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (20 / 100): [Loss_D_real: 0.635499, Loss_D_fake: 0.701926, acc: 59.38%] [Loss_G: 0.860522]\n",
      "Steps (30 / 100): [Loss_D_real: 0.656951, Loss_D_fake: 0.766852, acc: 53.91%] [Loss_G: 0.803601]\n",
      "Steps (40 / 100): [Loss_D_real: 0.676729, Loss_D_fake: 0.791411, acc: 45.70%] [Loss_G: 0.840218]\n",
      "Steps (50 / 100): [Loss_D_real: 0.678889, Loss_D_fake: 0.704970, acc: 55.08%] [Loss_G: 0.827372]\n",
      "Steps (60 / 100): [Loss_D_real: 0.675134, Loss_D_fake: 0.669629, acc: 62.50%] [Loss_G: 0.920371]\n",
      "Steps (70 / 100): [Loss_D_real: 0.692258, Loss_D_fake: 0.656257, acc: 57.03%] [Loss_G: 0.919382]\n",
      "Steps (80 / 100): [Loss_D_real: 0.660077, Loss_D_fake: 0.690421, acc: 56.64%] [Loss_G: 0.894136]\n",
      "Steps (90 / 100): [Loss_D_real: 0.692053, Loss_D_fake: 0.775650, acc: 49.61%] [Loss_G: 0.806598]\n",
      "Steps (100 / 100): [Loss_D_real: 0.769649, Loss_D_fake: 0.799149, acc: 42.58%] [Loss_G: 0.767125]\n",
      "EPOCH #  10 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.686837, Loss_D_fake: 0.786363, acc: 44.53%] [Loss_G: 0.775573]\n",
      "Steps (20 / 100): [Loss_D_real: 0.675308, Loss_D_fake: 0.684043, acc: 56.25%] [Loss_G: 0.870984]\n",
      "Steps (30 / 100): [Loss_D_real: 0.684124, Loss_D_fake: 0.652246, acc: 60.55%] [Loss_G: 0.958061]\n",
      "Steps (40 / 100): [Loss_D_real: 0.735189, Loss_D_fake: 0.733634, acc: 47.66%] [Loss_G: 0.869592]\n",
      "Steps (50 / 100): [Loss_D_real: 0.779776, Loss_D_fake: 0.825268, acc: 40.23%] [Loss_G: 0.863068]\n",
      "Steps (60 / 100): [Loss_D_real: 0.791160, Loss_D_fake: 0.750160, acc: 39.06%] [Loss_G: 0.807976]\n",
      "Steps (70 / 100): [Loss_D_real: 0.760636, Loss_D_fake: 0.714570, acc: 47.27%] [Loss_G: 0.856380]\n",
      "Steps (80 / 100): [Loss_D_real: 0.738171, Loss_D_fake: 0.695150, acc: 53.52%] [Loss_G: 0.896773]\n",
      "Steps (90 / 100): [Loss_D_real: 0.668511, Loss_D_fake: 0.760310, acc: 54.30%] [Loss_G: 0.819776]\n",
      "Steps (100 / 100): [Loss_D_real: 0.757757, Loss_D_fake: 0.792322, acc: 42.97%] [Loss_G: 0.786461]\n",
      "EPOCH #  11 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.755858, Loss_D_fake: 0.804773, acc: 42.97%] [Loss_G: 0.805411]\n",
      "Steps (20 / 100): [Loss_D_real: 0.759372, Loss_D_fake: 0.743687, acc: 46.09%] [Loss_G: 0.823525]\n",
      "Steps (30 / 100): [Loss_D_real: 0.788321, Loss_D_fake: 0.761660, acc: 40.23%] [Loss_G: 0.851681]\n",
      "Steps (40 / 100): [Loss_D_real: 0.781000, Loss_D_fake: 0.752930, acc: 40.23%] [Loss_G: 0.855036]\n",
      "Steps (50 / 100): [Loss_D_real: 0.807666, Loss_D_fake: 0.720415, acc: 39.06%] [Loss_G: 0.865360]\n",
      "Steps (60 / 100): [Loss_D_real: 0.750904, Loss_D_fake: 0.690643, acc: 46.88%] [Loss_G: 0.850496]\n",
      "Steps (70 / 100): [Loss_D_real: 0.755908, Loss_D_fake: 0.746198, acc: 41.41%] [Loss_G: 0.887836]\n",
      "Steps (80 / 100): [Loss_D_real: 0.845506, Loss_D_fake: 0.724066, acc: 38.28%] [Loss_G: 0.873994]\n",
      "Steps (90 / 100): [Loss_D_real: 0.861204, Loss_D_fake: 0.739339, acc: 37.11%] [Loss_G: 0.862976]\n",
      "Steps (100 / 100): [Loss_D_real: 0.815640, Loss_D_fake: 0.643433, acc: 50.00%] [Loss_G: 0.905504]\n",
      "EPOCH #  12 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.809482, Loss_D_fake: 0.693593, acc: 45.70%] [Loss_G: 0.860729]\n",
      "Steps (20 / 100): [Loss_D_real: 0.881441, Loss_D_fake: 0.804077, acc: 25.78%] [Loss_G: 0.819349]\n",
      "Steps (30 / 100): [Loss_D_real: 0.863323, Loss_D_fake: 0.709667, acc: 32.81%] [Loss_G: 0.872023]\n",
      "Steps (40 / 100): [Loss_D_real: 0.823514, Loss_D_fake: 0.650454, acc: 42.19%] [Loss_G: 0.970003]\n",
      "Steps (50 / 100): [Loss_D_real: 0.787996, Loss_D_fake: 0.676419, acc: 46.48%] [Loss_G: 0.877592]\n",
      "Steps (60 / 100): [Loss_D_real: 0.913990, Loss_D_fake: 0.716814, acc: 32.42%] [Loss_G: 0.878937]\n",
      "Steps (70 / 100): [Loss_D_real: 0.857856, Loss_D_fake: 0.696922, acc: 37.89%] [Loss_G: 0.891424]\n",
      "Steps (80 / 100): [Loss_D_real: 0.832396, Loss_D_fake: 0.697945, acc: 40.23%] [Loss_G: 0.952772]\n",
      "Steps (90 / 100): [Loss_D_real: 0.802029, Loss_D_fake: 0.708296, acc: 40.23%] [Loss_G: 0.885381]\n",
      "Steps (100 / 100): [Loss_D_real: 0.861122, Loss_D_fake: 0.721344, acc: 29.30%] [Loss_G: 0.834062]\n",
      "EPOCH #  13 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.857763, Loss_D_fake: 0.682186, acc: 38.28%] [Loss_G: 0.892204]\n",
      "Steps (20 / 100): [Loss_D_real: 0.868015, Loss_D_fake: 0.647055, acc: 39.84%] [Loss_G: 0.935522]\n",
      "Steps (30 / 100): [Loss_D_real: 0.856323, Loss_D_fake: 0.642235, acc: 41.80%] [Loss_G: 0.916057]\n",
      "Steps (40 / 100): [Loss_D_real: 0.866454, Loss_D_fake: 0.710568, acc: 33.20%] [Loss_G: 0.855385]\n",
      "Steps (50 / 100): [Loss_D_real: 0.821381, Loss_D_fake: 0.678482, acc: 36.72%] [Loss_G: 0.908708]\n",
      "Steps (60 / 100): [Loss_D_real: 0.762455, Loss_D_fake: 0.731408, acc: 35.55%] [Loss_G: 0.867204]\n",
      "Steps (70 / 100): [Loss_D_real: 0.839462, Loss_D_fake: 0.676199, acc: 35.94%] [Loss_G: 0.882078]\n",
      "Steps (80 / 100): [Loss_D_real: 0.834298, Loss_D_fake: 0.678120, acc: 36.33%] [Loss_G: 0.860855]\n",
      "Steps (90 / 100): [Loss_D_real: 0.822724, Loss_D_fake: 0.638058, acc: 48.05%] [Loss_G: 0.916921]\n",
      "Steps (100 / 100): [Loss_D_real: 0.814054, Loss_D_fake: 0.617547, acc: 50.39%] [Loss_G: 0.959348]\n",
      "EPOCH #  14 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.822078, Loss_D_fake: 0.709029, acc: 38.28%] [Loss_G: 0.874399]\n",
      "Steps (20 / 100): [Loss_D_real: 0.827108, Loss_D_fake: 0.721003, acc: 33.20%] [Loss_G: 0.858675]\n",
      "Steps (30 / 100): [Loss_D_real: 0.784913, Loss_D_fake: 0.651810, acc: 41.80%] [Loss_G: 0.920951]\n",
      "Steps (40 / 100): [Loss_D_real: 0.793054, Loss_D_fake: 0.673423, acc: 41.80%] [Loss_G: 0.897360]\n",
      "Steps (50 / 100): [Loss_D_real: 0.809923, Loss_D_fake: 0.693798, acc: 37.50%] [Loss_G: 0.869655]\n",
      "Steps (60 / 100): [Loss_D_real: 0.770966, Loss_D_fake: 0.734911, acc: 32.03%] [Loss_G: 0.868110]\n",
      "Steps (70 / 100): [Loss_D_real: 0.835180, Loss_D_fake: 0.687950, acc: 37.89%] [Loss_G: 0.829857]\n",
      "Steps (80 / 100): [Loss_D_real: 0.780090, Loss_D_fake: 0.692183, acc: 40.23%] [Loss_G: 0.851772]\n",
      "Steps (90 / 100): [Loss_D_real: 0.743477, Loss_D_fake: 0.664406, acc: 49.61%] [Loss_G: 0.867198]\n",
      "Steps (100 / 100): [Loss_D_real: 0.791470, Loss_D_fake: 0.737402, acc: 31.25%] [Loss_G: 0.859264]\n",
      "EPOCH #  15 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.735568, Loss_D_fake: 0.711085, acc: 44.92%] [Loss_G: 0.831993]\n",
      "Steps (20 / 100): [Loss_D_real: 0.736324, Loss_D_fake: 0.643546, acc: 52.34%] [Loss_G: 0.894171]\n",
      "Steps (30 / 100): [Loss_D_real: 0.741929, Loss_D_fake: 0.716514, acc: 44.92%] [Loss_G: 0.838458]\n",
      "Steps (40 / 100): [Loss_D_real: 0.803626, Loss_D_fake: 0.684261, acc: 41.80%] [Loss_G: 0.814537]\n",
      "Steps (50 / 100): [Loss_D_real: 0.860123, Loss_D_fake: 0.707151, acc: 37.11%] [Loss_G: 0.854173]\n",
      "Steps (60 / 100): [Loss_D_real: 0.853040, Loss_D_fake: 0.691493, acc: 35.94%] [Loss_G: 0.876026]\n",
      "Steps (70 / 100): [Loss_D_real: 0.798008, Loss_D_fake: 0.646343, acc: 42.97%] [Loss_G: 0.870008]\n",
      "Steps (80 / 100): [Loss_D_real: 0.748923, Loss_D_fake: 0.644101, acc: 47.66%] [Loss_G: 0.884674]\n",
      "Steps (90 / 100): [Loss_D_real: 0.708678, Loss_D_fake: 0.678989, acc: 48.44%] [Loss_G: 0.847508]\n",
      "Steps (100 / 100): [Loss_D_real: 0.727521, Loss_D_fake: 0.734187, acc: 38.67%] [Loss_G: 0.801263]\n",
      "EPOCH #  16 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.757186, Loss_D_fake: 0.762406, acc: 35.55%] [Loss_G: 0.795807]\n",
      "Steps (20 / 100): [Loss_D_real: 0.755677, Loss_D_fake: 0.753434, acc: 33.59%] [Loss_G: 0.793462]\n",
      "Steps (30 / 100): [Loss_D_real: 0.737256, Loss_D_fake: 0.660558, acc: 52.34%] [Loss_G: 0.863510]\n",
      "Steps (40 / 100): [Loss_D_real: 0.745548, Loss_D_fake: 0.673625, acc: 41.02%] [Loss_G: 0.864934]\n",
      "Steps (50 / 100): [Loss_D_real: 0.796904, Loss_D_fake: 0.668332, acc: 44.53%] [Loss_G: 0.916118]\n",
      "Steps (60 / 100): [Loss_D_real: 0.750243, Loss_D_fake: 0.697071, acc: 37.50%] [Loss_G: 0.861467]\n",
      "Steps (70 / 100): [Loss_D_real: 0.720554, Loss_D_fake: 0.671085, acc: 44.92%] [Loss_G: 0.845733]\n",
      "Steps (80 / 100): [Loss_D_real: 0.722565, Loss_D_fake: 0.687681, acc: 47.66%] [Loss_G: 0.858784]\n",
      "Steps (90 / 100): [Loss_D_real: 0.772454, Loss_D_fake: 0.698042, acc: 36.72%] [Loss_G: 0.805563]\n",
      "Steps (100 / 100): [Loss_D_real: 0.713784, Loss_D_fake: 0.705323, acc: 42.19%] [Loss_G: 0.813527]\n",
      "EPOCH #  17 --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (10 / 100): [Loss_D_real: 0.737864, Loss_D_fake: 0.704998, acc: 45.70%] [Loss_G: 0.841566]\n",
      "Steps (20 / 100): [Loss_D_real: 0.747791, Loss_D_fake: 0.684978, acc: 46.88%] [Loss_G: 0.832070]\n",
      "Steps (30 / 100): [Loss_D_real: 0.722303, Loss_D_fake: 0.707437, acc: 39.45%] [Loss_G: 0.825197]\n",
      "Steps (40 / 100): [Loss_D_real: 0.751006, Loss_D_fake: 0.647340, acc: 50.39%] [Loss_G: 0.883450]\n",
      "Steps (50 / 100): [Loss_D_real: 0.681653, Loss_D_fake: 0.648672, acc: 55.86%] [Loss_G: 0.864352]\n",
      "Steps (60 / 100): [Loss_D_real: 0.706286, Loss_D_fake: 0.762422, acc: 41.02%] [Loss_G: 0.772904]\n",
      "Steps (70 / 100): [Loss_D_real: 0.718140, Loss_D_fake: 0.733680, acc: 39.06%] [Loss_G: 0.784136]\n",
      "Steps (80 / 100): [Loss_D_real: 0.737021, Loss_D_fake: 0.731492, acc: 38.67%] [Loss_G: 0.820556]\n",
      "Steps (90 / 100): [Loss_D_real: 0.740837, Loss_D_fake: 0.676387, acc: 43.36%] [Loss_G: 0.857782]\n",
      "Steps (100 / 100): [Loss_D_real: 0.722612, Loss_D_fake: 0.661737, acc: 48.83%] [Loss_G: 0.881852]\n",
      "EPOCH #  18 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.693837, Loss_D_fake: 0.718604, acc: 46.09%] [Loss_G: 0.824833]\n",
      "Steps (20 / 100): [Loss_D_real: 0.753962, Loss_D_fake: 0.717372, acc: 44.92%] [Loss_G: 0.840739]\n",
      "Steps (30 / 100): [Loss_D_real: 0.754400, Loss_D_fake: 0.689214, acc: 44.53%] [Loss_G: 0.819294]\n",
      "Steps (40 / 100): [Loss_D_real: 0.726472, Loss_D_fake: 0.667410, acc: 54.69%] [Loss_G: 0.874419]\n",
      "Steps (50 / 100): [Loss_D_real: 0.728482, Loss_D_fake: 0.696357, acc: 44.53%] [Loss_G: 0.842898]\n",
      "Steps (60 / 100): [Loss_D_real: 0.755835, Loss_D_fake: 0.689459, acc: 43.36%] [Loss_G: 0.825580]\n",
      "Steps (70 / 100): [Loss_D_real: 0.720426, Loss_D_fake: 0.710807, acc: 38.67%] [Loss_G: 0.791801]\n",
      "Steps (80 / 100): [Loss_D_real: 0.701718, Loss_D_fake: 0.733303, acc: 43.75%] [Loss_G: 0.794898]\n",
      "Steps (90 / 100): [Loss_D_real: 0.669993, Loss_D_fake: 0.685120, acc: 53.52%] [Loss_G: 0.837462]\n",
      "Steps (100 / 100): [Loss_D_real: 0.691901, Loss_D_fake: 0.638556, acc: 60.94%] [Loss_G: 0.860275]\n",
      "EPOCH #  19 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.680863, Loss_D_fake: 0.680407, acc: 51.56%] [Loss_G: 0.868714]\n",
      "Steps (20 / 100): [Loss_D_real: 0.782343, Loss_D_fake: 0.722187, acc: 37.11%] [Loss_G: 0.825203]\n",
      "Steps (30 / 100): [Loss_D_real: 0.762423, Loss_D_fake: 0.707601, acc: 40.62%] [Loss_G: 0.789248]\n",
      "Steps (40 / 100): [Loss_D_real: 0.704891, Loss_D_fake: 0.665252, acc: 53.12%] [Loss_G: 0.827338]\n",
      "Steps (50 / 100): [Loss_D_real: 0.672375, Loss_D_fake: 0.665974, acc: 59.38%] [Loss_G: 0.829195]\n",
      "Steps (60 / 100): [Loss_D_real: 0.693672, Loss_D_fake: 0.738626, acc: 41.02%] [Loss_G: 0.793926]\n",
      "Steps (70 / 100): [Loss_D_real: 0.755253, Loss_D_fake: 0.707350, acc: 39.06%] [Loss_G: 0.809599]\n",
      "Steps (80 / 100): [Loss_D_real: 0.709771, Loss_D_fake: 0.715036, acc: 42.58%] [Loss_G: 0.829287]\n",
      "Steps (90 / 100): [Loss_D_real: 0.722075, Loss_D_fake: 0.680669, acc: 46.88%] [Loss_G: 0.846984]\n",
      "Steps (100 / 100): [Loss_D_real: 0.742702, Loss_D_fake: 0.630040, acc: 55.86%] [Loss_G: 0.884318]\n",
      "EPOCH #  20 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.716066, Loss_D_fake: 0.651318, acc: 51.95%] [Loss_G: 0.879457]\n",
      "Steps (20 / 100): [Loss_D_real: 0.697014, Loss_D_fake: 0.676336, acc: 53.12%] [Loss_G: 0.848317]\n",
      "Steps (30 / 100): [Loss_D_real: 0.701185, Loss_D_fake: 0.680654, acc: 53.52%] [Loss_G: 0.846797]\n",
      "Steps (40 / 100): [Loss_D_real: 0.721335, Loss_D_fake: 0.654703, acc: 48.83%] [Loss_G: 0.849003]\n",
      "Steps (50 / 100): [Loss_D_real: 0.749277, Loss_D_fake: 0.709937, acc: 41.80%] [Loss_G: 0.814637]\n",
      "Steps (60 / 100): [Loss_D_real: 0.741866, Loss_D_fake: 0.677873, acc: 44.14%] [Loss_G: 0.814589]\n",
      "Steps (70 / 100): [Loss_D_real: 0.716230, Loss_D_fake: 0.650329, acc: 53.91%] [Loss_G: 0.883314]\n",
      "Steps (80 / 100): [Loss_D_real: 0.634377, Loss_D_fake: 0.653342, acc: 60.55%] [Loss_G: 0.894452]\n",
      "Steps (90 / 100): [Loss_D_real: 0.714941, Loss_D_fake: 0.671652, acc: 51.17%] [Loss_G: 0.860422]\n",
      "Steps (100 / 100): [Loss_D_real: 0.698666, Loss_D_fake: 0.696949, acc: 52.34%] [Loss_G: 0.825170]\n"
     ]
    }
   ],
   "source": [
    "def discriminator(encoder):\n",
    "\n",
    "    data = Input(shape=(data_dim,))\n",
    "    x = encoder(data)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid', kernel_initializer=weight_init)(x)\n",
    "\n",
    "    model = Model(inputs=data, outputs=out)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generator(decoder):\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "\n",
    "    generated_feature = decoder(noise)\n",
    "    model = Model(inputs=noise, outputs=generated_feature)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_G(generator, discriminator):\n",
    "    # Freeze the discriminator when training generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "D = discriminator(en)\n",
    "G = generator(de)\n",
    "\n",
    "D.summary()\n",
    "G.summary()\n",
    "\n",
    "x_train = df_train.drop(target, axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target, axis=1)\n",
    "y_test = df_test[target]\n",
    "\n",
    "gan = GAN(g_model=G, d_model=D)\n",
    "\n",
    "EPOCHS = 20\n",
    "X_train = x_train.to_numpy()\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH # ', epoch + 1, '-' * 50)\n",
    "    gan.train(X_train, batch_size=128, steps_per_epoch=100)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        gan.generator.save('gan_pre_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "noise = np.random.normal(0, 1, size=(1000, 32))\n",
    "fake_data = gan.generator.predict(noise)\n",
    "decoded_feature = pd.DataFrame(fake_data)\n",
    "\n",
    "def boxplot_compare(df1, df2,title):\n",
    "  fig, ax = plt.subplots(figsize=(16,10))\n",
    "  bp1 = df1.boxplot(color='green', showfliers=False)\n",
    "  bp2 = df2.boxplot(color='purple', showfliers=False)\n",
    "\n",
    "  patch1 = mpatches.Patch(color='green', label='Original')\n",
    "  patch2 = mpatches.Patch(color='purple', label='GAN with Pre-Train')\n",
    "  plt.legend(handles=[patch1, patch2], prop={'size': 16})\n",
    "  ax.set_title(title)\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df['Time'] = (df['Time'].values / 3600)\n",
    "df['Amount'] = np.log10(df['Amount'].values + 1)\n",
    "\n",
    "feature = df.loc[:, df.columns != 'Class']\n",
    "true = df.loc[df['Class'] == 0]\n",
    "fraud = df.loc[df['Class'] == 1]\n",
    "true = true.loc[:, df.columns != 'Class']\n",
    "fraud = fraud.loc[:, df.columns != 'Class']\n",
    "\n",
    "boxplot_compare(fraud, df_gen_1000, 'Original Data Distribution versus GAN with Pre-Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_raw, train_size=0.8, random_state=42, stratify=df_raw[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test  = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "x_train = df_train.drop(target,axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target,axis=1)\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(generator, n_data):\n",
    "    noise = np.random.normal(0, 1, size=(n_data, latent_dim))\n",
    "    gen = generator.predict(noise)\n",
    "    x_train_gen = np.concatenate((x_train, gen))\n",
    "    y_gen = np.array(gen.shape[0] * [1])\n",
    "    y_train_gen = np.concatenate((y_train, y_gen))\n",
    "    return gen, x_train_gen, y_train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 more fraud\n",
    "gen_1000, x_train_gen_1000, y_train_gen_1000 = gen_data(gan.generator, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.142359</td>\n",
       "      <td>0.306394</td>\n",
       "      <td>-0.964754</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.275139</td>\n",
       "      <td>-0.468433</td>\n",
       "      <td>-0.108040</td>\n",
       "      <td>-0.972443</td>\n",
       "      <td>0.964377</td>\n",
       "      <td>-0.806843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247465</td>\n",
       "      <td>-0.045016</td>\n",
       "      <td>-0.592045</td>\n",
       "      <td>0.030859</td>\n",
       "      <td>0.233501</td>\n",
       "      <td>-0.121944</td>\n",
       "      <td>-0.010361</td>\n",
       "      <td>0.199061</td>\n",
       "      <td>-0.178766</td>\n",
       "      <td>1.933946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.761893</td>\n",
       "      <td>1.315714</td>\n",
       "      <td>1.198780</td>\n",
       "      <td>1.461240</td>\n",
       "      <td>1.445078</td>\n",
       "      <td>1.252516</td>\n",
       "      <td>1.311620</td>\n",
       "      <td>0.985449</td>\n",
       "      <td>1.024989</td>\n",
       "      <td>1.343331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553738</td>\n",
       "      <td>0.573271</td>\n",
       "      <td>0.515936</td>\n",
       "      <td>0.529107</td>\n",
       "      <td>0.518022</td>\n",
       "      <td>0.594324</td>\n",
       "      <td>0.417454</td>\n",
       "      <td>0.326081</td>\n",
       "      <td>0.423260</td>\n",
       "      <td>0.980971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.493851</td>\n",
       "      <td>-4.520346</td>\n",
       "      <td>-5.142768</td>\n",
       "      <td>-4.620116</td>\n",
       "      <td>-4.275031</td>\n",
       "      <td>-5.627410</td>\n",
       "      <td>-5.021013</td>\n",
       "      <td>-4.511738</td>\n",
       "      <td>-3.555690</td>\n",
       "      <td>-5.189088</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.467688</td>\n",
       "      <td>-1.913082</td>\n",
       "      <td>-2.371645</td>\n",
       "      <td>-1.720002</td>\n",
       "      <td>-1.425401</td>\n",
       "      <td>-2.636009</td>\n",
       "      <td>-1.238614</td>\n",
       "      <td>-0.863776</td>\n",
       "      <td>-1.697757</td>\n",
       "      <td>-1.070016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.263146</td>\n",
       "      <td>-0.540419</td>\n",
       "      <td>-1.698590</td>\n",
       "      <td>-0.377976</td>\n",
       "      <td>-0.742805</td>\n",
       "      <td>-1.245923</td>\n",
       "      <td>-0.911415</td>\n",
       "      <td>-1.586458</td>\n",
       "      <td>0.315522</td>\n",
       "      <td>-1.658486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154287</td>\n",
       "      <td>-0.388089</td>\n",
       "      <td>-0.932916</td>\n",
       "      <td>-0.272446</td>\n",
       "      <td>-0.112103</td>\n",
       "      <td>-0.524040</td>\n",
       "      <td>-0.282359</td>\n",
       "      <td>-0.027797</td>\n",
       "      <td>-0.456966</td>\n",
       "      <td>1.333754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.759063</td>\n",
       "      <td>0.357736</td>\n",
       "      <td>-0.866250</td>\n",
       "      <td>0.643730</td>\n",
       "      <td>0.215677</td>\n",
       "      <td>-0.448610</td>\n",
       "      <td>-0.081174</td>\n",
       "      <td>-0.939859</td>\n",
       "      <td>0.962229</td>\n",
       "      <td>-0.726060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234153</td>\n",
       "      <td>-0.016724</td>\n",
       "      <td>-0.572888</td>\n",
       "      <td>0.064068</td>\n",
       "      <td>0.237391</td>\n",
       "      <td>-0.118631</td>\n",
       "      <td>-0.000572</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>-0.159824</td>\n",
       "      <td>1.927577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.764905</td>\n",
       "      <td>1.231412</td>\n",
       "      <td>-0.145363</td>\n",
       "      <td>1.513784</td>\n",
       "      <td>1.228732</td>\n",
       "      <td>0.354678</td>\n",
       "      <td>0.759541</td>\n",
       "      <td>-0.303573</td>\n",
       "      <td>1.615206</td>\n",
       "      <td>0.061332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613763</td>\n",
       "      <td>0.323750</td>\n",
       "      <td>-0.224797</td>\n",
       "      <td>0.395856</td>\n",
       "      <td>0.566986</td>\n",
       "      <td>0.274827</td>\n",
       "      <td>0.259774</td>\n",
       "      <td>0.418251</td>\n",
       "      <td>0.108905</td>\n",
       "      <td>2.542723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79.543701</td>\n",
       "      <td>3.904326</td>\n",
       "      <td>2.224326</td>\n",
       "      <td>4.618461</td>\n",
       "      <td>4.931416</td>\n",
       "      <td>3.695782</td>\n",
       "      <td>4.316472</td>\n",
       "      <td>1.730675</td>\n",
       "      <td>4.203502</td>\n",
       "      <td>2.824570</td>\n",
       "      <td>...</td>\n",
       "      <td>2.666792</td>\n",
       "      <td>1.905602</td>\n",
       "      <td>1.103323</td>\n",
       "      <td>1.542134</td>\n",
       "      <td>1.752344</td>\n",
       "      <td>1.892429</td>\n",
       "      <td>1.532631</td>\n",
       "      <td>1.165525</td>\n",
       "      <td>1.363685</td>\n",
       "      <td>5.454622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     27.142359     0.306394    -0.964754     0.570263     0.275139   \n",
       "std      13.761893     1.315714     1.198780     1.461240     1.445078   \n",
       "min      -3.493851    -4.520346    -5.142768    -4.620116    -4.275031   \n",
       "25%      17.263146    -0.540419    -1.698590    -0.377976    -0.742805   \n",
       "50%      25.759063     0.357736    -0.866250     0.643730     0.215677   \n",
       "75%      35.764905     1.231412    -0.145363     1.513784     1.228732   \n",
       "max      79.543701     3.904326     2.224326     4.618461     4.931416   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -0.468433    -0.108040    -0.972443     0.964377    -0.806843  ...   \n",
       "std       1.252516     1.311620     0.985449     1.024989     1.343331  ...   \n",
       "min      -5.627410    -5.021013    -4.511738    -3.555690    -5.189088  ...   \n",
       "25%      -1.245923    -0.911415    -1.586458     0.315522    -1.658486  ...   \n",
       "50%      -0.448610    -0.081174    -0.939859     0.962229    -0.726060  ...   \n",
       "75%       0.354678     0.759541    -0.303573     1.615206     0.061332  ...   \n",
       "max       3.695782     4.316472     1.730675     4.203502     2.824570  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.247465    -0.045016    -0.592045     0.030859     0.233501   \n",
       "std       0.553738     0.573271     0.515936     0.529107     0.518022   \n",
       "min      -1.467688    -1.913082    -2.371645    -1.720002    -1.425401   \n",
       "25%      -0.154287    -0.388089    -0.932916    -0.272446    -0.112103   \n",
       "50%       0.234153    -0.016724    -0.572888     0.064068     0.237391   \n",
       "75%       0.613763     0.323750    -0.224797     0.395856     0.566986   \n",
       "max       2.666792     1.905602     1.103323     1.542134     1.752344   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.121944    -0.010361     0.199061    -0.178766     1.933946  \n",
       "std       0.594324     0.417454     0.326081     0.423260     0.980971  \n",
       "min      -2.636009    -1.238614    -0.863776    -1.697757    -1.070016  \n",
       "25%      -0.524040    -0.282359    -0.027797    -0.456966     1.333754  \n",
       "50%      -0.118631    -0.000572     0.203774    -0.159824     1.927577  \n",
       "75%       0.274827     0.259774     0.418251     0.108905     2.542723  \n",
       "max       1.892429     1.532631     1.165525     1.363685     5.454622  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gen_1000 = pd.DataFrame(data=gen_1000, index=None, columns=x_train.columns)\n",
    "df_gen_1000.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.647787</td>\n",
       "      <td>0.231930</td>\n",
       "      <td>-0.875528</td>\n",
       "      <td>0.603297</td>\n",
       "      <td>0.279646</td>\n",
       "      <td>-0.488107</td>\n",
       "      <td>-0.146655</td>\n",
       "      <td>-0.932475</td>\n",
       "      <td>0.943588</td>\n",
       "      <td>-0.880007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225911</td>\n",
       "      <td>-0.038430</td>\n",
       "      <td>-0.580500</td>\n",
       "      <td>0.033036</td>\n",
       "      <td>0.228251</td>\n",
       "      <td>-0.127186</td>\n",
       "      <td>0.018014</td>\n",
       "      <td>0.200446</td>\n",
       "      <td>-0.163753</td>\n",
       "      <td>1.902195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.634622</td>\n",
       "      <td>1.334839</td>\n",
       "      <td>1.157669</td>\n",
       "      <td>1.434891</td>\n",
       "      <td>1.456608</td>\n",
       "      <td>1.201575</td>\n",
       "      <td>1.277024</td>\n",
       "      <td>0.948309</td>\n",
       "      <td>1.008386</td>\n",
       "      <td>1.347821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538781</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.506478</td>\n",
       "      <td>0.524036</td>\n",
       "      <td>0.536711</td>\n",
       "      <td>0.588537</td>\n",
       "      <td>0.415439</td>\n",
       "      <td>0.331158</td>\n",
       "      <td>0.420293</td>\n",
       "      <td>0.983758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.885800</td>\n",
       "      <td>-5.783127</td>\n",
       "      <td>-7.233293</td>\n",
       "      <td>-5.981014</td>\n",
       "      <td>-5.869552</td>\n",
       "      <td>-7.559361</td>\n",
       "      <td>-5.951636</td>\n",
       "      <td>-5.919608</td>\n",
       "      <td>-3.733746</td>\n",
       "      <td>-7.825546</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.083735</td>\n",
       "      <td>-2.632096</td>\n",
       "      <td>-3.107893</td>\n",
       "      <td>-3.036778</td>\n",
       "      <td>-2.109868</td>\n",
       "      <td>-3.090655</td>\n",
       "      <td>-2.162573</td>\n",
       "      <td>-1.323745</td>\n",
       "      <td>-2.241807</td>\n",
       "      <td>-1.919243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.709303</td>\n",
       "      <td>-0.648644</td>\n",
       "      <td>-1.599144</td>\n",
       "      <td>-0.348247</td>\n",
       "      <td>-0.696015</td>\n",
       "      <td>-1.258976</td>\n",
       "      <td>-0.993099</td>\n",
       "      <td>-1.559993</td>\n",
       "      <td>0.276814</td>\n",
       "      <td>-1.758152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138330</td>\n",
       "      <td>-0.395160</td>\n",
       "      <td>-0.912496</td>\n",
       "      <td>-0.275500</td>\n",
       "      <td>-0.132698</td>\n",
       "      <td>-0.513529</td>\n",
       "      <td>-0.255119</td>\n",
       "      <td>-0.018428</td>\n",
       "      <td>-0.436358</td>\n",
       "      <td>1.229023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.340321</td>\n",
       "      <td>0.257215</td>\n",
       "      <td>-0.797923</td>\n",
       "      <td>0.638160</td>\n",
       "      <td>0.264003</td>\n",
       "      <td>-0.444768</td>\n",
       "      <td>-0.144228</td>\n",
       "      <td>-0.921167</td>\n",
       "      <td>0.952686</td>\n",
       "      <td>-0.823903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204265</td>\n",
       "      <td>-0.030279</td>\n",
       "      <td>-0.561586</td>\n",
       "      <td>0.070117</td>\n",
       "      <td>0.213413</td>\n",
       "      <td>-0.120299</td>\n",
       "      <td>0.018551</td>\n",
       "      <td>0.202215</td>\n",
       "      <td>-0.153932</td>\n",
       "      <td>1.886046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.226147</td>\n",
       "      <td>1.144628</td>\n",
       "      <td>-0.077686</td>\n",
       "      <td>1.591241</td>\n",
       "      <td>1.241282</td>\n",
       "      <td>0.334163</td>\n",
       "      <td>0.700207</td>\n",
       "      <td>-0.296821</td>\n",
       "      <td>1.623720</td>\n",
       "      <td>0.050747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564589</td>\n",
       "      <td>0.324716</td>\n",
       "      <td>-0.230532</td>\n",
       "      <td>0.385735</td>\n",
       "      <td>0.574437</td>\n",
       "      <td>0.266331</td>\n",
       "      <td>0.290667</td>\n",
       "      <td>0.421129</td>\n",
       "      <td>0.119014</td>\n",
       "      <td>2.553814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.744667</td>\n",
       "      <td>6.170323</td>\n",
       "      <td>3.424903</td>\n",
       "      <td>6.728444</td>\n",
       "      <td>8.294473</td>\n",
       "      <td>4.559908</td>\n",
       "      <td>6.230343</td>\n",
       "      <td>3.820824</td>\n",
       "      <td>5.622160</td>\n",
       "      <td>5.620726</td>\n",
       "      <td>...</td>\n",
       "      <td>3.433274</td>\n",
       "      <td>2.644539</td>\n",
       "      <td>1.429180</td>\n",
       "      <td>2.190902</td>\n",
       "      <td>3.343303</td>\n",
       "      <td>2.486927</td>\n",
       "      <td>1.961641</td>\n",
       "      <td>1.833407</td>\n",
       "      <td>1.599184</td>\n",
       "      <td>7.219814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean       26.647787       0.231930      -0.875528       0.603297   \n",
       "std        13.634622       1.334839       1.157669       1.434891   \n",
       "min        -8.885800      -5.783127      -7.233293      -5.981014   \n",
       "25%        16.709303      -0.648644      -1.599144      -0.348247   \n",
       "50%        25.340321       0.257215      -0.797923       0.638160   \n",
       "75%        35.226147       1.144628      -0.077686       1.591241   \n",
       "max       100.744667       6.170323       3.424903       6.728444   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean        0.279646      -0.488107      -0.146655      -0.932475   \n",
       "std         1.456608       1.201575       1.277024       0.948309   \n",
       "min        -5.869552      -7.559361      -5.951636      -5.919608   \n",
       "25%        -0.696015      -1.258976      -0.993099      -1.559993   \n",
       "50%         0.264003      -0.444768      -0.144228      -0.921167   \n",
       "75%         1.241282       0.334163       0.700207      -0.296821   \n",
       "max         8.294473       4.559908       6.230343       3.820824   \n",
       "\n",
       "                  V8             V9  ...            V20            V21  \\\n",
       "count  227057.000000  227057.000000  ...  227057.000000  227057.000000   \n",
       "mean        0.943588      -0.880007  ...       0.225911      -0.038430   \n",
       "std         1.008386       1.347821  ...       0.538781       0.542631   \n",
       "min        -3.733746      -7.825546  ...      -2.083735      -2.632096   \n",
       "25%         0.276814      -1.758152  ...      -0.138330      -0.395160   \n",
       "50%         0.952686      -0.823903  ...       0.204265      -0.030279   \n",
       "75%         1.623720       0.050747  ...       0.564589       0.324716   \n",
       "max         5.622160       5.620726  ...       3.433274       2.644539   \n",
       "\n",
       "                 V22            V23            V24            V25  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean       -0.580500       0.033036       0.228251      -0.127186   \n",
       "std         0.506478       0.524036       0.536711       0.588537   \n",
       "min        -3.107893      -3.036778      -2.109868      -3.090655   \n",
       "25%        -0.912496      -0.275500      -0.132698      -0.513529   \n",
       "50%        -0.561586       0.070117       0.213413      -0.120299   \n",
       "75%        -0.230532       0.385735       0.574437       0.266331   \n",
       "max         1.429180       2.190902       3.343303       2.486927   \n",
       "\n",
       "                 V26            V27            V28         Amount  \n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000  \n",
       "mean        0.018014       0.200446      -0.163753       1.902195  \n",
       "std         0.415439       0.331158       0.420293       0.983758  \n",
       "min        -2.162573      -1.323745      -2.241807      -1.919243  \n",
       "25%        -0.255119      -0.018428      -0.436358       1.229023  \n",
       "50%         0.018551       0.202215      -0.153932       1.886046  \n",
       "75%         0.290667       0.421129       0.119014       2.553814  \n",
       "max         1.961641       1.833407       1.599184       7.219814  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 227451 -394 = 227057  (total of x_train - fraud in x_train)\n",
    "gen_227057, x_train_gen_227057, y_train_gen_227057 = gen_data(gan.generator, 227057)\n",
    "\n",
    "df_gen_227057 = pd.DataFrame(data=gen_227057, index=None, columns=x_train.columns)\n",
    "df_gen_227057.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.008938</td>\n",
       "      <td>-4.707808</td>\n",
       "      <td>3.588729</td>\n",
       "      <td>-7.068378</td>\n",
       "      <td>4.592975</td>\n",
       "      <td>-3.101629</td>\n",
       "      <td>-1.387192</td>\n",
       "      <td>-5.539909</td>\n",
       "      <td>0.587920</td>\n",
       "      <td>-2.589654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358018</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.051318</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>-0.109108</td>\n",
       "      <td>0.019602</td>\n",
       "      <td>0.047827</td>\n",
       "      <td>0.155933</td>\n",
       "      <td>0.077212</td>\n",
       "      <td>1.228674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.347935</td>\n",
       "      <td>6.841390</td>\n",
       "      <td>4.309436</td>\n",
       "      <td>7.166449</td>\n",
       "      <td>2.883467</td>\n",
       "      <td>5.406586</td>\n",
       "      <td>1.864770</td>\n",
       "      <td>7.316745</td>\n",
       "      <td>6.676697</td>\n",
       "      <td>2.495584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384017</td>\n",
       "      <td>3.750615</td>\n",
       "      <td>1.457801</td>\n",
       "      <td>1.681228</td>\n",
       "      <td>0.509477</td>\n",
       "      <td>0.826820</td>\n",
       "      <td>0.467046</td>\n",
       "      <td>1.358987</td>\n",
       "      <td>0.555106</td>\n",
       "      <td>0.965996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.239444</td>\n",
       "      <td>-30.552380</td>\n",
       "      <td>-8.402154</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>-1.313275</td>\n",
       "      <td>-22.105532</td>\n",
       "      <td>-5.773192</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.128186</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-19.254328</td>\n",
       "      <td>-2.028024</td>\n",
       "      <td>-4.781606</td>\n",
       "      <td>-1.152671</td>\n",
       "      <td>-7.263482</td>\n",
       "      <td>-1.869290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.500278</td>\n",
       "      <td>-5.996596</td>\n",
       "      <td>1.229209</td>\n",
       "      <td>-8.436924</td>\n",
       "      <td>2.419178</td>\n",
       "      <td>-4.741036</td>\n",
       "      <td>-2.504633</td>\n",
       "      <td>-7.765017</td>\n",
       "      <td>-0.135707</td>\n",
       "      <td>-3.828323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181718</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>-0.515338</td>\n",
       "      <td>-0.330293</td>\n",
       "      <td>-0.445282</td>\n",
       "      <td>-0.312004</td>\n",
       "      <td>-0.253693</td>\n",
       "      <td>-0.025894</td>\n",
       "      <td>-0.096541</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.393056</td>\n",
       "      <td>-2.272114</td>\n",
       "      <td>2.662472</td>\n",
       "      <td>-5.133485</td>\n",
       "      <td>4.258196</td>\n",
       "      <td>-1.522962</td>\n",
       "      <td>-1.421577</td>\n",
       "      <td>-2.926216</td>\n",
       "      <td>0.642565</td>\n",
       "      <td>-2.230097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280862</td>\n",
       "      <td>0.576441</td>\n",
       "      <td>0.073696</td>\n",
       "      <td>-0.057241</td>\n",
       "      <td>-0.060269</td>\n",
       "      <td>0.088371</td>\n",
       "      <td>-0.003464</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>0.147380</td>\n",
       "      <td>1.007318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.912917</td>\n",
       "      <td>-0.410418</td>\n",
       "      <td>4.737900</td>\n",
       "      <td>-2.302626</td>\n",
       "      <td>6.390866</td>\n",
       "      <td>0.240184</td>\n",
       "      <td>-0.361122</td>\n",
       "      <td>-0.900824</td>\n",
       "      <td>1.743587</td>\n",
       "      <td>-0.825345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783528</td>\n",
       "      <td>1.204214</td>\n",
       "      <td>0.615344</td>\n",
       "      <td>0.307132</td>\n",
       "      <td>0.274014</td>\n",
       "      <td>0.441670</td>\n",
       "      <td>0.393148</td>\n",
       "      <td>0.779267</td>\n",
       "      <td>0.372389</td>\n",
       "      <td>2.028937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.318889</td>\n",
       "      <td>2.132386</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>2.250210</td>\n",
       "      <td>12.114672</td>\n",
       "      <td>11.095089</td>\n",
       "      <td>6.474115</td>\n",
       "      <td>5.802537</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>...</td>\n",
       "      <td>11.059004</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>5.466230</td>\n",
       "      <td>0.994110</td>\n",
       "      <td>2.208209</td>\n",
       "      <td>2.745261</td>\n",
       "      <td>3.052358</td>\n",
       "      <td>1.779364</td>\n",
       "      <td>3.327741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time          V1          V2          V3          V4          V5  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean    23.008938   -4.707808    3.588729   -7.068378    4.592975   -3.101629   \n",
       "std     13.347935    6.841390    4.309436    7.166449    2.883467    5.406586   \n",
       "min      1.239444  -30.552380   -8.402154  -31.103685   -1.313275  -22.105532   \n",
       "25%     11.500278   -5.996596    1.229209   -8.436924    2.419178   -4.741036   \n",
       "50%     21.393056   -2.272114    2.662472   -5.133485    4.258196   -1.522962   \n",
       "75%     35.912917   -0.410418    4.737900   -2.302626    6.390866    0.240184   \n",
       "max     47.318889    2.132386   22.057729    2.250210   12.114672   11.095089   \n",
       "\n",
       "               V6          V7          V8          V9  ...         V20  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  ...  394.000000   \n",
       "mean    -1.387192   -5.539909    0.587920   -2.589654  ...    0.358018   \n",
       "std      1.864770    7.316745    6.676697    2.495584  ...    1.384017   \n",
       "min     -5.773192  -43.557242  -41.044261  -13.434066  ...   -4.128186   \n",
       "25%     -2.504633   -7.765017   -0.135707   -3.828323  ...   -0.181718   \n",
       "50%     -1.421577   -2.926216    0.642565   -2.230097  ...    0.280862   \n",
       "75%     -0.361122   -0.900824    1.743587   -0.825345  ...    0.783528   \n",
       "max      6.474115    5.802537   20.007208    3.353525  ...   11.059004   \n",
       "\n",
       "              V21         V22         V23         V24         V25         V26  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean     0.628814    0.051318   -0.062790   -0.109108    0.019602    0.047827   \n",
       "std      3.750615    1.457801    1.681228    0.509477    0.826820    0.467046   \n",
       "min    -22.797604   -8.887017  -19.254328   -2.028024   -4.781606   -1.152671   \n",
       "25%      0.040122   -0.515338   -0.330293   -0.445282   -0.312004   -0.253693   \n",
       "50%      0.576441    0.073696   -0.057241   -0.060269    0.088371   -0.003464   \n",
       "75%      1.204214    0.615344    0.307132    0.274014    0.441670    0.393148   \n",
       "max     27.202839    8.361985    5.466230    0.994110    2.208209    2.745261   \n",
       "\n",
       "              V27         V28      Amount  \n",
       "count  394.000000  394.000000  394.000000  \n",
       "mean     0.155933    0.077212    1.228674  \n",
       "std      1.358987    0.555106    0.965996  \n",
       "min     -7.263482   -1.869290    0.000000  \n",
       "25%     -0.025894   -0.096541    0.301030  \n",
       "50%      0.394926    0.147380    1.007318  \n",
       "75%      0.779267    0.372389    2.028937  \n",
       "max      3.052358    1.779364    3.327741  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_fraud = x_train[y_train==1]\n",
    "x_train_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:86: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# with GPU\n",
    "ros = RandomOverSampler()\n",
    "x, y = ros.fit_sample(x_train_gen_1000, y_train_gen_1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with GPU\n",
    "ros = RandomOverSampler()\n",
    "def XGBC_model_predit(x, y):   \n",
    "    x, y = ros.fit_sample(x, y)\n",
    "    clf_xgb_os = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
    "              gamma=0.2, gpu_id=0, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.02, max_delta_step=0,\n",
    "              max_depth=10, min_child_weight=2,\n",
    "              monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
    "              n_estimators=800, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "              tree_method='gpu_hist', validate_parameters=1)\n",
    "    \n",
    "    clf_xgb_os.fit(x, y)  \n",
    "    y_pred_gen_os = clf_xgb_os.predict(x_test.to_numpy())\n",
    "    return y_pred_gen_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(y_test, y_pred):\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1 score: ', f1_score(y_test, y_pred))\n",
    "    print('ROC AUC score: ',  roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "import scikitplot as skplt\n",
    "def plot_cm(y_test, y_pred, title):\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, y_pred,figsize=(8,8))\n",
    "    plt.title('Confusion Matrix ' + title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:86: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995435553526912\n",
      "Precision:  0.875\n",
      "Recall:  0.8571428571428571\n",
      "F1 score:  0.8659793814432989\n",
      "ROC AUC score:  0.9284659136586543\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_1000 = XGBC_model_predit(x_train_gen_1000, y_train_gen_1000)\n",
    "check_performance(y_test, y_pred_gen_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_1000, 'Adding GAN 1000 Fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
