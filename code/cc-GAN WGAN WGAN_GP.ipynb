{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN WGAN WGAN_GP   \n",
    "by Hao Ning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/creditcardfraud/creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "import scikitplot as skplt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>2.177883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.567026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000278</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>2.579395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000278</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>2.095169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000556</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>1.851197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  0.000000 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1  0.000000  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2  0.000278 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3  0.000278 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4  0.000556 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  2.177883      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724  0.567026      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  2.579395      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  2.095169      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153  1.851197      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the transformation, time transformed from sec to hour\n",
    "df_raw['Amount'] = np.log10( df_raw['Amount'].values + 1 )\n",
    "df_raw['Time'] = df_raw['Time'].values/3600 \n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test\n",
    "stratify target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_raw, train_size=0.8, random_state=42, stratify=df_raw[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test  = df_train.reset_index(drop=True), df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 31) (56962, 31)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    227451\n",
       "1       394\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.drop(target,axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target,axis=1)\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 30) (227845,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_fraud = x_train[y_train==1]\n",
    "x_train_fraud.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model & Performance Evaluation Functions\n",
    "### Parameters obtained from Base Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_xug_us = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#             colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
    "#             gamma=0.2, gpu_id=-1, importance_type='gain',\n",
    "#             interaction_constraints='', learning_rate=0.02, max_delta_step=0,\n",
    "#             max_depth=6, min_child_weight=2, \n",
    "#             monotone_constraints='()', n_estimators=600, n_jobs=-1,\n",
    "#             num_parallel_tree=1, random_state=42, reg_alpha=0, reg_lambda=1,\n",
    "#             scale_pos_weight=1, subsample=0.7, tree_method='exact',\n",
    "#             validate_parameters=1, verbosity=None)\n",
    "\n",
    "\n",
    "# clf_xug_us.fit(x_train_us, y_train_us)   \n",
    "# y_pred_us = clf_xug_us.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(y_test, y_pred):\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1 score: ', f1_score(y_test, y_pred))\n",
    "    print('ROC AUC score: ',  roc_auc_score(y_test, y_pred))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(y_test, y_pred, title):\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, y_pred,figsize=(8,8))\n",
    "    plt.title('Confusion Matrix ' + title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, multiply, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras import layers\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 49,153\n",
      "Trainable params: 49,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 51,166\n",
      "Trainable params: 51,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim= 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer = Adam(lr=0.00001)\n",
    "\n",
    "# %% --------------------------------------- Set Seeds -----------------------------------------------------------------\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "weight_init = glorot_normal(seed=SEED)\n",
    "\n",
    "# %% --------------------------------------- G D-----------------------------------------------------------------\n",
    "def generator():\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(noise)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(256, kernel_initializer=weight_init)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # tanh is removed since we are not dealing with normalized image data    \n",
    "    out = Dense(data_dim, kernel_initializer=weight_init)(x)\n",
    "    \n",
    "    model = Model(inputs=noise, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator():\n",
    "    data = Input(shape=data_dim)\n",
    "    x = Dense(256, kernel_initializer=weight_init)(data)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "#    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "#    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "#    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    out = Dense(1, activation='sigmoid', kernel_initializer=weight_init)(x)\n",
    "\n",
    "    model = Model(inputs=data, outputs=out)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_G(generator, discriminator):\n",
    "    # Freeze the discriminator when training generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "# %% ----------------------------------- GAN ----------------------------------------------------------------------\n",
    "# modified from https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self, g_model, d_model):\n",
    "        self.z = latent_dim\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.generator = g_model\n",
    "        self.discriminator = d_model\n",
    "\n",
    "        self.train_G = train_G(self.generator, self.discriminator)\n",
    "        self.loss_D, self.loss_G = [], []\n",
    "\n",
    "    def train(self, data, batch_size=128, steps_per_epoch=50):    \n",
    "\n",
    "        for epoch in range(steps_per_epoch):\n",
    "            # Select a random batch of transactions data \n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            real_data = data[idx]\n",
    "\n",
    "            # generate a batch of new data\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            fake_data= self.generator.predict(noise)\n",
    "\n",
    "            # Train D\n",
    "            loss_real = self.discriminator.train_on_batch(real_data, np.ones(batch_size))\n",
    "            loss_fake = self.discriminator.train_on_batch(fake_data, np.zeros(batch_size))\n",
    "            self.loss_D.append(0.5 * np.add(loss_fake, loss_real))\n",
    "\n",
    "            # Train G\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            loss_G = self.train_G.train_on_batch(noise, np.ones(batch_size))\n",
    "            self.loss_G.append(loss_G)\n",
    "\n",
    "            if (epoch + 1) * 10 % steps_per_epoch == 0:\n",
    "                print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f, acc: %.2f%%] [Loss_G: %f]' %\n",
    "                  (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], 100*self.loss_D[-1][1], loss_G))\n",
    "\n",
    "        return\n",
    "\n",
    "D = discriminator()\n",
    "G = generator()\n",
    "\n",
    "D.summary()\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #  1 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.963686, Loss_D_fake: 0.700344, acc: 45.31%] [Loss_G: 0.691058]\n",
      "Steps (20 / 100): [Loss_D_real: 0.738024, Loss_D_fake: 0.710239, acc: 45.70%] [Loss_G: 0.679139]\n",
      "Steps (30 / 100): [Loss_D_real: 0.498922, Loss_D_fake: 0.719145, acc: 48.05%] [Loss_G: 0.665071]\n",
      "Steps (40 / 100): [Loss_D_real: 0.365797, Loss_D_fake: 0.727036, acc: 48.83%] [Loss_G: 0.660530]\n",
      "Steps (50 / 100): [Loss_D_real: 0.262502, Loss_D_fake: 0.736440, acc: 53.52%] [Loss_G: 0.663346]\n",
      "Steps (60 / 100): [Loss_D_real: 0.189305, Loss_D_fake: 0.755222, acc: 52.34%] [Loss_G: 0.640558]\n",
      "Steps (70 / 100): [Loss_D_real: 0.132081, Loss_D_fake: 0.760665, acc: 52.34%] [Loss_G: 0.635177]\n",
      "Steps (80 / 100): [Loss_D_real: 0.151776, Loss_D_fake: 0.761376, acc: 51.95%] [Loss_G: 0.625603]\n",
      "Steps (90 / 100): [Loss_D_real: 0.146473, Loss_D_fake: 0.774640, acc: 50.78%] [Loss_G: 0.624140]\n",
      "Steps (100 / 100): [Loss_D_real: 0.149267, Loss_D_fake: 0.779554, acc: 50.00%] [Loss_G: 0.615288]\n",
      "EPOCH #  2 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.083585, Loss_D_fake: 0.793124, acc: 50.78%] [Loss_G: 0.613973]\n",
      "Steps (20 / 100): [Loss_D_real: 0.119746, Loss_D_fake: 0.797481, acc: 49.61%] [Loss_G: 0.604923]\n",
      "Steps (30 / 100): [Loss_D_real: 0.107802, Loss_D_fake: 0.800984, acc: 48.44%] [Loss_G: 0.607199]\n",
      "Steps (40 / 100): [Loss_D_real: 0.085968, Loss_D_fake: 0.804355, acc: 50.00%] [Loss_G: 0.587990]\n",
      "Steps (50 / 100): [Loss_D_real: 0.093862, Loss_D_fake: 0.814709, acc: 49.22%] [Loss_G: 0.599507]\n",
      "Steps (60 / 100): [Loss_D_real: 0.055436, Loss_D_fake: 0.815513, acc: 50.00%] [Loss_G: 0.598110]\n",
      "Steps (70 / 100): [Loss_D_real: 0.069467, Loss_D_fake: 0.816274, acc: 50.00%] [Loss_G: 0.592350]\n",
      "Steps (80 / 100): [Loss_D_real: 0.073591, Loss_D_fake: 0.806237, acc: 50.00%] [Loss_G: 0.592077]\n",
      "Steps (90 / 100): [Loss_D_real: 0.076432, Loss_D_fake: 0.808358, acc: 49.61%] [Loss_G: 0.587958]\n",
      "Steps (100 / 100): [Loss_D_real: 0.055764, Loss_D_fake: 0.810743, acc: 49.61%] [Loss_G: 0.594150]\n",
      "EPOCH #  3 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.061961, Loss_D_fake: 0.806066, acc: 48.83%] [Loss_G: 0.598424]\n",
      "Steps (20 / 100): [Loss_D_real: 0.054361, Loss_D_fake: 0.808077, acc: 50.00%] [Loss_G: 0.596331]\n",
      "Steps (30 / 100): [Loss_D_real: 0.066826, Loss_D_fake: 0.788462, acc: 49.61%] [Loss_G: 0.607993]\n",
      "Steps (40 / 100): [Loss_D_real: 0.056748, Loss_D_fake: 0.789532, acc: 48.83%] [Loss_G: 0.607716]\n",
      "Steps (50 / 100): [Loss_D_real: 0.070253, Loss_D_fake: 0.782737, acc: 50.39%] [Loss_G: 0.612540]\n",
      "Steps (60 / 100): [Loss_D_real: 0.047011, Loss_D_fake: 0.777820, acc: 50.00%] [Loss_G: 0.616397]\n",
      "Steps (70 / 100): [Loss_D_real: 0.054161, Loss_D_fake: 0.768523, acc: 50.78%] [Loss_G: 0.621201]\n",
      "Steps (80 / 100): [Loss_D_real: 0.050814, Loss_D_fake: 0.769037, acc: 50.39%] [Loss_G: 0.627629]\n",
      "Steps (90 / 100): [Loss_D_real: 0.048702, Loss_D_fake: 0.762482, acc: 50.00%] [Loss_G: 0.629971]\n",
      "Steps (100 / 100): [Loss_D_real: 0.036555, Loss_D_fake: 0.762994, acc: 50.78%] [Loss_G: 0.637547]\n",
      "EPOCH #  4 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.032506, Loss_D_fake: 0.748849, acc: 51.95%] [Loss_G: 0.637348]\n",
      "Steps (20 / 100): [Loss_D_real: 0.055991, Loss_D_fake: 0.743770, acc: 54.69%] [Loss_G: 0.643154]\n",
      "Steps (30 / 100): [Loss_D_real: 0.041698, Loss_D_fake: 0.737256, acc: 54.69%] [Loss_G: 0.650304]\n",
      "Steps (40 / 100): [Loss_D_real: 0.049204, Loss_D_fake: 0.735001, acc: 56.25%] [Loss_G: 0.652928]\n",
      "Steps (50 / 100): [Loss_D_real: 0.053124, Loss_D_fake: 0.734218, acc: 57.03%] [Loss_G: 0.652811]\n",
      "Steps (60 / 100): [Loss_D_real: 0.055606, Loss_D_fake: 0.735283, acc: 56.25%] [Loss_G: 0.662108]\n",
      "Steps (70 / 100): [Loss_D_real: 0.046075, Loss_D_fake: 0.721259, acc: 62.50%] [Loss_G: 0.662273]\n",
      "Steps (80 / 100): [Loss_D_real: 0.043207, Loss_D_fake: 0.724766, acc: 62.11%] [Loss_G: 0.668364]\n",
      "Steps (90 / 100): [Loss_D_real: 0.042996, Loss_D_fake: 0.725557, acc: 60.16%] [Loss_G: 0.669835]\n",
      "Steps (100 / 100): [Loss_D_real: 0.045392, Loss_D_fake: 0.720482, acc: 62.89%] [Loss_G: 0.672816]\n",
      "EPOCH #  5 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.060718, Loss_D_fake: 0.714371, acc: 67.19%] [Loss_G: 0.667329]\n",
      "Steps (20 / 100): [Loss_D_real: 0.038874, Loss_D_fake: 0.721327, acc: 65.23%] [Loss_G: 0.668679]\n",
      "Steps (30 / 100): [Loss_D_real: 0.047217, Loss_D_fake: 0.716702, acc: 67.58%] [Loss_G: 0.668444]\n",
      "Steps (40 / 100): [Loss_D_real: 0.036157, Loss_D_fake: 0.714917, acc: 70.31%] [Loss_G: 0.669289]\n",
      "Steps (50 / 100): [Loss_D_real: 0.053896, Loss_D_fake: 0.733949, acc: 60.94%] [Loss_G: 0.654766]\n",
      "Steps (60 / 100): [Loss_D_real: 0.056342, Loss_D_fake: 0.738583, acc: 62.11%] [Loss_G: 0.655387]\n",
      "Steps (70 / 100): [Loss_D_real: 0.042123, Loss_D_fake: 0.745470, acc: 57.03%] [Loss_G: 0.656788]\n",
      "Steps (80 / 100): [Loss_D_real: 0.057744, Loss_D_fake: 0.747524, acc: 56.64%] [Loss_G: 0.643571]\n",
      "Steps (90 / 100): [Loss_D_real: 0.056046, Loss_D_fake: 0.745931, acc: 57.42%] [Loss_G: 0.642207]\n",
      "Steps (100 / 100): [Loss_D_real: 0.051613, Loss_D_fake: 0.757433, acc: 57.81%] [Loss_G: 0.637219]\n",
      "EPOCH #  6 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.078171, Loss_D_fake: 0.763199, acc: 56.64%] [Loss_G: 0.637501]\n",
      "Steps (20 / 100): [Loss_D_real: 0.066877, Loss_D_fake: 0.763160, acc: 55.86%] [Loss_G: 0.630699]\n",
      "Steps (30 / 100): [Loss_D_real: 0.052392, Loss_D_fake: 0.763055, acc: 55.47%] [Loss_G: 0.628962]\n",
      "Steps (40 / 100): [Loss_D_real: 0.069162, Loss_D_fake: 0.769995, acc: 52.73%] [Loss_G: 0.622135]\n",
      "Steps (50 / 100): [Loss_D_real: 0.055657, Loss_D_fake: 0.767561, acc: 54.69%] [Loss_G: 0.622130]\n",
      "Steps (60 / 100): [Loss_D_real: 0.073861, Loss_D_fake: 0.775988, acc: 54.69%] [Loss_G: 0.628190]\n",
      "Steps (70 / 100): [Loss_D_real: 0.077196, Loss_D_fake: 0.761915, acc: 55.86%] [Loss_G: 0.625959]\n",
      "Steps (80 / 100): [Loss_D_real: 0.084435, Loss_D_fake: 0.763127, acc: 53.52%] [Loss_G: 0.627931]\n",
      "Steps (90 / 100): [Loss_D_real: 0.093610, Loss_D_fake: 0.771024, acc: 52.73%] [Loss_G: 0.627714]\n",
      "Steps (100 / 100): [Loss_D_real: 0.075962, Loss_D_fake: 0.771660, acc: 53.52%] [Loss_G: 0.635434]\n",
      "EPOCH #  7 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.064365, Loss_D_fake: 0.764819, acc: 53.91%] [Loss_G: 0.624966]\n",
      "Steps (20 / 100): [Loss_D_real: 0.078807, Loss_D_fake: 0.755990, acc: 57.81%] [Loss_G: 0.635049]\n",
      "Steps (30 / 100): [Loss_D_real: 0.065346, Loss_D_fake: 0.756390, acc: 57.03%] [Loss_G: 0.632210]\n",
      "Steps (40 / 100): [Loss_D_real: 0.073426, Loss_D_fake: 0.756650, acc: 56.64%] [Loss_G: 0.640113]\n",
      "Steps (50 / 100): [Loss_D_real: 0.081177, Loss_D_fake: 0.747854, acc: 56.64%] [Loss_G: 0.640915]\n",
      "Steps (60 / 100): [Loss_D_real: 0.083924, Loss_D_fake: 0.745975, acc: 58.59%] [Loss_G: 0.638166]\n",
      "Steps (70 / 100): [Loss_D_real: 0.067528, Loss_D_fake: 0.742007, acc: 59.38%] [Loss_G: 0.643185]\n",
      "Steps (80 / 100): [Loss_D_real: 0.076353, Loss_D_fake: 0.740624, acc: 58.20%] [Loss_G: 0.648463]\n",
      "Steps (90 / 100): [Loss_D_real: 0.069945, Loss_D_fake: 0.734624, acc: 60.16%] [Loss_G: 0.653146]\n",
      "Steps (100 / 100): [Loss_D_real: 0.076523, Loss_D_fake: 0.729659, acc: 60.55%] [Loss_G: 0.651999]\n",
      "EPOCH #  8 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.079597, Loss_D_fake: 0.731658, acc: 61.33%] [Loss_G: 0.657386]\n",
      "Steps (20 / 100): [Loss_D_real: 0.079890, Loss_D_fake: 0.734966, acc: 60.55%] [Loss_G: 0.659329]\n",
      "Steps (30 / 100): [Loss_D_real: 0.081023, Loss_D_fake: 0.733532, acc: 59.77%] [Loss_G: 0.660400]\n",
      "Steps (40 / 100): [Loss_D_real: 0.091859, Loss_D_fake: 0.726823, acc: 60.94%] [Loss_G: 0.661133]\n",
      "Steps (50 / 100): [Loss_D_real: 0.080947, Loss_D_fake: 0.723178, acc: 61.72%] [Loss_G: 0.663809]\n",
      "Steps (60 / 100): [Loss_D_real: 0.085240, Loss_D_fake: 0.717577, acc: 64.45%] [Loss_G: 0.663380]\n",
      "Steps (70 / 100): [Loss_D_real: 0.084100, Loss_D_fake: 0.725825, acc: 63.28%] [Loss_G: 0.669333]\n",
      "Steps (80 / 100): [Loss_D_real: 0.089135, Loss_D_fake: 0.723417, acc: 62.11%] [Loss_G: 0.671249]\n",
      "Steps (90 / 100): [Loss_D_real: 0.081278, Loss_D_fake: 0.717894, acc: 64.45%] [Loss_G: 0.666681]\n",
      "Steps (100 / 100): [Loss_D_real: 0.075132, Loss_D_fake: 0.720598, acc: 62.89%] [Loss_G: 0.673323]\n",
      "EPOCH #  9 --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (10 / 100): [Loss_D_real: 0.101657, Loss_D_fake: 0.707293, acc: 67.58%] [Loss_G: 0.678805]\n",
      "Steps (20 / 100): [Loss_D_real: 0.094967, Loss_D_fake: 0.707195, acc: 68.75%] [Loss_G: 0.681281]\n",
      "Steps (30 / 100): [Loss_D_real: 0.093957, Loss_D_fake: 0.710665, acc: 65.62%] [Loss_G: 0.684685]\n",
      "Steps (40 / 100): [Loss_D_real: 0.081320, Loss_D_fake: 0.700718, acc: 71.88%] [Loss_G: 0.687961]\n",
      "Steps (50 / 100): [Loss_D_real: 0.082580, Loss_D_fake: 0.701316, acc: 71.09%] [Loss_G: 0.688516]\n",
      "Steps (60 / 100): [Loss_D_real: 0.080174, Loss_D_fake: 0.702266, acc: 70.70%] [Loss_G: 0.694299]\n",
      "Steps (70 / 100): [Loss_D_real: 0.071277, Loss_D_fake: 0.693728, acc: 75.78%] [Loss_G: 0.693609]\n",
      "Steps (80 / 100): [Loss_D_real: 0.095912, Loss_D_fake: 0.690134, acc: 78.52%] [Loss_G: 0.692794]\n",
      "Steps (90 / 100): [Loss_D_real: 0.095385, Loss_D_fake: 0.680741, acc: 82.03%] [Loss_G: 0.702665]\n",
      "Steps (100 / 100): [Loss_D_real: 0.098336, Loss_D_fake: 0.683814, acc: 82.42%] [Loss_G: 0.705765]\n",
      "EPOCH #  10 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.111382, Loss_D_fake: 0.684764, acc: 81.25%] [Loss_G: 0.712679]\n",
      "Steps (20 / 100): [Loss_D_real: 0.109255, Loss_D_fake: 0.677153, acc: 84.77%] [Loss_G: 0.713664]\n",
      "Steps (30 / 100): [Loss_D_real: 0.101266, Loss_D_fake: 0.678099, acc: 86.33%] [Loss_G: 0.716080]\n",
      "Steps (40 / 100): [Loss_D_real: 0.101724, Loss_D_fake: 0.666773, acc: 87.89%] [Loss_G: 0.719870]\n",
      "Steps (50 / 100): [Loss_D_real: 0.083318, Loss_D_fake: 0.669179, acc: 89.06%] [Loss_G: 0.717473]\n",
      "Steps (60 / 100): [Loss_D_real: 0.090148, Loss_D_fake: 0.669791, acc: 86.72%] [Loss_G: 0.718432]\n",
      "Steps (70 / 100): [Loss_D_real: 0.107258, Loss_D_fake: 0.669048, acc: 85.94%] [Loss_G: 0.723410]\n",
      "Steps (80 / 100): [Loss_D_real: 0.103611, Loss_D_fake: 0.665883, acc: 88.28%] [Loss_G: 0.721091]\n",
      "Steps (90 / 100): [Loss_D_real: 0.084980, Loss_D_fake: 0.663872, acc: 89.84%] [Loss_G: 0.727577]\n",
      "Steps (100 / 100): [Loss_D_real: 0.082441, Loss_D_fake: 0.666728, acc: 88.67%] [Loss_G: 0.716970]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(g_model=G, d_model=D)\n",
    "EPOCHS = 10\n",
    "X_train_fraud = x_train_fraud.to_numpy()\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH # ', epoch + 1, '-' * 50)\n",
    "    gan.train(X_train_fraud, batch_size=128, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator.save('gan_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAN ROS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(generator, n_data):\n",
    "    noise = np.random.normal(0, 1, size=(n_data, latent_dim))\n",
    "    gen = generator.predict(noise)\n",
    "    x_train_gen = np.concatenate((x_train, gen))\n",
    "    y_gen = np.array(gen.shape[0] * [1])\n",
    "    y_train_gen = np.concatenate((y_train, y_gen))\n",
    "    return gen, x_train_gen, y_train_gen    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.198706</td>\n",
       "      <td>-0.762683</td>\n",
       "      <td>0.550463</td>\n",
       "      <td>-0.873680</td>\n",
       "      <td>1.195872</td>\n",
       "      <td>-0.272598</td>\n",
       "      <td>-0.131035</td>\n",
       "      <td>-0.885020</td>\n",
       "      <td>0.172107</td>\n",
       "      <td>-0.630421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172998</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>-0.442863</td>\n",
       "      <td>-0.008667</td>\n",
       "      <td>-0.134734</td>\n",
       "      <td>-0.074124</td>\n",
       "      <td>-0.308090</td>\n",
       "      <td>-0.086204</td>\n",
       "      <td>0.298442</td>\n",
       "      <td>0.090632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.809531</td>\n",
       "      <td>0.242220</td>\n",
       "      <td>0.253717</td>\n",
       "      <td>0.331004</td>\n",
       "      <td>0.368732</td>\n",
       "      <td>0.255431</td>\n",
       "      <td>0.269883</td>\n",
       "      <td>0.360909</td>\n",
       "      <td>0.249147</td>\n",
       "      <td>0.311389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194187</td>\n",
       "      <td>0.187618</td>\n",
       "      <td>0.290937</td>\n",
       "      <td>0.266189</td>\n",
       "      <td>0.256705</td>\n",
       "      <td>0.269221</td>\n",
       "      <td>0.204048</td>\n",
       "      <td>0.238961</td>\n",
       "      <td>0.284064</td>\n",
       "      <td>0.226025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.008746</td>\n",
       "      <td>-1.553882</td>\n",
       "      <td>-0.155522</td>\n",
       "      <td>-2.179274</td>\n",
       "      <td>0.302949</td>\n",
       "      <td>-1.204299</td>\n",
       "      <td>-1.027184</td>\n",
       "      <td>-2.258254</td>\n",
       "      <td>-0.672015</td>\n",
       "      <td>-1.820887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420414</td>\n",
       "      <td>-0.507902</td>\n",
       "      <td>-1.583942</td>\n",
       "      <td>-1.109340</td>\n",
       "      <td>-0.947352</td>\n",
       "      <td>-0.928648</td>\n",
       "      <td>-1.170368</td>\n",
       "      <td>-0.948985</td>\n",
       "      <td>-0.615322</td>\n",
       "      <td>-0.521733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.607534</td>\n",
       "      <td>-0.912969</td>\n",
       "      <td>0.371841</td>\n",
       "      <td>-1.077091</td>\n",
       "      <td>0.923528</td>\n",
       "      <td>-0.423944</td>\n",
       "      <td>-0.310951</td>\n",
       "      <td>-1.102189</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>-0.832885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039617</td>\n",
       "      <td>-0.047567</td>\n",
       "      <td>-0.608654</td>\n",
       "      <td>-0.181488</td>\n",
       "      <td>-0.317593</td>\n",
       "      <td>-0.259135</td>\n",
       "      <td>-0.439663</td>\n",
       "      <td>-0.243062</td>\n",
       "      <td>0.102301</td>\n",
       "      <td>-0.066395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.141662</td>\n",
       "      <td>-0.749413</td>\n",
       "      <td>0.537844</td>\n",
       "      <td>-0.843805</td>\n",
       "      <td>1.163631</td>\n",
       "      <td>-0.256563</td>\n",
       "      <td>-0.115851</td>\n",
       "      <td>-0.863069</td>\n",
       "      <td>0.161042</td>\n",
       "      <td>-0.602991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164542</td>\n",
       "      <td>0.074092</td>\n",
       "      <td>-0.430965</td>\n",
       "      <td>-0.007723</td>\n",
       "      <td>-0.136500</td>\n",
       "      <td>-0.090567</td>\n",
       "      <td>-0.297199</td>\n",
       "      <td>-0.068053</td>\n",
       "      <td>0.297197</td>\n",
       "      <td>0.076631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.701789</td>\n",
       "      <td>-0.591090</td>\n",
       "      <td>0.721799</td>\n",
       "      <td>-0.630324</td>\n",
       "      <td>1.437741</td>\n",
       "      <td>-0.110520</td>\n",
       "      <td>0.049075</td>\n",
       "      <td>-0.620011</td>\n",
       "      <td>0.346040</td>\n",
       "      <td>-0.405448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301933</td>\n",
       "      <td>0.200008</td>\n",
       "      <td>-0.255343</td>\n",
       "      <td>0.164115</td>\n",
       "      <td>0.041329</td>\n",
       "      <td>0.104701</td>\n",
       "      <td>-0.168827</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>0.498284</td>\n",
       "      <td>0.239294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.568624</td>\n",
       "      <td>-0.130545</td>\n",
       "      <td>1.558313</td>\n",
       "      <td>-0.094780</td>\n",
       "      <td>2.794879</td>\n",
       "      <td>0.407427</td>\n",
       "      <td>0.780318</td>\n",
       "      <td>-0.010885</td>\n",
       "      <td>1.309904</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>0.736460</td>\n",
       "      <td>0.386309</td>\n",
       "      <td>0.891412</td>\n",
       "      <td>0.678998</td>\n",
       "      <td>0.836135</td>\n",
       "      <td>0.251568</td>\n",
       "      <td>0.741321</td>\n",
       "      <td>1.327040</td>\n",
       "      <td>0.853007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      3.198706    -0.762683     0.550463    -0.873680     1.195872   \n",
       "std       0.809531     0.242220     0.253717     0.331004     0.368732   \n",
       "min       1.008746    -1.553882    -0.155522    -2.179274     0.302949   \n",
       "25%       2.607534    -0.912969     0.371841    -1.077091     0.923528   \n",
       "50%       3.141662    -0.749413     0.537844    -0.843805     1.163631   \n",
       "75%       3.701789    -0.591090     0.721799    -0.630324     1.437741   \n",
       "max       6.568624    -0.130545     1.558313    -0.094780     2.794879   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -0.272598    -0.131035    -0.885020     0.172107    -0.630421  ...   \n",
       "std       0.255431     0.269883     0.360909     0.249147     0.311389  ...   \n",
       "min      -1.204299    -1.027184    -2.258254    -0.672015    -1.820887  ...   \n",
       "25%      -0.423944    -0.310951    -1.102189     0.004328    -0.832885  ...   \n",
       "50%      -0.256563    -0.115851    -0.863069     0.161042    -0.602991  ...   \n",
       "75%      -0.110520     0.049075    -0.620011     0.346040    -0.405448  ...   \n",
       "max       0.407427     0.780318    -0.010885     1.309904     0.156086  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.172998     0.081598    -0.442863    -0.008667    -0.134734   \n",
       "std       0.194187     0.187618     0.290937     0.266189     0.256705   \n",
       "min      -0.420414    -0.507902    -1.583942    -1.109340    -0.947352   \n",
       "25%       0.039617    -0.047567    -0.608654    -0.181488    -0.317593   \n",
       "50%       0.164542     0.074092    -0.430965    -0.007723    -0.136500   \n",
       "75%       0.301933     0.200008    -0.255343     0.164115     0.041329   \n",
       "max       0.951111     0.736460     0.386309     0.891412     0.678998   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.074124    -0.308090    -0.086204     0.298442     0.090632  \n",
       "std       0.269221     0.204048     0.238961     0.284064     0.226025  \n",
       "min      -0.928648    -1.170368    -0.948985    -0.615322    -0.521733  \n",
       "25%      -0.259135    -0.439663    -0.243062     0.102301    -0.066395  \n",
       "50%      -0.090567    -0.297199    -0.068053     0.297197     0.076631  \n",
       "75%       0.104701    -0.168827     0.088158     0.498284     0.239294  \n",
       "max       0.836135     0.251568     0.741321     1.327040     0.853007  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 1000 more fraud\n",
    "gen_1000, x_train_gen_1000, y_train_gen_1000 = gen_data(gan.generator, 1000)\n",
    "df_gen_1000 = pd.DataFrame(data=gen_1000, index=None, columns=x_train.columns)\n",
    "df_gen_1000.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.218229</td>\n",
       "      <td>-0.756737</td>\n",
       "      <td>0.545450</td>\n",
       "      <td>-0.865985</td>\n",
       "      <td>1.202481</td>\n",
       "      <td>-0.291968</td>\n",
       "      <td>-0.135377</td>\n",
       "      <td>-0.896216</td>\n",
       "      <td>0.172936</td>\n",
       "      <td>-0.649727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163267</td>\n",
       "      <td>0.070233</td>\n",
       "      <td>-0.446318</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>-0.142786</td>\n",
       "      <td>-0.066574</td>\n",
       "      <td>-0.303814</td>\n",
       "      <td>-0.095933</td>\n",
       "      <td>0.288435</td>\n",
       "      <td>0.085132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811030</td>\n",
       "      <td>0.237068</td>\n",
       "      <td>0.248480</td>\n",
       "      <td>0.324748</td>\n",
       "      <td>0.375655</td>\n",
       "      <td>0.256967</td>\n",
       "      <td>0.262603</td>\n",
       "      <td>0.354987</td>\n",
       "      <td>0.260421</td>\n",
       "      <td>0.328550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202289</td>\n",
       "      <td>0.185099</td>\n",
       "      <td>0.297259</td>\n",
       "      <td>0.268653</td>\n",
       "      <td>0.245904</td>\n",
       "      <td>0.274539</td>\n",
       "      <td>0.203102</td>\n",
       "      <td>0.238672</td>\n",
       "      <td>0.294179</td>\n",
       "      <td>0.221833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.864514</td>\n",
       "      <td>-2.054588</td>\n",
       "      <td>-0.467439</td>\n",
       "      <td>-3.083127</td>\n",
       "      <td>-0.161780</td>\n",
       "      <td>-1.679737</td>\n",
       "      <td>-1.472482</td>\n",
       "      <td>-2.770850</td>\n",
       "      <td>-0.876556</td>\n",
       "      <td>-2.878052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.744244</td>\n",
       "      <td>-0.724654</td>\n",
       "      <td>-2.054292</td>\n",
       "      <td>-1.390064</td>\n",
       "      <td>-1.226690</td>\n",
       "      <td>-1.385246</td>\n",
       "      <td>-1.398025</td>\n",
       "      <td>-1.388916</td>\n",
       "      <td>-1.065479</td>\n",
       "      <td>-0.907872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.637174</td>\n",
       "      <td>-0.907591</td>\n",
       "      <td>0.373981</td>\n",
       "      <td>-1.067487</td>\n",
       "      <td>0.936968</td>\n",
       "      <td>-0.454711</td>\n",
       "      <td>-0.305363</td>\n",
       "      <td>-1.119759</td>\n",
       "      <td>-0.006194</td>\n",
       "      <td>-0.856450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027138</td>\n",
       "      <td>-0.055728</td>\n",
       "      <td>-0.638298</td>\n",
       "      <td>-0.184396</td>\n",
       "      <td>-0.307600</td>\n",
       "      <td>-0.252043</td>\n",
       "      <td>-0.436240</td>\n",
       "      <td>-0.249986</td>\n",
       "      <td>0.092895</td>\n",
       "      <td>-0.066339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.155191</td>\n",
       "      <td>-0.741627</td>\n",
       "      <td>0.535681</td>\n",
       "      <td>-0.835274</td>\n",
       "      <td>1.177739</td>\n",
       "      <td>-0.278182</td>\n",
       "      <td>-0.129652</td>\n",
       "      <td>-0.866632</td>\n",
       "      <td>0.163114</td>\n",
       "      <td>-0.624668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156409</td>\n",
       "      <td>0.063012</td>\n",
       "      <td>-0.433781</td>\n",
       "      <td>-0.006708</td>\n",
       "      <td>-0.146187</td>\n",
       "      <td>-0.074882</td>\n",
       "      <td>-0.300090</td>\n",
       "      <td>-0.087382</td>\n",
       "      <td>0.291074</td>\n",
       "      <td>0.075964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.728535</td>\n",
       "      <td>-0.590100</td>\n",
       "      <td>0.706414</td>\n",
       "      <td>-0.633683</td>\n",
       "      <td>1.443086</td>\n",
       "      <td>-0.115086</td>\n",
       "      <td>0.040646</td>\n",
       "      <td>-0.641440</td>\n",
       "      <td>0.341009</td>\n",
       "      <td>-0.415283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292845</td>\n",
       "      <td>0.188909</td>\n",
       "      <td>-0.241708</td>\n",
       "      <td>0.172219</td>\n",
       "      <td>0.017624</td>\n",
       "      <td>0.110001</td>\n",
       "      <td>-0.167320</td>\n",
       "      <td>0.066672</td>\n",
       "      <td>0.486651</td>\n",
       "      <td>0.226646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.098598</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>1.781558</td>\n",
       "      <td>0.096638</td>\n",
       "      <td>3.173489</td>\n",
       "      <td>0.693634</td>\n",
       "      <td>1.145547</td>\n",
       "      <td>0.370712</td>\n",
       "      <td>1.762260</td>\n",
       "      <td>0.374252</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221887</td>\n",
       "      <td>1.129017</td>\n",
       "      <td>0.788170</td>\n",
       "      <td>1.249569</td>\n",
       "      <td>1.158705</td>\n",
       "      <td>1.394942</td>\n",
       "      <td>0.672848</td>\n",
       "      <td>0.942883</td>\n",
       "      <td>1.733728</td>\n",
       "      <td>1.194972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean        3.218229      -0.756737       0.545450      -0.865985   \n",
       "std         0.811030       0.237068       0.248480       0.324748   \n",
       "min         0.864514      -2.054588      -0.467439      -3.083127   \n",
       "25%         2.637174      -0.907591       0.373981      -1.067487   \n",
       "50%         3.155191      -0.741627       0.535681      -0.835274   \n",
       "75%         3.728535      -0.590100       0.706414      -0.633683   \n",
       "max         8.098598       0.093388       1.781558       0.096638   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean        1.202481      -0.291968      -0.135377      -0.896216   \n",
       "std         0.375655       0.256967       0.262603       0.354987   \n",
       "min        -0.161780      -1.679737      -1.472482      -2.770850   \n",
       "25%         0.936968      -0.454711      -0.305363      -1.119759   \n",
       "50%         1.177739      -0.278182      -0.129652      -0.866632   \n",
       "75%         1.443086      -0.115086       0.040646      -0.641440   \n",
       "max         3.173489       0.693634       1.145547       0.370712   \n",
       "\n",
       "                  V8             V9  ...            V20            V21  \\\n",
       "count  227057.000000  227057.000000  ...  227057.000000  227057.000000   \n",
       "mean        0.172936      -0.649727  ...       0.163267       0.070233   \n",
       "std         0.260421       0.328550  ...       0.202289       0.185099   \n",
       "min        -0.876556      -2.878052  ...      -0.744244      -0.724654   \n",
       "25%        -0.006194      -0.856450  ...       0.027138      -0.055728   \n",
       "50%         0.163114      -0.624668  ...       0.156409       0.063012   \n",
       "75%         0.341009      -0.415283  ...       0.292845       0.188909   \n",
       "max         1.762260       0.374252  ...       1.221887       1.129017   \n",
       "\n",
       "                 V22            V23            V24            V25  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean       -0.446318      -0.006336      -0.142786      -0.066574   \n",
       "std         0.297259       0.268653       0.245904       0.274539   \n",
       "min        -2.054292      -1.390064      -1.226690      -1.385246   \n",
       "25%        -0.638298      -0.184396      -0.307600      -0.252043   \n",
       "50%        -0.433781      -0.006708      -0.146187      -0.074882   \n",
       "75%        -0.241708       0.172219       0.017624       0.110001   \n",
       "max         0.788170       1.249569       1.158705       1.394942   \n",
       "\n",
       "                 V26            V27            V28         Amount  \n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000  \n",
       "mean       -0.303814      -0.095933       0.288435       0.085132  \n",
       "std         0.203102       0.238672       0.294179       0.221833  \n",
       "min        -1.398025      -1.388916      -1.065479      -0.907872  \n",
       "25%        -0.436240      -0.249986       0.092895      -0.066339  \n",
       "50%        -0.300090      -0.087382       0.291074       0.075964  \n",
       "75%        -0.167320       0.066672       0.486651       0.226646  \n",
       "max         0.672848       0.942883       1.733728       1.194972  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 227451 -394 = 227057  (total of x_train - fraud in x_train)\n",
    "gen_227057, x_train_gen_227057, y_train_gen_227057 = gen_data(gan.generator, 227057)\n",
    "\n",
    "df_gen_227057 = pd.DataFrame(data=gen_227057, index=None, columns=x_train.columns)\n",
    "df_gen_227057.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.008938</td>\n",
       "      <td>-4.707808</td>\n",
       "      <td>3.588729</td>\n",
       "      <td>-7.068378</td>\n",
       "      <td>4.592975</td>\n",
       "      <td>-3.101629</td>\n",
       "      <td>-1.387192</td>\n",
       "      <td>-5.539909</td>\n",
       "      <td>0.587920</td>\n",
       "      <td>-2.589654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358018</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.051318</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>-0.109108</td>\n",
       "      <td>0.019602</td>\n",
       "      <td>0.047827</td>\n",
       "      <td>0.155933</td>\n",
       "      <td>0.077212</td>\n",
       "      <td>1.228674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.347935</td>\n",
       "      <td>6.841390</td>\n",
       "      <td>4.309436</td>\n",
       "      <td>7.166449</td>\n",
       "      <td>2.883467</td>\n",
       "      <td>5.406586</td>\n",
       "      <td>1.864770</td>\n",
       "      <td>7.316745</td>\n",
       "      <td>6.676697</td>\n",
       "      <td>2.495584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384017</td>\n",
       "      <td>3.750615</td>\n",
       "      <td>1.457801</td>\n",
       "      <td>1.681228</td>\n",
       "      <td>0.509477</td>\n",
       "      <td>0.826820</td>\n",
       "      <td>0.467046</td>\n",
       "      <td>1.358987</td>\n",
       "      <td>0.555106</td>\n",
       "      <td>0.965996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.239444</td>\n",
       "      <td>-30.552380</td>\n",
       "      <td>-8.402154</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>-1.313275</td>\n",
       "      <td>-22.105532</td>\n",
       "      <td>-5.773192</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.128186</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-19.254328</td>\n",
       "      <td>-2.028024</td>\n",
       "      <td>-4.781606</td>\n",
       "      <td>-1.152671</td>\n",
       "      <td>-7.263482</td>\n",
       "      <td>-1.869290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.500278</td>\n",
       "      <td>-5.996596</td>\n",
       "      <td>1.229209</td>\n",
       "      <td>-8.436924</td>\n",
       "      <td>2.419178</td>\n",
       "      <td>-4.741036</td>\n",
       "      <td>-2.504633</td>\n",
       "      <td>-7.765017</td>\n",
       "      <td>-0.135707</td>\n",
       "      <td>-3.828323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181718</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>-0.515338</td>\n",
       "      <td>-0.330293</td>\n",
       "      <td>-0.445282</td>\n",
       "      <td>-0.312004</td>\n",
       "      <td>-0.253693</td>\n",
       "      <td>-0.025894</td>\n",
       "      <td>-0.096541</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.393056</td>\n",
       "      <td>-2.272114</td>\n",
       "      <td>2.662472</td>\n",
       "      <td>-5.133485</td>\n",
       "      <td>4.258196</td>\n",
       "      <td>-1.522962</td>\n",
       "      <td>-1.421577</td>\n",
       "      <td>-2.926216</td>\n",
       "      <td>0.642565</td>\n",
       "      <td>-2.230097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280862</td>\n",
       "      <td>0.576441</td>\n",
       "      <td>0.073696</td>\n",
       "      <td>-0.057241</td>\n",
       "      <td>-0.060269</td>\n",
       "      <td>0.088371</td>\n",
       "      <td>-0.003464</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>0.147380</td>\n",
       "      <td>1.007318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.912917</td>\n",
       "      <td>-0.410418</td>\n",
       "      <td>4.737900</td>\n",
       "      <td>-2.302626</td>\n",
       "      <td>6.390866</td>\n",
       "      <td>0.240184</td>\n",
       "      <td>-0.361122</td>\n",
       "      <td>-0.900824</td>\n",
       "      <td>1.743587</td>\n",
       "      <td>-0.825345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783528</td>\n",
       "      <td>1.204214</td>\n",
       "      <td>0.615344</td>\n",
       "      <td>0.307132</td>\n",
       "      <td>0.274014</td>\n",
       "      <td>0.441670</td>\n",
       "      <td>0.393148</td>\n",
       "      <td>0.779267</td>\n",
       "      <td>0.372389</td>\n",
       "      <td>2.028937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.318889</td>\n",
       "      <td>2.132386</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>2.250210</td>\n",
       "      <td>12.114672</td>\n",
       "      <td>11.095089</td>\n",
       "      <td>6.474115</td>\n",
       "      <td>5.802537</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>...</td>\n",
       "      <td>11.059004</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>5.466230</td>\n",
       "      <td>0.994110</td>\n",
       "      <td>2.208209</td>\n",
       "      <td>2.745261</td>\n",
       "      <td>3.052358</td>\n",
       "      <td>1.779364</td>\n",
       "      <td>3.327741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time          V1          V2          V3          V4          V5  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean    23.008938   -4.707808    3.588729   -7.068378    4.592975   -3.101629   \n",
       "std     13.347935    6.841390    4.309436    7.166449    2.883467    5.406586   \n",
       "min      1.239444  -30.552380   -8.402154  -31.103685   -1.313275  -22.105532   \n",
       "25%     11.500278   -5.996596    1.229209   -8.436924    2.419178   -4.741036   \n",
       "50%     21.393056   -2.272114    2.662472   -5.133485    4.258196   -1.522962   \n",
       "75%     35.912917   -0.410418    4.737900   -2.302626    6.390866    0.240184   \n",
       "max     47.318889    2.132386   22.057729    2.250210   12.114672   11.095089   \n",
       "\n",
       "               V6          V7          V8          V9  ...         V20  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  ...  394.000000   \n",
       "mean    -1.387192   -5.539909    0.587920   -2.589654  ...    0.358018   \n",
       "std      1.864770    7.316745    6.676697    2.495584  ...    1.384017   \n",
       "min     -5.773192  -43.557242  -41.044261  -13.434066  ...   -4.128186   \n",
       "25%     -2.504633   -7.765017   -0.135707   -3.828323  ...   -0.181718   \n",
       "50%     -1.421577   -2.926216    0.642565   -2.230097  ...    0.280862   \n",
       "75%     -0.361122   -0.900824    1.743587   -0.825345  ...    0.783528   \n",
       "max      6.474115    5.802537   20.007208    3.353525  ...   11.059004   \n",
       "\n",
       "              V21         V22         V23         V24         V25         V26  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean     0.628814    0.051318   -0.062790   -0.109108    0.019602    0.047827   \n",
       "std      3.750615    1.457801    1.681228    0.509477    0.826820    0.467046   \n",
       "min    -22.797604   -8.887017  -19.254328   -2.028024   -4.781606   -1.152671   \n",
       "25%      0.040122   -0.515338   -0.330293   -0.445282   -0.312004   -0.253693   \n",
       "50%      0.576441    0.073696   -0.057241   -0.060269    0.088371   -0.003464   \n",
       "75%      1.204214    0.615344    0.307132    0.274014    0.441670    0.393148   \n",
       "max     27.202839    8.361985    5.466230    0.994110    2.208209    2.745261   \n",
       "\n",
       "              V27         V28      Amount  \n",
       "count  394.000000  394.000000  394.000000  \n",
       "mean     0.155933    0.077212    1.228674  \n",
       "std      1.358987    0.555106    0.965996  \n",
       "min     -7.263482   -1.869290    0.000000  \n",
       "25%     -0.025894   -0.096541    0.301030  \n",
       "50%      0.394926    0.147380    1.007318  \n",
       "75%      0.779267    0.372389    2.028937  \n",
       "max      3.052358    1.779364    3.327741  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with GPU\n",
    "ros = RandomOverSampler()\n",
    "def XGBC_model_predit(x, y):   \n",
    "    x, y = ros.fit_sample(x, y)\n",
    "    clf_xgb_os = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
    "              gamma=0.2, gpu_id=0, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.02, max_delta_step=0,\n",
    "              max_depth=10, min_child_weight=2,\n",
    "              monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
    "              n_estimators=800, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)\n",
    "    \n",
    "    clf_xgb_os.fit(x, y)  \n",
    "    y_pred_gen_os = clf_xgb_os.predict(x_test.to_numpy())\n",
    "    return y_pred_gen_os       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995084442259752\n",
      "Precision:  0.8645833333333334\n",
      "Recall:  0.8469387755102041\n",
      "F1 score:  0.8556701030927835\n",
      "ROC AUC score:  0.9233550799329299\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_1000 = XGBC_model_predit(x_train_gen_1000, y_train_gen_1000)\n",
    "check_performance(y_test, y_pred_gen_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_1000, 'Adding GAN 1000 Fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 49,153\n",
      "Trainable params: 49,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 51,166\n",
      "Trainable params: 51,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "latent_dim= 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer_wgan = RMSprop(lr=0.00001)\n",
    "\n",
    "# %% --------------------------------------- Set Seeds -----------------------------------------------------------------\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "weight_init = glorot_normal(seed=SEED)\n",
    "\n",
    "\n",
    "# %% --------------------------------------- G D-----------------------------------------------------------------\n",
    "def generator_wgan():\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(noise)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(256, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # tanh is removed since we are not dealing with normalized image data\n",
    "    out = Dense(data_dim, kernel_initializer=weight_init)(x)\n",
    "    \n",
    "    model = Model(inputs=noise, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_wgan():\n",
    "    data = Input(shape=data_dim)\n",
    "    x = Dense(256, kernel_initializer=weight_init)(data)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # remove sigmoid for D in WGAN\n",
    "    out = Dense(1, kernel_initializer=weight_init)(x)\n",
    "\n",
    "    model = Model(inputs=data, outputs=out)\n",
    "    \n",
    "    # RMSprop optimizer, w_loss\n",
    "    model.compile(optimizer=optimizer_wgan, loss=w_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "def w_loss(y, y_pred):\n",
    "    return tf.reduce_mean(tf.multiply(y, y_pred))\n",
    "\n",
    "# def w_loss(y, y_pred):\n",
    "#     return K.mean(y* y_pred)\n",
    "\n",
    "def train_G_wgan(generator, discriminator):\n",
    "    # Freeze the discriminator when training generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(optimizer=optimizer_wgan, loss=w_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "# %% ----------------------------------- WGAN ----------------------------------------------------------------------\n",
    "# modified from https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan/wgan.py\n",
    "clip_value = 0.01\n",
    "train_D = 2 # train D more than G\n",
    "\n",
    "class WGAN:\n",
    "    def __init__(self, g_model, d_model):\n",
    "        self.z = latent_dim\n",
    "        self.optimizer = optimizer_wgan\n",
    "\n",
    "        self.generator = g_model\n",
    "        self.discriminator = d_model\n",
    "\n",
    "        self.train_G = train_G_wgan(self.generator, self.discriminator)\n",
    "        self.loss_D, self.loss_G = [], []\n",
    "\n",
    "    def train(self, data, batch_size=128, steps_per_epoch=50):    \n",
    "\n",
    "        for epoch in range(steps_per_epoch):\n",
    "            # Select a random batch of transactions data \n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            real_data = data[idx]\n",
    "\n",
    "            # generate a batch of new data\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            fake_data= self.generator.predict(noise)\n",
    "            \n",
    "            for _ in range(train_D):\n",
    "\n",
    "                # Train D                \n",
    "                loss_real = self.discriminator.train_on_batch(real_data, -np.ones(batch_size))\n",
    "                loss_fake = self.discriminator.train_on_batch(fake_data, np.ones(batch_size))\n",
    "                # loss_d = loss fake -  loss real: the wasserstein loss\n",
    "                loss_d = 0.5 * np.add(loss_real, loss_fake)\n",
    "                \n",
    "                self.loss_D.append(loss_d)\n",
    "\n",
    "\n",
    "                # clip the weight for D\n",
    "                for l in self.discriminator.layers:\n",
    "                        weights = l.get_weights()\n",
    "                        weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
    "                        l.set_weights(weights)\n",
    "\n",
    "            # Train G\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            loss_G = self.train_G.train_on_batch(noise, -np.ones(batch_size))\n",
    "            self.loss_G.append(loss_G)\n",
    "\n",
    "            if (epoch + 1) * 10 % steps_per_epoch == 0:\n",
    "                print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f] [loss_D: %f] [Loss_G: %f]' % \n",
    "                      (epoch+1, steps_per_epoch, loss_real, loss_fake, loss_d, loss_G))\n",
    "#                   (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], loss_d[0], loss_G[0]))\n",
    "\n",
    "#                 print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f, acc: %.2f%%] [Loss_G: %f]' %\n",
    "#                   (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], loss_d, loss_G[0]))\n",
    "#                 print('Steps (%d / %d): [Loss_D %f] [Loss_G: %f]' %\n",
    "#                   (epoch+1, steps_per_epoch, self.loss_D, loss_G))\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "# # %% -----------------------------------set up G D for WGAN ------------------------------------------------------------------\n",
    "D_wgan = discriminator_wgan()\n",
    "G_wgan  = generator_wgan()\n",
    "\n",
    "D_wgan.summary()\n",
    "G_wgan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #  1 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.000276, Loss_D_fake: 0.000029] [loss_D: -0.000123] [Loss_G: -0.000016]\n",
      "Steps (20 / 100): [Loss_D_real: -0.000335, Loss_D_fake: 0.000023] [loss_D: -0.000156] [Loss_G: -0.000009]\n",
      "Steps (30 / 100): [Loss_D_real: -0.000618, Loss_D_fake: 0.000009] [loss_D: -0.000305] [Loss_G: 0.000004]\n",
      "Steps (40 / 100): [Loss_D_real: -0.000969, Loss_D_fake: -0.000009] [loss_D: -0.000489] [Loss_G: 0.000023]\n",
      "Steps (50 / 100): [Loss_D_real: -0.001407, Loss_D_fake: -0.000038] [loss_D: -0.000723] [Loss_G: 0.000049]\n",
      "Steps (60 / 100): [Loss_D_real: -0.002074, Loss_D_fake: -0.000060] [loss_D: -0.001067] [Loss_G: 0.000076]\n",
      "Steps (70 / 100): [Loss_D_real: -0.002538, Loss_D_fake: -0.000089] [loss_D: -0.001313] [Loss_G: 0.000109]\n",
      "Steps (80 / 100): [Loss_D_real: -0.002984, Loss_D_fake: -0.000119] [loss_D: -0.001552] [Loss_G: 0.000139]\n",
      "Steps (90 / 100): [Loss_D_real: -0.003541, Loss_D_fake: -0.000153] [loss_D: -0.001847] [Loss_G: 0.000166]\n",
      "Steps (100 / 100): [Loss_D_real: -0.004270, Loss_D_fake: -0.000194] [loss_D: -0.002232] [Loss_G: 0.000199]\n",
      "EPOCH #  2 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.005338, Loss_D_fake: -0.000212] [loss_D: -0.002775] [Loss_G: 0.000218]\n",
      "Steps (20 / 100): [Loss_D_real: -0.006151, Loss_D_fake: -0.000237] [loss_D: -0.003194] [Loss_G: 0.000256]\n",
      "Steps (30 / 100): [Loss_D_real: -0.007367, Loss_D_fake: -0.000264] [loss_D: -0.003816] [Loss_G: 0.000284]\n",
      "Steps (40 / 100): [Loss_D_real: -0.008889, Loss_D_fake: -0.000286] [loss_D: -0.004588] [Loss_G: 0.000272]\n",
      "Steps (50 / 100): [Loss_D_real: -0.009922, Loss_D_fake: -0.000282] [loss_D: -0.005102] [Loss_G: 0.000321]\n",
      "Steps (60 / 100): [Loss_D_real: -0.012040, Loss_D_fake: -0.000258] [loss_D: -0.006149] [Loss_G: 0.000316]\n",
      "Steps (70 / 100): [Loss_D_real: -0.013406, Loss_D_fake: -0.000257] [loss_D: -0.006831] [Loss_G: 0.000279]\n",
      "Steps (80 / 100): [Loss_D_real: -0.013948, Loss_D_fake: -0.000248] [loss_D: -0.007098] [Loss_G: 0.000279]\n",
      "Steps (90 / 100): [Loss_D_real: -0.016645, Loss_D_fake: -0.000230] [loss_D: -0.008437] [Loss_G: 0.000219]\n",
      "Steps (100 / 100): [Loss_D_real: -0.019932, Loss_D_fake: -0.000109] [loss_D: -0.010021] [Loss_G: 0.000259]\n",
      "EPOCH #  3 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.020337, Loss_D_fake: -0.000093] [loss_D: -0.010215] [Loss_G: 0.000129]\n",
      "Steps (20 / 100): [Loss_D_real: -0.023879, Loss_D_fake: 0.000039] [loss_D: -0.011920] [Loss_G: 0.000066]\n",
      "Steps (30 / 100): [Loss_D_real: -0.026069, Loss_D_fake: 0.000045] [loss_D: -0.013012] [Loss_G: -0.000010]\n",
      "Steps (40 / 100): [Loss_D_real: -0.029662, Loss_D_fake: 0.000193] [loss_D: -0.014734] [Loss_G: -0.000224]\n",
      "Steps (50 / 100): [Loss_D_real: -0.029693, Loss_D_fake: 0.000312] [loss_D: -0.014691] [Loss_G: -0.000382]\n",
      "Steps (60 / 100): [Loss_D_real: -0.035835, Loss_D_fake: 0.000656] [loss_D: -0.017590] [Loss_G: -0.000643]\n",
      "Steps (70 / 100): [Loss_D_real: -0.039640, Loss_D_fake: 0.000728] [loss_D: -0.019456] [Loss_G: -0.000730]\n",
      "Steps (80 / 100): [Loss_D_real: -0.041055, Loss_D_fake: 0.000998] [loss_D: -0.020029] [Loss_G: -0.000951]\n",
      "Steps (90 / 100): [Loss_D_real: -0.045918, Loss_D_fake: 0.001252] [loss_D: -0.022333] [Loss_G: -0.001277]\n",
      "Steps (100 / 100): [Loss_D_real: -0.051906, Loss_D_fake: 0.001741] [loss_D: -0.025083] [Loss_G: -0.001510]\n",
      "EPOCH #  4 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.057075, Loss_D_fake: 0.001870] [loss_D: -0.027603] [Loss_G: -0.001873]\n",
      "Steps (20 / 100): [Loss_D_real: -0.054083, Loss_D_fake: 0.002203] [loss_D: -0.025940] [Loss_G: -0.002246]\n",
      "Steps (30 / 100): [Loss_D_real: -0.064776, Loss_D_fake: 0.002483] [loss_D: -0.031147] [Loss_G: -0.002531]\n",
      "Steps (40 / 100): [Loss_D_real: -0.067247, Loss_D_fake: 0.003058] [loss_D: -0.032094] [Loss_G: -0.002860]\n",
      "Steps (50 / 100): [Loss_D_real: -0.069548, Loss_D_fake: 0.003511] [loss_D: -0.033019] [Loss_G: -0.003557]\n",
      "Steps (60 / 100): [Loss_D_real: -0.062561, Loss_D_fake: 0.004238] [loss_D: -0.029162] [Loss_G: -0.003909]\n",
      "Steps (70 / 100): [Loss_D_real: -0.082488, Loss_D_fake: 0.004254] [loss_D: -0.039117] [Loss_G: -0.004457]\n",
      "Steps (80 / 100): [Loss_D_real: -0.087636, Loss_D_fake: 0.005003] [loss_D: -0.041317] [Loss_G: -0.005083]\n",
      "Steps (90 / 100): [Loss_D_real: -0.097291, Loss_D_fake: 0.005631] [loss_D: -0.045830] [Loss_G: -0.005439]\n",
      "Steps (100 / 100): [Loss_D_real: -0.102638, Loss_D_fake: 0.006235] [loss_D: -0.048202] [Loss_G: -0.006135]\n",
      "EPOCH #  5 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.101867, Loss_D_fake: 0.006496] [loss_D: -0.047685] [Loss_G: -0.007067]\n",
      "Steps (20 / 100): [Loss_D_real: -0.113421, Loss_D_fake: 0.007684] [loss_D: -0.052868] [Loss_G: -0.007597]\n",
      "Steps (30 / 100): [Loss_D_real: -0.119756, Loss_D_fake: 0.008309] [loss_D: -0.055723] [Loss_G: -0.008262]\n",
      "Steps (40 / 100): [Loss_D_real: -0.127219, Loss_D_fake: 0.008830] [loss_D: -0.059194] [Loss_G: -0.009143]\n",
      "Steps (50 / 100): [Loss_D_real: -0.123341, Loss_D_fake: 0.010378] [loss_D: -0.056482] [Loss_G: -0.010291]\n",
      "Steps (60 / 100): [Loss_D_real: -0.131050, Loss_D_fake: 0.011134] [loss_D: -0.059958] [Loss_G: -0.011446]\n",
      "Steps (70 / 100): [Loss_D_real: -0.152415, Loss_D_fake: 0.012751] [loss_D: -0.069832] [Loss_G: -0.011982]\n",
      "Steps (80 / 100): [Loss_D_real: -0.145079, Loss_D_fake: 0.013383] [loss_D: -0.065848] [Loss_G: -0.013088]\n",
      "Steps (90 / 100): [Loss_D_real: -0.165176, Loss_D_fake: 0.014209] [loss_D: -0.075484] [Loss_G: -0.014361]\n",
      "Steps (100 / 100): [Loss_D_real: -0.179324, Loss_D_fake: 0.015784] [loss_D: -0.081770] [Loss_G: -0.015704]\n",
      "EPOCH #  6 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.171945, Loss_D_fake: 0.017434] [loss_D: -0.077255] [Loss_G: -0.016096]\n",
      "Steps (20 / 100): [Loss_D_real: -0.191882, Loss_D_fake: 0.018118] [loss_D: -0.086882] [Loss_G: -0.017688]\n",
      "Steps (30 / 100): [Loss_D_real: -0.211475, Loss_D_fake: 0.019837] [loss_D: -0.095819] [Loss_G: -0.019467]\n",
      "Steps (40 / 100): [Loss_D_real: -0.181645, Loss_D_fake: 0.020888] [loss_D: -0.080379] [Loss_G: -0.020949]\n",
      "Steps (50 / 100): [Loss_D_real: -0.210724, Loss_D_fake: 0.022952] [loss_D: -0.093886] [Loss_G: -0.022705]\n",
      "Steps (60 / 100): [Loss_D_real: -0.226911, Loss_D_fake: 0.024379] [loss_D: -0.101266] [Loss_G: -0.023953]\n",
      "Steps (70 / 100): [Loss_D_real: -0.225953, Loss_D_fake: 0.025098] [loss_D: -0.100427] [Loss_G: -0.025524]\n",
      "Steps (80 / 100): [Loss_D_real: -0.218038, Loss_D_fake: 0.026813] [loss_D: -0.095612] [Loss_G: -0.026895]\n",
      "Steps (90 / 100): [Loss_D_real: -0.256111, Loss_D_fake: 0.029801] [loss_D: -0.113155] [Loss_G: -0.029955]\n",
      "Steps (100 / 100): [Loss_D_real: -0.245461, Loss_D_fake: 0.031495] [loss_D: -0.106983] [Loss_G: -0.030878]\n",
      "EPOCH #  7 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.266148, Loss_D_fake: 0.032802] [loss_D: -0.116673] [Loss_G: -0.033772]\n",
      "Steps (20 / 100): [Loss_D_real: -0.270568, Loss_D_fake: 0.035275] [loss_D: -0.117646] [Loss_G: -0.035745]\n",
      "Steps (30 / 100): [Loss_D_real: -0.319608, Loss_D_fake: 0.037045] [loss_D: -0.141281] [Loss_G: -0.038686]\n",
      "Steps (40 / 100): [Loss_D_real: -0.314377, Loss_D_fake: 0.040924] [loss_D: -0.136727] [Loss_G: -0.039371]\n",
      "Steps (50 / 100): [Loss_D_real: -0.317577, Loss_D_fake: 0.041285] [loss_D: -0.138146] [Loss_G: -0.041688]\n",
      "Steps (60 / 100): [Loss_D_real: -0.336953, Loss_D_fake: 0.045923] [loss_D: -0.145515] [Loss_G: -0.046945]\n",
      "Steps (70 / 100): [Loss_D_real: -0.346438, Loss_D_fake: 0.047015] [loss_D: -0.149712] [Loss_G: -0.049497]\n",
      "Steps (80 / 100): [Loss_D_real: -0.333088, Loss_D_fake: 0.050560] [loss_D: -0.141264] [Loss_G: -0.051704]\n",
      "Steps (90 / 100): [Loss_D_real: -0.358367, Loss_D_fake: 0.053312] [loss_D: -0.152527] [Loss_G: -0.052575]\n",
      "Steps (100 / 100): [Loss_D_real: -0.348376, Loss_D_fake: 0.055370] [loss_D: -0.146503] [Loss_G: -0.057571]\n",
      "EPOCH #  8 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.396233, Loss_D_fake: 0.058508] [loss_D: -0.168863] [Loss_G: -0.059225]\n",
      "Steps (20 / 100): [Loss_D_real: -0.391368, Loss_D_fake: 0.064713] [loss_D: -0.163328] [Loss_G: -0.064049]\n",
      "Steps (30 / 100): [Loss_D_real: -0.413179, Loss_D_fake: 0.067727] [loss_D: -0.172726] [Loss_G: -0.064282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (40 / 100): [Loss_D_real: -0.429590, Loss_D_fake: 0.070169] [loss_D: -0.179711] [Loss_G: -0.070635]\n",
      "Steps (50 / 100): [Loss_D_real: -0.434347, Loss_D_fake: 0.074431] [loss_D: -0.179958] [Loss_G: -0.073998]\n",
      "Steps (60 / 100): [Loss_D_real: -0.433043, Loss_D_fake: 0.075103] [loss_D: -0.178970] [Loss_G: -0.076999]\n",
      "Steps (70 / 100): [Loss_D_real: -0.473358, Loss_D_fake: 0.081093] [loss_D: -0.196132] [Loss_G: -0.081815]\n",
      "Steps (80 / 100): [Loss_D_real: -0.472709, Loss_D_fake: 0.087090] [loss_D: -0.192810] [Loss_G: -0.086016]\n",
      "Steps (90 / 100): [Loss_D_real: -0.493896, Loss_D_fake: 0.089168] [loss_D: -0.202364] [Loss_G: -0.089716]\n",
      "Steps (100 / 100): [Loss_D_real: -0.522904, Loss_D_fake: 0.094729] [loss_D: -0.214088] [Loss_G: -0.091661]\n",
      "EPOCH #  9 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.525642, Loss_D_fake: 0.092483] [loss_D: -0.216580] [Loss_G: -0.095804]\n",
      "Steps (20 / 100): [Loss_D_real: -0.429198, Loss_D_fake: 0.100233] [loss_D: -0.164482] [Loss_G: -0.098222]\n",
      "Steps (30 / 100): [Loss_D_real: -0.487414, Loss_D_fake: 0.106436] [loss_D: -0.190489] [Loss_G: -0.101610]\n",
      "Steps (40 / 100): [Loss_D_real: -0.484107, Loss_D_fake: 0.106268] [loss_D: -0.188919] [Loss_G: -0.103768]\n",
      "Steps (50 / 100): [Loss_D_real: -0.497223, Loss_D_fake: 0.109623] [loss_D: -0.193800] [Loss_G: -0.113261]\n",
      "Steps (60 / 100): [Loss_D_real: -0.545467, Loss_D_fake: 0.115752] [loss_D: -0.214858] [Loss_G: -0.106671]\n",
      "Steps (70 / 100): [Loss_D_real: -0.563333, Loss_D_fake: 0.113579] [loss_D: -0.224877] [Loss_G: -0.114950]\n",
      "Steps (80 / 100): [Loss_D_real: -0.544284, Loss_D_fake: 0.116732] [loss_D: -0.213776] [Loss_G: -0.119914]\n",
      "Steps (90 / 100): [Loss_D_real: -0.481054, Loss_D_fake: 0.117860] [loss_D: -0.181597] [Loss_G: -0.121102]\n",
      "Steps (100 / 100): [Loss_D_real: -0.501014, Loss_D_fake: 0.124595] [loss_D: -0.188209] [Loss_G: -0.123702]\n",
      "EPOCH #  10 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.531215, Loss_D_fake: 0.125829] [loss_D: -0.202693] [Loss_G: -0.120304]\n",
      "Steps (20 / 100): [Loss_D_real: -0.518339, Loss_D_fake: 0.129108] [loss_D: -0.194615] [Loss_G: -0.123875]\n",
      "Steps (30 / 100): [Loss_D_real: -0.521627, Loss_D_fake: 0.133052] [loss_D: -0.194287] [Loss_G: -0.126348]\n",
      "Steps (40 / 100): [Loss_D_real: -0.508550, Loss_D_fake: 0.129526] [loss_D: -0.189512] [Loss_G: -0.131111]\n",
      "Steps (50 / 100): [Loss_D_real: -0.535571, Loss_D_fake: 0.135120] [loss_D: -0.200225] [Loss_G: -0.139758]\n",
      "Steps (60 / 100): [Loss_D_real: -0.558246, Loss_D_fake: 0.136261] [loss_D: -0.210992] [Loss_G: -0.139744]\n",
      "Steps (70 / 100): [Loss_D_real: -0.558147, Loss_D_fake: 0.147174] [loss_D: -0.205486] [Loss_G: -0.140078]\n",
      "Steps (80 / 100): [Loss_D_real: -0.551334, Loss_D_fake: 0.144721] [loss_D: -0.203306] [Loss_G: -0.142464]\n",
      "Steps (90 / 100): [Loss_D_real: -0.571100, Loss_D_fake: 0.147404] [loss_D: -0.211848] [Loss_G: -0.146362]\n",
      "Steps (100 / 100): [Loss_D_real: -0.638089, Loss_D_fake: 0.149464] [loss_D: -0.244312] [Loss_G: -0.151280]\n",
      "EPOCH #  11 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.535906, Loss_D_fake: 0.151689] [loss_D: -0.192109] [Loss_G: -0.146075]\n",
      "Steps (20 / 100): [Loss_D_real: -0.537568, Loss_D_fake: 0.155962] [loss_D: -0.190803] [Loss_G: -0.151801]\n",
      "Steps (30 / 100): [Loss_D_real: -0.531261, Loss_D_fake: 0.157402] [loss_D: -0.186930] [Loss_G: -0.161834]\n",
      "Steps (40 / 100): [Loss_D_real: -0.503820, Loss_D_fake: 0.147626] [loss_D: -0.178097] [Loss_G: -0.157026]\n",
      "Steps (50 / 100): [Loss_D_real: -0.518981, Loss_D_fake: 0.160900] [loss_D: -0.179041] [Loss_G: -0.155530]\n",
      "Steps (60 / 100): [Loss_D_real: -0.572013, Loss_D_fake: 0.161452] [loss_D: -0.205281] [Loss_G: -0.162644]\n",
      "Steps (70 / 100): [Loss_D_real: -0.577589, Loss_D_fake: 0.165217] [loss_D: -0.206186] [Loss_G: -0.168860]\n",
      "Steps (80 / 100): [Loss_D_real: -0.531552, Loss_D_fake: 0.173563] [loss_D: -0.178994] [Loss_G: -0.165631]\n",
      "Steps (90 / 100): [Loss_D_real: -0.558005, Loss_D_fake: 0.173368] [loss_D: -0.192319] [Loss_G: -0.172271]\n",
      "Steps (100 / 100): [Loss_D_real: -0.532218, Loss_D_fake: 0.172494] [loss_D: -0.179862] [Loss_G: -0.173585]\n",
      "EPOCH #  12 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.566832, Loss_D_fake: 0.177666] [loss_D: -0.194583] [Loss_G: -0.179802]\n",
      "Steps (20 / 100): [Loss_D_real: -0.550532, Loss_D_fake: 0.180856] [loss_D: -0.184838] [Loss_G: -0.182508]\n",
      "Steps (30 / 100): [Loss_D_real: -0.521498, Loss_D_fake: 0.182713] [loss_D: -0.169392] [Loss_G: -0.185243]\n",
      "Steps (40 / 100): [Loss_D_real: -0.510366, Loss_D_fake: 0.193356] [loss_D: -0.158505] [Loss_G: -0.188769]\n",
      "Steps (50 / 100): [Loss_D_real: -0.514269, Loss_D_fake: 0.183629] [loss_D: -0.165320] [Loss_G: -0.189077]\n",
      "Steps (60 / 100): [Loss_D_real: -0.564200, Loss_D_fake: 0.195320] [loss_D: -0.184440] [Loss_G: -0.192738]\n",
      "Steps (70 / 100): [Loss_D_real: -0.498379, Loss_D_fake: 0.191600] [loss_D: -0.153389] [Loss_G: -0.190593]\n",
      "Steps (80 / 100): [Loss_D_real: -0.590978, Loss_D_fake: 0.197377] [loss_D: -0.196801] [Loss_G: -0.192515]\n",
      "Steps (90 / 100): [Loss_D_real: -0.537146, Loss_D_fake: 0.205234] [loss_D: -0.165956] [Loss_G: -0.193669]\n",
      "Steps (100 / 100): [Loss_D_real: -0.509562, Loss_D_fake: 0.208766] [loss_D: -0.150398] [Loss_G: -0.205949]\n",
      "EPOCH #  13 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.495700, Loss_D_fake: 0.201069] [loss_D: -0.147316] [Loss_G: -0.204124]\n",
      "Steps (20 / 100): [Loss_D_real: -0.541610, Loss_D_fake: 0.204997] [loss_D: -0.168307] [Loss_G: -0.208706]\n",
      "Steps (30 / 100): [Loss_D_real: -0.532596, Loss_D_fake: 0.215146] [loss_D: -0.158725] [Loss_G: -0.213791]\n",
      "Steps (40 / 100): [Loss_D_real: -0.538026, Loss_D_fake: 0.217912] [loss_D: -0.160057] [Loss_G: -0.207039]\n",
      "Steps (50 / 100): [Loss_D_real: -0.548578, Loss_D_fake: 0.223492] [loss_D: -0.162543] [Loss_G: -0.215331]\n",
      "Steps (60 / 100): [Loss_D_real: -0.527969, Loss_D_fake: 0.216049] [loss_D: -0.155960] [Loss_G: -0.218073]\n",
      "Steps (70 / 100): [Loss_D_real: -0.572269, Loss_D_fake: 0.229640] [loss_D: -0.171315] [Loss_G: -0.222423]\n",
      "Steps (80 / 100): [Loss_D_real: -0.551529, Loss_D_fake: 0.228058] [loss_D: -0.161736] [Loss_G: -0.220574]\n",
      "Steps (90 / 100): [Loss_D_real: -0.521217, Loss_D_fake: 0.231528] [loss_D: -0.144845] [Loss_G: -0.221830]\n",
      "Steps (100 / 100): [Loss_D_real: -0.543505, Loss_D_fake: 0.231664] [loss_D: -0.155920] [Loss_G: -0.222605]\n",
      "EPOCH #  14 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.499309, Loss_D_fake: 0.234540] [loss_D: -0.132385] [Loss_G: -0.224566]\n",
      "Steps (20 / 100): [Loss_D_real: -0.496875, Loss_D_fake: 0.238103] [loss_D: -0.129386] [Loss_G: -0.229263]\n",
      "Steps (30 / 100): [Loss_D_real: -0.517726, Loss_D_fake: 0.236247] [loss_D: -0.140739] [Loss_G: -0.240290]\n",
      "Steps (40 / 100): [Loss_D_real: -0.526819, Loss_D_fake: 0.242881] [loss_D: -0.141969] [Loss_G: -0.236429]\n",
      "Steps (50 / 100): [Loss_D_real: -0.591180, Loss_D_fake: 0.239075] [loss_D: -0.176052] [Loss_G: -0.235638]\n",
      "Steps (60 / 100): [Loss_D_real: -0.521654, Loss_D_fake: 0.244061] [loss_D: -0.138797] [Loss_G: -0.247040]\n",
      "Steps (70 / 100): [Loss_D_real: -0.555371, Loss_D_fake: 0.253013] [loss_D: -0.151179] [Loss_G: -0.247029]\n",
      "Steps (80 / 100): [Loss_D_real: -0.512760, Loss_D_fake: 0.257709] [loss_D: -0.127525] [Loss_G: -0.249032]\n",
      "Steps (90 / 100): [Loss_D_real: -0.490547, Loss_D_fake: 0.254345] [loss_D: -0.118101] [Loss_G: -0.261405]\n",
      "Steps (100 / 100): [Loss_D_real: -0.508516, Loss_D_fake: 0.256096] [loss_D: -0.126210] [Loss_G: -0.265919]\n",
      "EPOCH #  15 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.626931, Loss_D_fake: 0.261558] [loss_D: -0.182687] [Loss_G: -0.264789]\n",
      "Steps (20 / 100): [Loss_D_real: -0.530560, Loss_D_fake: 0.266452] [loss_D: -0.132054] [Loss_G: -0.249670]\n",
      "Steps (30 / 100): [Loss_D_real: -0.526016, Loss_D_fake: 0.275560] [loss_D: -0.125228] [Loss_G: -0.274844]\n",
      "Steps (40 / 100): [Loss_D_real: -0.502531, Loss_D_fake: 0.277871] [loss_D: -0.112330] [Loss_G: -0.264657]\n",
      "Steps (50 / 100): [Loss_D_real: -0.493584, Loss_D_fake: 0.270748] [loss_D: -0.111418] [Loss_G: -0.258825]\n",
      "Steps (60 / 100): [Loss_D_real: -0.577266, Loss_D_fake: 0.279857] [loss_D: -0.148705] [Loss_G: -0.265575]\n",
      "Steps (70 / 100): [Loss_D_real: -0.530847, Loss_D_fake: 0.267859] [loss_D: -0.131494] [Loss_G: -0.279167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (80 / 100): [Loss_D_real: -0.533130, Loss_D_fake: 0.283724] [loss_D: -0.124703] [Loss_G: -0.279952]\n",
      "Steps (90 / 100): [Loss_D_real: -0.532214, Loss_D_fake: 0.290887] [loss_D: -0.120663] [Loss_G: -0.283596]\n",
      "Steps (100 / 100): [Loss_D_real: -0.533592, Loss_D_fake: 0.290791] [loss_D: -0.121401] [Loss_G: -0.278192]\n",
      "EPOCH #  16 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.495205, Loss_D_fake: 0.289416] [loss_D: -0.102894] [Loss_G: -0.282818]\n",
      "Steps (20 / 100): [Loss_D_real: -0.522162, Loss_D_fake: 0.287018] [loss_D: -0.117572] [Loss_G: -0.282321]\n",
      "Steps (30 / 100): [Loss_D_real: -0.532921, Loss_D_fake: 0.308699] [loss_D: -0.112111] [Loss_G: -0.288539]\n",
      "Steps (40 / 100): [Loss_D_real: -0.503958, Loss_D_fake: 0.308547] [loss_D: -0.097706] [Loss_G: -0.298276]\n",
      "Steps (50 / 100): [Loss_D_real: -0.525074, Loss_D_fake: 0.303890] [loss_D: -0.110592] [Loss_G: -0.293342]\n",
      "Steps (60 / 100): [Loss_D_real: -0.506355, Loss_D_fake: 0.317411] [loss_D: -0.094472] [Loss_G: -0.302998]\n",
      "Steps (70 / 100): [Loss_D_real: -0.525423, Loss_D_fake: 0.307226] [loss_D: -0.109098] [Loss_G: -0.299805]\n",
      "Steps (80 / 100): [Loss_D_real: -0.523432, Loss_D_fake: 0.320666] [loss_D: -0.101383] [Loss_G: -0.308460]\n",
      "Steps (90 / 100): [Loss_D_real: -0.527136, Loss_D_fake: 0.321269] [loss_D: -0.102933] [Loss_G: -0.302370]\n",
      "Steps (100 / 100): [Loss_D_real: -0.533013, Loss_D_fake: 0.331567] [loss_D: -0.100723] [Loss_G: -0.313807]\n",
      "EPOCH #  17 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.550452, Loss_D_fake: 0.316670] [loss_D: -0.116891] [Loss_G: -0.315906]\n",
      "Steps (20 / 100): [Loss_D_real: -0.498997, Loss_D_fake: 0.320663] [loss_D: -0.089167] [Loss_G: -0.319154]\n",
      "Steps (30 / 100): [Loss_D_real: -0.517627, Loss_D_fake: 0.317480] [loss_D: -0.100074] [Loss_G: -0.316841]\n",
      "Steps (40 / 100): [Loss_D_real: -0.501198, Loss_D_fake: 0.321516] [loss_D: -0.089841] [Loss_G: -0.323480]\n",
      "Steps (50 / 100): [Loss_D_real: -0.478522, Loss_D_fake: 0.335467] [loss_D: -0.071528] [Loss_G: -0.329108]\n",
      "Steps (60 / 100): [Loss_D_real: -0.505167, Loss_D_fake: 0.328563] [loss_D: -0.088302] [Loss_G: -0.327686]\n",
      "Steps (70 / 100): [Loss_D_real: -0.556810, Loss_D_fake: 0.331374] [loss_D: -0.112718] [Loss_G: -0.337476]\n",
      "Steps (80 / 100): [Loss_D_real: -0.488652, Loss_D_fake: 0.340312] [loss_D: -0.074170] [Loss_G: -0.336463]\n",
      "Steps (90 / 100): [Loss_D_real: -0.515267, Loss_D_fake: 0.344788] [loss_D: -0.085239] [Loss_G: -0.339352]\n",
      "Steps (100 / 100): [Loss_D_real: -0.544216, Loss_D_fake: 0.342472] [loss_D: -0.100872] [Loss_G: -0.339542]\n",
      "EPOCH #  18 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.488940, Loss_D_fake: 0.348142] [loss_D: -0.070399] [Loss_G: -0.348631]\n",
      "Steps (20 / 100): [Loss_D_real: -0.513494, Loss_D_fake: 0.356611] [loss_D: -0.078442] [Loss_G: -0.344795]\n",
      "Steps (30 / 100): [Loss_D_real: -0.518604, Loss_D_fake: 0.343384] [loss_D: -0.087610] [Loss_G: -0.356565]\n",
      "Steps (40 / 100): [Loss_D_real: -0.556748, Loss_D_fake: 0.347622] [loss_D: -0.104563] [Loss_G: -0.355677]\n",
      "Steps (50 / 100): [Loss_D_real: -0.516212, Loss_D_fake: 0.361465] [loss_D: -0.077373] [Loss_G: -0.369151]\n",
      "Steps (60 / 100): [Loss_D_real: -0.520164, Loss_D_fake: 0.365796] [loss_D: -0.077184] [Loss_G: -0.359203]\n",
      "Steps (70 / 100): [Loss_D_real: -0.478772, Loss_D_fake: 0.369069] [loss_D: -0.054851] [Loss_G: -0.357012]\n",
      "Steps (80 / 100): [Loss_D_real: -0.497173, Loss_D_fake: 0.352767] [loss_D: -0.072203] [Loss_G: -0.361839]\n",
      "Steps (90 / 100): [Loss_D_real: -0.536976, Loss_D_fake: 0.367660] [loss_D: -0.084658] [Loss_G: -0.362247]\n",
      "Steps (100 / 100): [Loss_D_real: -0.486851, Loss_D_fake: 0.362757] [loss_D: -0.062047] [Loss_G: -0.361580]\n",
      "EPOCH #  19 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.482288, Loss_D_fake: 0.372480] [loss_D: -0.054904] [Loss_G: -0.364963]\n",
      "Steps (20 / 100): [Loss_D_real: -0.548189, Loss_D_fake: 0.379472] [loss_D: -0.084359] [Loss_G: -0.359932]\n",
      "Steps (30 / 100): [Loss_D_real: -0.494762, Loss_D_fake: 0.372197] [loss_D: -0.061282] [Loss_G: -0.374768]\n",
      "Steps (40 / 100): [Loss_D_real: -0.521850, Loss_D_fake: 0.368361] [loss_D: -0.076745] [Loss_G: -0.368385]\n",
      "Steps (50 / 100): [Loss_D_real: -0.463223, Loss_D_fake: 0.378575] [loss_D: -0.042324] [Loss_G: -0.381251]\n",
      "Steps (60 / 100): [Loss_D_real: -0.500462, Loss_D_fake: 0.375942] [loss_D: -0.062260] [Loss_G: -0.373699]\n",
      "Steps (70 / 100): [Loss_D_real: -0.479801, Loss_D_fake: 0.384275] [loss_D: -0.047763] [Loss_G: -0.389126]\n",
      "Steps (80 / 100): [Loss_D_real: -0.486542, Loss_D_fake: 0.382847] [loss_D: -0.051847] [Loss_G: -0.372131]\n",
      "Steps (90 / 100): [Loss_D_real: -0.495300, Loss_D_fake: 0.385033] [loss_D: -0.055134] [Loss_G: -0.377594]\n",
      "Steps (100 / 100): [Loss_D_real: -0.496257, Loss_D_fake: 0.392393] [loss_D: -0.051932] [Loss_G: -0.389488]\n",
      "EPOCH #  20 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.494366, Loss_D_fake: 0.387558] [loss_D: -0.053404] [Loss_G: -0.389853]\n",
      "Steps (20 / 100): [Loss_D_real: -0.452661, Loss_D_fake: 0.400125] [loss_D: -0.026268] [Loss_G: -0.384335]\n",
      "Steps (30 / 100): [Loss_D_real: -0.493970, Loss_D_fake: 0.389575] [loss_D: -0.052197] [Loss_G: -0.404711]\n",
      "Steps (40 / 100): [Loss_D_real: -0.467702, Loss_D_fake: 0.416180] [loss_D: -0.025761] [Loss_G: -0.379995]\n",
      "Steps (50 / 100): [Loss_D_real: -0.465818, Loss_D_fake: 0.405849] [loss_D: -0.029984] [Loss_G: -0.389340]\n",
      "Steps (60 / 100): [Loss_D_real: -0.480280, Loss_D_fake: 0.401465] [loss_D: -0.039407] [Loss_G: -0.387807]\n",
      "Steps (70 / 100): [Loss_D_real: -0.454519, Loss_D_fake: 0.390782] [loss_D: -0.031868] [Loss_G: -0.404876]\n",
      "Steps (80 / 100): [Loss_D_real: -0.469890, Loss_D_fake: 0.405267] [loss_D: -0.032311] [Loss_G: -0.392758]\n",
      "Steps (90 / 100): [Loss_D_real: -0.468460, Loss_D_fake: 0.393147] [loss_D: -0.037657] [Loss_G: -0.401485]\n",
      "Steps (100 / 100): [Loss_D_real: -0.499510, Loss_D_fake: 0.407305] [loss_D: -0.046103] [Loss_G: -0.407437]\n",
      "EPOCH #  21 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.507586, Loss_D_fake: 0.405380] [loss_D: -0.051103] [Loss_G: -0.393991]\n",
      "Steps (20 / 100): [Loss_D_real: -0.499017, Loss_D_fake: 0.400726] [loss_D: -0.049146] [Loss_G: -0.415324]\n",
      "Steps (30 / 100): [Loss_D_real: -0.496506, Loss_D_fake: 0.401358] [loss_D: -0.047574] [Loss_G: -0.414067]\n",
      "Steps (40 / 100): [Loss_D_real: -0.455122, Loss_D_fake: 0.405508] [loss_D: -0.024807] [Loss_G: -0.388770]\n",
      "Steps (50 / 100): [Loss_D_real: -0.457637, Loss_D_fake: 0.415593] [loss_D: -0.021022] [Loss_G: -0.404662]\n",
      "Steps (60 / 100): [Loss_D_real: -0.454427, Loss_D_fake: 0.401794] [loss_D: -0.026317] [Loss_G: -0.400584]\n",
      "Steps (70 / 100): [Loss_D_real: -0.478284, Loss_D_fake: 0.416539] [loss_D: -0.030872] [Loss_G: -0.397805]\n",
      "Steps (80 / 100): [Loss_D_real: -0.454005, Loss_D_fake: 0.431265] [loss_D: -0.011370] [Loss_G: -0.393306]\n",
      "Steps (90 / 100): [Loss_D_real: -0.433589, Loss_D_fake: 0.400619] [loss_D: -0.016485] [Loss_G: -0.402525]\n",
      "Steps (100 / 100): [Loss_D_real: -0.460730, Loss_D_fake: 0.395306] [loss_D: -0.032712] [Loss_G: -0.413565]\n",
      "EPOCH #  22 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.444187, Loss_D_fake: 0.413876] [loss_D: -0.015156] [Loss_G: -0.417699]\n",
      "Steps (20 / 100): [Loss_D_real: -0.437226, Loss_D_fake: 0.404819] [loss_D: -0.016204] [Loss_G: -0.405601]\n",
      "Steps (30 / 100): [Loss_D_real: -0.470244, Loss_D_fake: 0.402926] [loss_D: -0.033659] [Loss_G: -0.403018]\n",
      "Steps (40 / 100): [Loss_D_real: -0.442405, Loss_D_fake: 0.421251] [loss_D: -0.010577] [Loss_G: -0.399710]\n",
      "Steps (50 / 100): [Loss_D_real: -0.440199, Loss_D_fake: 0.425645] [loss_D: -0.007277] [Loss_G: -0.416689]\n",
      "Steps (60 / 100): [Loss_D_real: -0.473465, Loss_D_fake: 0.416112] [loss_D: -0.028677] [Loss_G: -0.414736]\n",
      "Steps (70 / 100): [Loss_D_real: -0.441259, Loss_D_fake: 0.408238] [loss_D: -0.016511] [Loss_G: -0.400464]\n",
      "Steps (80 / 100): [Loss_D_real: -0.478643, Loss_D_fake: 0.399875] [loss_D: -0.039384] [Loss_G: -0.392927]\n",
      "Steps (90 / 100): [Loss_D_real: -0.406708, Loss_D_fake: 0.400804] [loss_D: -0.002952] [Loss_G: -0.407857]\n",
      "Steps (100 / 100): [Loss_D_real: -0.402429, Loss_D_fake: 0.409682] [loss_D: 0.003627] [Loss_G: -0.422844]\n",
      "EPOCH #  23 --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (10 / 100): [Loss_D_real: -0.383561, Loss_D_fake: 0.405564] [loss_D: 0.011002] [Loss_G: -0.407586]\n",
      "Steps (20 / 100): [Loss_D_real: -0.387783, Loss_D_fake: 0.400810] [loss_D: 0.006514] [Loss_G: -0.403273]\n",
      "Steps (30 / 100): [Loss_D_real: -0.448515, Loss_D_fake: 0.402118] [loss_D: -0.023199] [Loss_G: -0.411359]\n",
      "Steps (40 / 100): [Loss_D_real: -0.416860, Loss_D_fake: 0.412109] [loss_D: -0.002375] [Loss_G: -0.397570]\n",
      "Steps (50 / 100): [Loss_D_real: -0.438488, Loss_D_fake: 0.403005] [loss_D: -0.017742] [Loss_G: -0.412979]\n",
      "Steps (60 / 100): [Loss_D_real: -0.399133, Loss_D_fake: 0.394874] [loss_D: -0.002130] [Loss_G: -0.404954]\n",
      "Steps (70 / 100): [Loss_D_real: -0.410238, Loss_D_fake: 0.397681] [loss_D: -0.006278] [Loss_G: -0.404110]\n",
      "Steps (80 / 100): [Loss_D_real: -0.411013, Loss_D_fake: 0.407505] [loss_D: -0.001754] [Loss_G: -0.398273]\n",
      "Steps (90 / 100): [Loss_D_real: -0.380009, Loss_D_fake: 0.413161] [loss_D: 0.016576] [Loss_G: -0.397908]\n",
      "Steps (100 / 100): [Loss_D_real: -0.386745, Loss_D_fake: 0.398151] [loss_D: 0.005703] [Loss_G: -0.394332]\n",
      "EPOCH #  24 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.383693, Loss_D_fake: 0.381946] [loss_D: -0.000874] [Loss_G: -0.402227]\n",
      "Steps (20 / 100): [Loss_D_real: -0.372156, Loss_D_fake: 0.381438] [loss_D: 0.004641] [Loss_G: -0.387583]\n",
      "Steps (30 / 100): [Loss_D_real: -0.393963, Loss_D_fake: 0.397050] [loss_D: 0.001544] [Loss_G: -0.385365]\n",
      "Steps (40 / 100): [Loss_D_real: -0.364291, Loss_D_fake: 0.393196] [loss_D: 0.014453] [Loss_G: -0.382662]\n",
      "Steps (50 / 100): [Loss_D_real: -0.394991, Loss_D_fake: 0.370554] [loss_D: -0.012219] [Loss_G: -0.379108]\n",
      "Steps (60 / 100): [Loss_D_real: -0.394907, Loss_D_fake: 0.387784] [loss_D: -0.003561] [Loss_G: -0.381537]\n",
      "Steps (70 / 100): [Loss_D_real: -0.360311, Loss_D_fake: 0.377335] [loss_D: 0.008512] [Loss_G: -0.356720]\n",
      "Steps (80 / 100): [Loss_D_real: -0.349043, Loss_D_fake: 0.371596] [loss_D: 0.011276] [Loss_G: -0.360944]\n",
      "Steps (90 / 100): [Loss_D_real: -0.349122, Loss_D_fake: 0.370581] [loss_D: 0.010729] [Loss_G: -0.356007]\n",
      "Steps (100 / 100): [Loss_D_real: -0.366148, Loss_D_fake: 0.353446] [loss_D: -0.006351] [Loss_G: -0.366471]\n",
      "EPOCH #  25 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.341343, Loss_D_fake: 0.354391] [loss_D: 0.006524] [Loss_G: -0.354556]\n",
      "Steps (20 / 100): [Loss_D_real: -0.338171, Loss_D_fake: 0.359762] [loss_D: 0.010795] [Loss_G: -0.356082]\n",
      "Steps (30 / 100): [Loss_D_real: -0.341883, Loss_D_fake: 0.355991] [loss_D: 0.007054] [Loss_G: -0.344766]\n",
      "Steps (40 / 100): [Loss_D_real: -0.376390, Loss_D_fake: 0.338737] [loss_D: -0.018827] [Loss_G: -0.344352]\n",
      "Steps (50 / 100): [Loss_D_real: -0.324772, Loss_D_fake: 0.337368] [loss_D: 0.006298] [Loss_G: -0.339929]\n",
      "Steps (60 / 100): [Loss_D_real: -0.342895, Loss_D_fake: 0.336646] [loss_D: -0.003124] [Loss_G: -0.331621]\n",
      "Steps (70 / 100): [Loss_D_real: -0.308424, Loss_D_fake: 0.326584] [loss_D: 0.009080] [Loss_G: -0.316403]\n",
      "Steps (80 / 100): [Loss_D_real: -0.312868, Loss_D_fake: 0.321104] [loss_D: 0.004118] [Loss_G: -0.327871]\n",
      "Steps (90 / 100): [Loss_D_real: -0.316464, Loss_D_fake: 0.325571] [loss_D: 0.004554] [Loss_G: -0.323458]\n",
      "Steps (100 / 100): [Loss_D_real: -0.304579, Loss_D_fake: 0.320970] [loss_D: 0.008195] [Loss_G: -0.320510]\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN(g_model=G_wgan, d_model=D_wgan)\n",
    "\n",
    "EPOCHS = 25\n",
    "X_train_fraud = x_train_fraud.to_numpy()\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH # ', epoch + 1, '-' * 50)\n",
    "    wgan.train(X_train_fraud, batch_size=128, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.generator.save('wgan_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.014839</td>\n",
       "      <td>-6.305023</td>\n",
       "      <td>10.341335</td>\n",
       "      <td>-8.574209</td>\n",
       "      <td>8.299900</td>\n",
       "      <td>-9.044720</td>\n",
       "      <td>-4.633235</td>\n",
       "      <td>-11.389522</td>\n",
       "      <td>0.204499</td>\n",
       "      <td>-7.789335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>-0.014080</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>-0.073361</td>\n",
       "      <td>-0.092199</td>\n",
       "      <td>-0.017477</td>\n",
       "      <td>0.106953</td>\n",
       "      <td>0.167389</td>\n",
       "      <td>0.107494</td>\n",
       "      <td>3.427012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.801548</td>\n",
       "      <td>1.402232</td>\n",
       "      <td>2.401234</td>\n",
       "      <td>2.029324</td>\n",
       "      <td>1.945500</td>\n",
       "      <td>2.128428</td>\n",
       "      <td>1.141382</td>\n",
       "      <td>2.698648</td>\n",
       "      <td>0.297286</td>\n",
       "      <td>1.878873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359798</td>\n",
       "      <td>0.309235</td>\n",
       "      <td>0.372441</td>\n",
       "      <td>0.339191</td>\n",
       "      <td>0.301301</td>\n",
       "      <td>0.367543</td>\n",
       "      <td>0.265344</td>\n",
       "      <td>0.338120</td>\n",
       "      <td>0.396265</td>\n",
       "      <td>0.842888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.049931</td>\n",
       "      <td>-11.222009</td>\n",
       "      <td>3.486689</td>\n",
       "      <td>-15.068597</td>\n",
       "      <td>2.739768</td>\n",
       "      <td>-16.394028</td>\n",
       "      <td>-8.617829</td>\n",
       "      <td>-20.925287</td>\n",
       "      <td>-1.078363</td>\n",
       "      <td>-14.348653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084793</td>\n",
       "      <td>-0.914717</td>\n",
       "      <td>-1.099021</td>\n",
       "      <td>-1.171458</td>\n",
       "      <td>-0.924914</td>\n",
       "      <td>-1.043308</td>\n",
       "      <td>-0.764414</td>\n",
       "      <td>-1.107698</td>\n",
       "      <td>-1.326581</td>\n",
       "      <td>1.401648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.999061</td>\n",
       "      <td>-7.196817</td>\n",
       "      <td>8.626605</td>\n",
       "      <td>-9.757539</td>\n",
       "      <td>6.917021</td>\n",
       "      <td>-10.372511</td>\n",
       "      <td>-5.322743</td>\n",
       "      <td>-13.013300</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>-8.923686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726711</td>\n",
       "      <td>-0.233750</td>\n",
       "      <td>-0.253383</td>\n",
       "      <td>-0.290629</td>\n",
       "      <td>-0.300965</td>\n",
       "      <td>-0.269029</td>\n",
       "      <td>-0.079615</td>\n",
       "      <td>-0.061865</td>\n",
       "      <td>-0.155170</td>\n",
       "      <td>2.834129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.807904</td>\n",
       "      <td>-6.215917</td>\n",
       "      <td>10.205181</td>\n",
       "      <td>-8.455754</td>\n",
       "      <td>8.195798</td>\n",
       "      <td>-8.812211</td>\n",
       "      <td>-4.535729</td>\n",
       "      <td>-11.154415</td>\n",
       "      <td>0.200554</td>\n",
       "      <td>-7.562764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960777</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>-0.005370</td>\n",
       "      <td>-0.069215</td>\n",
       "      <td>-0.107100</td>\n",
       "      <td>-0.052621</td>\n",
       "      <td>0.096420</td>\n",
       "      <td>0.168851</td>\n",
       "      <td>0.125779</td>\n",
       "      <td>3.344017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.763011</td>\n",
       "      <td>-5.354254</td>\n",
       "      <td>11.860113</td>\n",
       "      <td>-7.109643</td>\n",
       "      <td>9.497823</td>\n",
       "      <td>-7.527036</td>\n",
       "      <td>-3.848781</td>\n",
       "      <td>-9.431325</td>\n",
       "      <td>0.409282</td>\n",
       "      <td>-6.445704</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200804</td>\n",
       "      <td>0.187754</td>\n",
       "      <td>0.240286</td>\n",
       "      <td>0.136887</td>\n",
       "      <td>0.120294</td>\n",
       "      <td>0.216406</td>\n",
       "      <td>0.272310</td>\n",
       "      <td>0.394001</td>\n",
       "      <td>0.376652</td>\n",
       "      <td>3.947572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.611126</td>\n",
       "      <td>-2.239507</td>\n",
       "      <td>18.201218</td>\n",
       "      <td>-2.895845</td>\n",
       "      <td>14.578374</td>\n",
       "      <td>-3.071097</td>\n",
       "      <td>-1.530158</td>\n",
       "      <td>-3.991794</td>\n",
       "      <td>1.102077</td>\n",
       "      <td>-2.510722</td>\n",
       "      <td>...</td>\n",
       "      <td>2.296494</td>\n",
       "      <td>1.050034</td>\n",
       "      <td>1.329247</td>\n",
       "      <td>0.914874</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>1.582470</td>\n",
       "      <td>0.948782</td>\n",
       "      <td>1.246364</td>\n",
       "      <td>1.279843</td>\n",
       "      <td>6.344531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     12.014839    -6.305023    10.341335    -8.574209     8.299900   \n",
       "std       2.801548     1.402232     2.401234     2.029324     1.945500   \n",
       "min       4.049931   -11.222009     3.486689   -15.068597     2.739768   \n",
       "25%       9.999061    -7.196817     8.626605    -9.757539     6.917021   \n",
       "50%      11.807904    -6.215917    10.205181    -8.455754     8.195798   \n",
       "75%      13.763011    -5.354254    11.860113    -7.109643     9.497823   \n",
       "max      21.611126    -2.239507    18.201218    -2.895845    14.578374   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -9.044720    -4.633235   -11.389522     0.204499    -7.789335  ...   \n",
       "std       2.128428     1.141382     2.698648     0.297286     1.878873  ...   \n",
       "min     -16.394028    -8.617829   -20.925287    -1.078363   -14.348653  ...   \n",
       "25%     -10.372511    -5.322743   -13.013300     0.008828    -8.923686  ...   \n",
       "50%      -8.812211    -4.535729   -11.154415     0.200554    -7.562764  ...   \n",
       "75%      -7.527036    -3.848781    -9.431325     0.409282    -6.445704  ...   \n",
       "max      -3.071097    -1.530158    -3.991794     1.102077    -2.510722  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.975866    -0.014080     0.002197    -0.073361    -0.092199   \n",
       "std       0.359798     0.309235     0.372441     0.339191     0.301301   \n",
       "min      -0.084793    -0.914717    -1.099021    -1.171458    -0.924914   \n",
       "25%       0.726711    -0.233750    -0.253383    -0.290629    -0.300965   \n",
       "50%       0.960777    -0.014733    -0.005370    -0.069215    -0.107100   \n",
       "75%       1.200804     0.187754     0.240286     0.136887     0.120294   \n",
       "max       2.296494     1.050034     1.329247     0.914874     0.953445   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.017477     0.106953     0.167389     0.107494     3.427012  \n",
       "std       0.367543     0.265344     0.338120     0.396265     0.842888  \n",
       "min      -1.043308    -0.764414    -1.107698    -1.326581     1.401648  \n",
       "25%      -0.269029    -0.079615    -0.061865    -0.155170     2.834129  \n",
       "50%      -0.052621     0.096420     0.168851     0.125779     3.344017  \n",
       "75%       0.216406     0.272310     0.394001     0.376652     3.947572  \n",
       "max       1.582470     0.948782     1.246364     1.279843     6.344531  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_1000wgan, x_train_gen_1000wgan, y_train_gen_1000wgan = gen_data(wgan.generator, 1000)\n",
    "df_gen_1000wgan = pd.DataFrame(data=gen_1000wgan, index=None, columns=x_train.columns)\n",
    "df_gen_1000wgan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995435553526912\n",
      "Precision:  0.8673469387755102\n",
      "Recall:  0.8673469387755102\n",
      "F1 score:  0.8673469387755102\n",
      "ROC AUC score:  0.9335591615655828\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_1000wgan = XGBC_model_predit(x_train_gen_1000wgan, y_train_gen_1000wgan)\n",
    "check_performance(y_test, y_pred_gen_1000wgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_1000wgan, 'WGAN 1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP  \n",
    "modify WGAN loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: 51.8762 - g_loss: -0.0168\n",
      "Epoch 2/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 51.0969 - g_loss: -0.0048\n",
      "Epoch 3/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 46.8410 - g_loss: -0.0324\n",
      "Epoch 4/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 45.9880 - g_loss: -0.0490\n",
      "Epoch 5/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: 43.0220 - g_loss: -0.0327\n",
      "Epoch 6/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 41.3529 - g_loss: -0.0263\n",
      "Epoch 7/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 40.1561 - g_loss: -0.0232\n",
      "Epoch 8/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 37.3178 - g_loss: -0.0380\n",
      "Epoch 9/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: 35.0866 - g_loss: -0.0322\n",
      "Epoch 10/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: 34.8189 - g_loss: -0.0229\n",
      "Epoch 11/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: 32.2977 - g_loss: -0.0449\n",
      "Epoch 12/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: 30.8223 - g_loss: -0.0321\n",
      "Epoch 13/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: 29.7480 - g_loss: -0.0339\n",
      "Epoch 14/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: 27.7667 - g_loss: -0.0380\n",
      "Epoch 15/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: 26.9896 - g_loss: -0.0544\n",
      "Epoch 16/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: 25.0773 - g_loss: -0.0415\n",
      "Epoch 17/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: 23.7741 - g_loss: -0.0421\n",
      "Epoch 18/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 22.1540 - g_loss: -0.0570\n",
      "Epoch 19/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: 21.3615 - g_loss: -0.0459\n",
      "Epoch 20/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 20.1755 - g_loss: -0.0522\n",
      "Epoch 21/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 18.8252 - g_loss: -0.0678\n",
      "Epoch 22/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 17.6672 - g_loss: -0.0519\n",
      "Epoch 23/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 17.1827 - g_loss: -0.0760\n",
      "Epoch 24/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 16.0952 - g_loss: -0.0532\n",
      "Epoch 25/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 15.1982 - g_loss: -0.0523\n",
      "Epoch 26/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 14.2140 - g_loss: -0.0435\n",
      "Epoch 27/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 13.3250 - g_loss: -0.0562\n",
      "Epoch 28/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 12.3445 - g_loss: -0.0612\n",
      "Epoch 29/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 11.9493 - g_loss: -0.0606\n",
      "Epoch 30/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 11.1603 - g_loss: -0.0511\n",
      "Epoch 31/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 10.6035 - g_loss: -0.0636\n",
      "Epoch 32/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: 9.9453 - g_loss: -0.0543\n",
      "Epoch 33/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: 9.4299 - g_loss: -0.0568\n",
      "Epoch 34/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 8.6485 - g_loss: -0.0579\n",
      "Epoch 35/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: 8.4232 - g_loss: -0.0708\n",
      "Epoch 36/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 7.7500 - g_loss: -0.0533\n",
      "Epoch 37/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 7.4678 - g_loss: -0.0488\n",
      "Epoch 38/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 7.0247 - g_loss: -0.0633\n",
      "Epoch 39/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 6.2781 - g_loss: -0.0547\n",
      "Epoch 40/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 6.3337 - g_loss: -0.0583\n",
      "Epoch 41/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 5.9008 - g_loss: -0.0577\n",
      "Epoch 42/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 5.6708 - g_loss: -0.0543\n",
      "Epoch 43/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 5.3251 - g_loss: -0.0587\n",
      "Epoch 44/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 4.9431 - g_loss: -0.0515\n",
      "Epoch 45/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 4.6108 - g_loss: -0.0668\n",
      "Epoch 46/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 4.3317 - g_loss: -0.0669\n",
      "Epoch 47/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 4.3121 - g_loss: -0.0595\n",
      "Epoch 48/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 3.9277 - g_loss: -0.0602\n",
      "Epoch 49/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 3.9139 - g_loss: -0.0703\n",
      "Epoch 50/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 3.8941 - g_loss: -0.0657\n",
      "Epoch 51/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 3.5291 - g_loss: -0.0722\n",
      "Epoch 52/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 3.4571 - g_loss: -0.0600\n",
      "Epoch 53/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 3.1188 - g_loss: -0.0637\n",
      "Epoch 54/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 2.8353 - g_loss: -0.0686\n",
      "Epoch 55/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 2.8780 - g_loss: -0.0753\n",
      "Epoch 56/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 2.4668 - g_loss: -0.0657\n",
      "Epoch 57/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 2.4070 - g_loss: -0.0847\n",
      "Epoch 58/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 2.2312 - g_loss: -0.0696\n",
      "Epoch 59/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 2.2093 - g_loss: -0.0714\n",
      "Epoch 60/1200\n",
      "4/4 [==============================] - 0s 37ms/step - d_loss: 2.2280 - g_loss: -0.0820\n",
      "Epoch 61/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: 2.0803 - g_loss: -0.0810\n",
      "Epoch 62/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: 1.9374 - g_loss: -0.0743\n",
      "Epoch 63/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: 1.8436 - g_loss: -0.0839\n",
      "Epoch 64/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: 1.5537 - g_loss: -0.0777\n",
      "Epoch 65/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: 1.6419 - g_loss: -0.0768\n",
      "Epoch 66/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: 1.3168 - g_loss: -0.0865\n",
      "Epoch 67/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 1.4506 - g_loss: -0.0860\n",
      "Epoch 68/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 1.3535 - g_loss: -0.0764\n",
      "Epoch 69/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 1.2904 - g_loss: -0.0835\n",
      "Epoch 70/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 1.0278 - g_loss: -0.0904\n",
      "Epoch 71/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.8078 - g_loss: -0.0789\n",
      "Epoch 72/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.7689 - g_loss: -0.0833\n",
      "Epoch 73/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.7922 - g_loss: -0.0925\n",
      "Epoch 74/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.7027 - g_loss: -0.0903\n",
      "Epoch 75/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.6162 - g_loss: -0.0811\n",
      "Epoch 76/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.7608 - g_loss: -0.0871\n",
      "Epoch 77/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.6618 - g_loss: -0.0833\n",
      "Epoch 78/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.6855 - g_loss: -0.0875\n",
      "Epoch 79/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.4664 - g_loss: -0.0824\n",
      "Epoch 80/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.5521 - g_loss: -0.0887\n",
      "Epoch 81/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.3849 - g_loss: -0.0908\n",
      "Epoch 82/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.2497 - g_loss: -0.0810\n",
      "Epoch 83/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.2222 - g_loss: -0.0966\n",
      "Epoch 84/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.0318 - g_loss: -0.0930\n",
      "Epoch 85/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.1640 - g_loss: -0.0925\n",
      "Epoch 86/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: 0.1771 - g_loss: -0.0928\n",
      "Epoch 87/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: 0.0105 - g_loss: -0.0886\n",
      "Epoch 88/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: 0.0125 - g_loss: -0.0860\n",
      "Epoch 89/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -0.0080 - g_loss: -0.0935\n",
      "Epoch 90/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.1377 - g_loss: -0.0894\n",
      "Epoch 91/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.2347 - g_loss: -0.0945\n",
      "Epoch 92/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.1489 - g_loss: -0.0955\n",
      "Epoch 93/1200\n",
      "4/4 [==============================] - 0s 9ms/step - d_loss: -0.0498 - g_loss: -0.0963\n",
      "Epoch 94/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4752 - g_loss: -0.0971\n",
      "Epoch 95/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4600 - g_loss: -0.0830\n",
      "Epoch 96/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.1460 - g_loss: -0.0900\n",
      "Epoch 97/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.1197 - g_loss: -0.0957\n",
      "Epoch 98/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.0987 - g_loss: -0.1000\n",
      "Epoch 99/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4066 - g_loss: -0.0957\n",
      "Epoch 100/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6411 - g_loss: -0.0874\n",
      "Epoch 101/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3860 - g_loss: -0.0915\n",
      "Epoch 102/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4310 - g_loss: -0.0971\n",
      "Epoch 103/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5557 - g_loss: -0.0979\n",
      "Epoch 104/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8239 - g_loss: -0.0884\n",
      "Epoch 105/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6998 - g_loss: -0.0941\n",
      "Epoch 106/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7976 - g_loss: -0.1077\n",
      "Epoch 107/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.7574 - g_loss: -0.0961\n",
      "Epoch 108/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -0.6361 - g_loss: -0.0857\n",
      "Epoch 109/1200\n",
      "4/4 [==============================] - 0s 20ms/step - d_loss: -0.5968 - g_loss: -0.0892\n",
      "Epoch 110/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.5482 - g_loss: -0.0956\n",
      "Epoch 111/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.6593 - g_loss: -0.1000\n",
      "Epoch 112/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -0.6542 - g_loss: -0.0920\n",
      "Epoch 113/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -0.8044 - g_loss: -0.0862\n",
      "Epoch 114/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7720 - g_loss: -0.1001\n",
      "Epoch 115/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7642 - g_loss: -0.0843\n",
      "Epoch 116/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5565 - g_loss: -0.0938\n",
      "Epoch 117/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -0.6701 - g_loss: -0.0948\n",
      "Epoch 118/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -0.7903 - g_loss: -0.0937\n",
      "Epoch 119/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8483 - g_loss: -0.0964\n",
      "Epoch 120/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6950 - g_loss: -0.0988\n",
      "Epoch 121/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8019 - g_loss: -0.0935\n",
      "Epoch 122/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -0.8954 - g_loss: -0.0967\n",
      "Epoch 123/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8131 - g_loss: -0.1037\n",
      "Epoch 124/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1605 - g_loss: -0.0972\n",
      "Epoch 125/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1343 - g_loss: -0.1001\n",
      "Epoch 126/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.2608 - g_loss: -0.1037\n",
      "Epoch 127/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2935 - g_loss: -0.1024\n",
      "Epoch 128/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9322 - g_loss: -0.0997\n",
      "Epoch 129/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1352 - g_loss: -0.1024\n",
      "Epoch 130/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1764 - g_loss: -0.0995\n",
      "Epoch 131/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2040 - g_loss: -0.0940\n",
      "Epoch 132/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2703 - g_loss: -0.0993\n",
      "Epoch 133/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3838 - g_loss: -0.0984\n",
      "Epoch 134/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2622 - g_loss: -0.1065\n",
      "Epoch 135/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0154 - g_loss: -0.1017\n",
      "Epoch 136/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0753 - g_loss: -0.1099\n",
      "Epoch 137/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8815 - g_loss: -0.0972\n",
      "Epoch 138/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4725 - g_loss: -0.1024\n",
      "Epoch 139/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2450 - g_loss: -0.1135\n",
      "Epoch 140/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9361 - g_loss: -0.1073\n",
      "Epoch 141/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4961 - g_loss: -0.1097\n",
      "Epoch 142/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6615 - g_loss: -0.1038\n",
      "Epoch 143/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -1.2648 - g_loss: -0.1127\n",
      "Epoch 144/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3686 - g_loss: -0.1063\n",
      "Epoch 145/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.3644 - g_loss: -0.1137\n",
      "Epoch 146/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.3844 - g_loss: -0.1133\n",
      "Epoch 147/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3697 - g_loss: -0.1067\n",
      "Epoch 148/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.2380 - g_loss: -0.0923\n",
      "Epoch 149/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4417 - g_loss: -0.1166\n",
      "Epoch 150/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2321 - g_loss: -0.1174\n",
      "Epoch 151/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -0.9510 - g_loss: -0.1111\n",
      "Epoch 152/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4321 - g_loss: -0.1100\n",
      "Epoch 153/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1255 - g_loss: -0.1126\n",
      "Epoch 154/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3379 - g_loss: -0.1046\n",
      "Epoch 155/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6659 - g_loss: -0.1102\n",
      "Epoch 156/1200\n",
      "4/4 [==============================] - 0s 22ms/step - d_loss: -1.2678 - g_loss: -0.1189\n",
      "Epoch 157/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.4366 - g_loss: -0.1028\n",
      "Epoch 158/1200\n",
      "4/4 [==============================] - 0s 20ms/step - d_loss: -1.6593 - g_loss: -0.1166\n",
      "Epoch 159/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: -1.5309 - g_loss: -0.1179\n",
      "Epoch 160/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.5444 - g_loss: -0.1152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1200\n",
      "4/4 [==============================] - 0s 19ms/step - d_loss: -1.4087 - g_loss: -0.1184\n",
      "Epoch 162/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.5616 - g_loss: -0.1131\n",
      "Epoch 163/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.3967 - g_loss: -0.1145\n",
      "Epoch 164/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.3756 - g_loss: -0.1225\n",
      "Epoch 165/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.4498 - g_loss: -0.1234\n",
      "Epoch 166/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.5798 - g_loss: -0.1221\n",
      "Epoch 167/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.7636 - g_loss: -0.1153\n",
      "Epoch 168/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.5992 - g_loss: -0.1199\n",
      "Epoch 169/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -1.7739 - g_loss: -0.1138\n",
      "Epoch 170/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4583 - g_loss: -0.1157\n",
      "Epoch 171/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.0921 - g_loss: -0.1243\n",
      "Epoch 172/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3050 - g_loss: -0.1274\n",
      "Epoch 173/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2227 - g_loss: -0.1123\n",
      "Epoch 174/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -1.7545 - g_loss: -0.1137\n",
      "Epoch 175/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2651 - g_loss: -0.1181\n",
      "Epoch 176/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3105 - g_loss: -0.1290\n",
      "Epoch 177/1200\n",
      "4/4 [==============================] - 0s 23ms/step - d_loss: -1.9560 - g_loss: -0.1224\n",
      "Epoch 178/1200\n",
      "4/4 [==============================] - 0s 19ms/step - d_loss: -2.0398 - g_loss: -0.1272\n",
      "Epoch 179/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: -1.3487 - g_loss: -0.1187\n",
      "Epoch 180/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.4174 - g_loss: -0.1207\n",
      "Epoch 181/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.8399 - g_loss: -0.1304\n",
      "Epoch 182/1200\n",
      "4/4 [==============================] - 0s 20ms/step - d_loss: -1.5236 - g_loss: -0.1310\n",
      "Epoch 183/1200\n",
      "4/4 [==============================] - 0s 22ms/step - d_loss: -1.3607 - g_loss: -0.1257\n",
      "Epoch 184/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.2221 - g_loss: -0.1310\n",
      "Epoch 185/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.6716 - g_loss: -0.1288\n",
      "Epoch 186/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5885 - g_loss: -0.1296\n",
      "Epoch 187/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2881 - g_loss: -0.1319\n",
      "Epoch 188/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8206 - g_loss: -0.1296\n",
      "Epoch 189/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.0011 - g_loss: -0.1177\n",
      "Epoch 190/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2094 - g_loss: -0.1252\n",
      "Epoch 191/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8874 - g_loss: -0.1360\n",
      "Epoch 192/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6112 - g_loss: -0.1364\n",
      "Epoch 193/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6155 - g_loss: -0.1466\n",
      "Epoch 194/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4142 - g_loss: -0.1241\n",
      "Epoch 195/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6452 - g_loss: -0.1343\n",
      "Epoch 196/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7257 - g_loss: -0.1345\n",
      "Epoch 197/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.8578 - g_loss: -0.1345\n",
      "Epoch 198/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6657 - g_loss: -0.1420\n",
      "Epoch 199/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8854 - g_loss: -0.1342\n",
      "Epoch 200/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7198 - g_loss: -0.1501\n",
      "Epoch 201/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6009 - g_loss: -0.1570\n",
      "Epoch 202/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7144 - g_loss: -0.1443\n",
      "Epoch 203/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7515 - g_loss: -0.1464\n",
      "Epoch 204/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8036 - g_loss: -0.1444\n",
      "Epoch 205/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4358 - g_loss: -0.1497\n",
      "Epoch 206/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.4325 - g_loss: -0.1653\n",
      "Epoch 207/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5392 - g_loss: -0.1508\n",
      "Epoch 208/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7568 - g_loss: -0.1527\n",
      "Epoch 209/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6061 - g_loss: -0.1578\n",
      "Epoch 210/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0241 - g_loss: -0.1540\n",
      "Epoch 211/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8082 - g_loss: -0.1621\n",
      "Epoch 212/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9388 - g_loss: -0.1653\n",
      "Epoch 213/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8610 - g_loss: -0.1723\n",
      "Epoch 214/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.3631 - g_loss: -0.1672\n",
      "Epoch 215/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.5717 - g_loss: -0.1591\n",
      "Epoch 216/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5639 - g_loss: -0.1604\n",
      "Epoch 217/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5045 - g_loss: -0.1691\n",
      "Epoch 218/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7841 - g_loss: -0.1713\n",
      "Epoch 219/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7237 - g_loss: -0.1781\n",
      "Epoch 220/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6838 - g_loss: -0.1930\n",
      "Epoch 221/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7154 - g_loss: -0.1740\n",
      "Epoch 222/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9583 - g_loss: -0.1765\n",
      "Epoch 223/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5983 - g_loss: -0.1719\n",
      "Epoch 224/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7953 - g_loss: -0.1860\n",
      "Epoch 225/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6164 - g_loss: -0.1788\n",
      "Epoch 226/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7669 - g_loss: -0.1741\n",
      "Epoch 227/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7237 - g_loss: -0.1882\n",
      "Epoch 228/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6139 - g_loss: -0.1975\n",
      "Epoch 229/1200\n",
      "4/4 [==============================] - 0s 9ms/step - d_loss: -1.9314 - g_loss: -0.1885\n",
      "Epoch 230/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9270 - g_loss: -0.1910\n",
      "Epoch 231/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9821 - g_loss: -0.1959\n",
      "Epoch 232/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9098 - g_loss: -0.2070\n",
      "Epoch 233/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9439 - g_loss: -0.1813\n",
      "Epoch 234/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6225 - g_loss: -0.1880\n",
      "Epoch 235/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7090 - g_loss: -0.2041\n",
      "Epoch 236/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9541 - g_loss: -0.1988\n",
      "Epoch 237/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0983 - g_loss: -0.1991\n",
      "Epoch 238/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.7659 - g_loss: -0.2104\n",
      "Epoch 239/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5942 - g_loss: -0.2163\n",
      "Epoch 240/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8767 - g_loss: -0.2092\n",
      "Epoch 241/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5913 - g_loss: -0.2187\n",
      "Epoch 242/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9793 - g_loss: -0.1967\n",
      "Epoch 243/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5826 - g_loss: -0.1978\n",
      "Epoch 244/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8411 - g_loss: -0.2026\n",
      "Epoch 245/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9063 - g_loss: -0.2190\n",
      "Epoch 246/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.7081 - g_loss: -0.2114\n",
      "Epoch 247/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5077 - g_loss: -0.2342\n",
      "Epoch 248/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6306 - g_loss: -0.2100\n",
      "Epoch 249/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5855 - g_loss: -0.2150\n",
      "Epoch 250/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.8449 - g_loss: -0.2033\n",
      "Epoch 251/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6464 - g_loss: -0.2182\n",
      "Epoch 252/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5860 - g_loss: -0.2205\n",
      "Epoch 253/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6317 - g_loss: -0.2008\n",
      "Epoch 254/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5488 - g_loss: -0.2194\n",
      "Epoch 255/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7760 - g_loss: -0.2237\n",
      "Epoch 256/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.4403 - g_loss: -0.2150\n",
      "Epoch 257/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3622 - g_loss: -0.2147\n",
      "Epoch 258/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6992 - g_loss: -0.2318\n",
      "Epoch 259/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9033 - g_loss: -0.2231\n",
      "Epoch 260/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6909 - g_loss: -0.2100\n",
      "Epoch 261/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9613 - g_loss: -0.2206\n",
      "Epoch 262/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6772 - g_loss: -0.2282\n",
      "Epoch 263/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.4872 - g_loss: -0.2229\n",
      "Epoch 264/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.1760 - g_loss: -0.2481\n",
      "Epoch 265/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.0846 - g_loss: -0.2253\n",
      "Epoch 266/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.0720 - g_loss: -0.2392\n",
      "Epoch 267/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9380 - g_loss: -0.2440\n",
      "Epoch 268/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.1500 - g_loss: -0.2385\n",
      "Epoch 269/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.8741 - g_loss: -0.2436\n",
      "Epoch 270/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1272 - g_loss: -0.2365\n",
      "Epoch 271/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.9916 - g_loss: -0.2502\n",
      "Epoch 272/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8542 - g_loss: -0.2617\n",
      "Epoch 273/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1244 - g_loss: -0.2555\n",
      "Epoch 274/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8551 - g_loss: -0.2642\n",
      "Epoch 275/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8446 - g_loss: -0.2572\n",
      "Epoch 276/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0328 - g_loss: -0.2473\n",
      "Epoch 277/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6358 - g_loss: -0.2551\n",
      "Epoch 278/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.8500 - g_loss: -0.2717\n",
      "Epoch 279/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.7834 - g_loss: -0.2666\n",
      "Epoch 280/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.0560 - g_loss: -0.2796\n",
      "Epoch 281/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0087 - g_loss: -0.2820\n",
      "Epoch 282/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7284 - g_loss: -0.2860\n",
      "Epoch 283/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0063 - g_loss: -0.2668\n",
      "Epoch 284/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7805 - g_loss: -0.2734\n",
      "Epoch 285/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: -2.1075 - g_loss: -0.2725\n",
      "Epoch 286/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8541 - g_loss: -0.2479\n",
      "Epoch 287/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0194 - g_loss: -0.2630\n",
      "Epoch 288/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7117 - g_loss: -0.2662\n",
      "Epoch 289/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7163 - g_loss: -0.2875\n",
      "Epoch 290/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9916 - g_loss: -0.2720\n",
      "Epoch 291/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6728 - g_loss: -0.2710\n",
      "Epoch 292/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7723 - g_loss: -0.2886\n",
      "Epoch 293/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6897 - g_loss: -0.2780\n",
      "Epoch 294/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7677 - g_loss: -0.2882\n",
      "Epoch 295/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8376 - g_loss: -0.3118\n",
      "Epoch 296/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9733 - g_loss: -0.3057\n",
      "Epoch 297/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1210 - g_loss: -0.3020\n",
      "Epoch 298/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2395 - g_loss: -0.2899\n",
      "Epoch 299/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2695 - g_loss: -0.3044\n",
      "Epoch 300/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7623 - g_loss: -0.3050\n",
      "Epoch 301/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7725 - g_loss: -0.3264\n",
      "Epoch 302/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7019 - g_loss: -0.2983\n",
      "Epoch 303/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9077 - g_loss: -0.2862\n",
      "Epoch 304/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9848 - g_loss: -0.3060\n",
      "Epoch 305/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5794 - g_loss: -0.2939\n",
      "Epoch 306/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6886 - g_loss: -0.3191\n",
      "Epoch 307/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0220 - g_loss: -0.3171\n",
      "Epoch 308/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8920 - g_loss: -0.3075\n",
      "Epoch 309/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9811 - g_loss: -0.3115\n",
      "Epoch 310/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1409 - g_loss: -0.3140\n",
      "Epoch 311/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.3653 - g_loss: -0.3115\n",
      "Epoch 312/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0488 - g_loss: -0.3129\n",
      "Epoch 313/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0363 - g_loss: -0.3222\n",
      "Epoch 314/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6918 - g_loss: -0.3232\n",
      "Epoch 315/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8346 - g_loss: -0.3332\n",
      "Epoch 316/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7871 - g_loss: -0.3384\n",
      "Epoch 317/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9521 - g_loss: -0.3183\n",
      "Epoch 318/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9992 - g_loss: -0.3485\n",
      "Epoch 319/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7994 - g_loss: -0.3225\n",
      "Epoch 320/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8934 - g_loss: -0.3428\n",
      "Epoch 321/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7662 - g_loss: -0.3312\n",
      "Epoch 322/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1578 - g_loss: -0.3417\n",
      "Epoch 323/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2651 - g_loss: -0.3467\n",
      "Epoch 324/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0721 - g_loss: -0.3596\n",
      "Epoch 325/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6266 - g_loss: -0.3603\n",
      "Epoch 326/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6755 - g_loss: -0.3580\n",
      "Epoch 327/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0275 - g_loss: -0.3448\n",
      "Epoch 328/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7067 - g_loss: -0.3642\n",
      "Epoch 329/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9753 - g_loss: -0.3564\n",
      "Epoch 330/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9628 - g_loss: -0.3462\n",
      "Epoch 331/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2839 - g_loss: -0.3492\n",
      "Epoch 332/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9115 - g_loss: -0.3773\n",
      "Epoch 333/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4996 - g_loss: -0.3725\n",
      "Epoch 334/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7352 - g_loss: -0.3370\n",
      "Epoch 335/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1930 - g_loss: -0.3838\n",
      "Epoch 336/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8202 - g_loss: -0.3870\n",
      "Epoch 337/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9376 - g_loss: -0.3601\n",
      "Epoch 338/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9837 - g_loss: -0.3822\n",
      "Epoch 339/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0119 - g_loss: -0.3906\n",
      "Epoch 340/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7518 - g_loss: -0.3610\n",
      "Epoch 341/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6146 - g_loss: -0.3780\n",
      "Epoch 342/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9063 - g_loss: -0.3889\n",
      "Epoch 343/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7245 - g_loss: -0.3995\n",
      "Epoch 344/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8111 - g_loss: -0.3544\n",
      "Epoch 345/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7149 - g_loss: -0.3938\n",
      "Epoch 346/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8224 - g_loss: -0.3806\n",
      "Epoch 347/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1500 - g_loss: -0.3764\n",
      "Epoch 348/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8026 - g_loss: -0.4285\n",
      "Epoch 349/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0147 - g_loss: -0.4142\n",
      "Epoch 350/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2213 - g_loss: -0.4025\n",
      "Epoch 351/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0546 - g_loss: -0.3789\n",
      "Epoch 352/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -2.0500 - g_loss: -0.3729\n",
      "Epoch 353/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5914 - g_loss: -0.4332\n",
      "Epoch 354/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5581 - g_loss: -0.3945\n",
      "Epoch 355/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9884 - g_loss: -0.4019\n",
      "Epoch 356/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8360 - g_loss: -0.4181\n",
      "Epoch 357/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7813 - g_loss: -0.3925\n",
      "Epoch 358/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8505 - g_loss: -0.3985\n",
      "Epoch 359/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7541 - g_loss: -0.4066\n",
      "Epoch 360/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9564 - g_loss: -0.3914\n",
      "Epoch 361/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.8595 - g_loss: -0.4046\n",
      "Epoch 362/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -2.0906 - g_loss: -0.4149\n",
      "Epoch 363/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -2.0083 - g_loss: -0.4253\n",
      "Epoch 364/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.6447 - g_loss: -0.4192\n",
      "Epoch 365/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.9714 - g_loss: -0.3951\n",
      "Epoch 366/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.8944 - g_loss: -0.4087\n",
      "Epoch 367/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.9616 - g_loss: -0.4235\n",
      "Epoch 368/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.9575 - g_loss: -0.4530\n",
      "Epoch 369/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2058 - g_loss: -0.4080\n",
      "Epoch 370/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9494 - g_loss: -0.4406\n",
      "Epoch 371/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0711 - g_loss: -0.4365\n",
      "Epoch 372/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0123 - g_loss: -0.4404\n",
      "Epoch 373/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9885 - g_loss: -0.4514\n",
      "Epoch 374/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9467 - g_loss: -0.4291\n",
      "Epoch 375/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8713 - g_loss: -0.4609\n",
      "Epoch 376/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0994 - g_loss: -0.4160\n",
      "Epoch 377/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0497 - g_loss: -0.4251\n",
      "Epoch 378/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9054 - g_loss: -0.4186\n",
      "Epoch 379/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9839 - g_loss: -0.3983\n",
      "Epoch 380/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9201 - g_loss: -0.4608\n",
      "Epoch 381/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9841 - g_loss: -0.4258\n",
      "Epoch 382/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8198 - g_loss: -0.4432\n",
      "Epoch 383/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1771 - g_loss: -0.4355\n",
      "Epoch 384/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8994 - g_loss: -0.4520\n",
      "Epoch 385/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8604 - g_loss: -0.4784\n",
      "Epoch 386/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0311 - g_loss: -0.4316\n",
      "Epoch 387/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7225 - g_loss: -0.4669\n",
      "Epoch 388/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.8771 - g_loss: -0.4449\n",
      "Epoch 389/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.9569 - g_loss: -0.4618\n",
      "Epoch 390/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5660 - g_loss: -0.4673\n",
      "Epoch 391/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0807 - g_loss: -0.4528\n",
      "Epoch 392/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6832 - g_loss: -0.4724\n",
      "Epoch 393/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7565 - g_loss: -0.4163\n",
      "Epoch 394/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1117 - g_loss: -0.4589\n",
      "Epoch 395/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0349 - g_loss: -0.4201\n",
      "Epoch 396/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9965 - g_loss: -0.4758\n",
      "Epoch 397/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4636 - g_loss: -0.4536\n",
      "Epoch 398/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1539 - g_loss: -0.4671\n",
      "Epoch 399/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.2676 - g_loss: -0.4655\n",
      "Epoch 400/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8843 - g_loss: -0.4679\n",
      "Epoch 401/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7075 - g_loss: -0.4734\n",
      "Epoch 402/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0200 - g_loss: -0.4632\n",
      "Epoch 403/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0888 - g_loss: -0.4875\n",
      "Epoch 404/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7818 - g_loss: -0.5036\n",
      "Epoch 405/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7668 - g_loss: -0.5023\n",
      "Epoch 406/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1715 - g_loss: -0.4526\n",
      "Epoch 407/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9132 - g_loss: -0.4886\n",
      "Epoch 408/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9725 - g_loss: -0.4895\n",
      "Epoch 409/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8363 - g_loss: -0.5199\n",
      "Epoch 410/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9902 - g_loss: -0.4954\n",
      "Epoch 411/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8240 - g_loss: -0.4787\n",
      "Epoch 412/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8983 - g_loss: -0.5022\n",
      "Epoch 413/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9770 - g_loss: -0.5023\n",
      "Epoch 414/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9887 - g_loss: -0.4891\n",
      "Epoch 415/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9517 - g_loss: -0.4826\n",
      "Epoch 416/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8778 - g_loss: -0.5001\n",
      "Epoch 417/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7119 - g_loss: -0.5218\n",
      "Epoch 418/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0680 - g_loss: -0.4859\n",
      "Epoch 419/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9092 - g_loss: -0.5286\n",
      "Epoch 420/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1039 - g_loss: -0.5039\n",
      "Epoch 421/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8401 - g_loss: -0.5218\n",
      "Epoch 422/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7257 - g_loss: -0.5152\n",
      "Epoch 423/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1531 - g_loss: -0.5107\n",
      "Epoch 424/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0393 - g_loss: -0.5415\n",
      "Epoch 425/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5679 - g_loss: -0.5478\n",
      "Epoch 426/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5310 - g_loss: -0.5043\n",
      "Epoch 427/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9511 - g_loss: -0.5543\n",
      "Epoch 428/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9167 - g_loss: -0.5502\n",
      "Epoch 429/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8493 - g_loss: -0.5618\n",
      "Epoch 430/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8628 - g_loss: -0.5608\n",
      "Epoch 431/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0614 - g_loss: -0.5851\n",
      "Epoch 432/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0901 - g_loss: -0.5865\n",
      "Epoch 433/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8752 - g_loss: -0.5781\n",
      "Epoch 434/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0204 - g_loss: -0.5753\n",
      "Epoch 435/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9757 - g_loss: -0.5911\n",
      "Epoch 436/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8757 - g_loss: -0.5531\n",
      "Epoch 437/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8444 - g_loss: -0.5742\n",
      "Epoch 438/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7176 - g_loss: -0.6125\n",
      "Epoch 439/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7566 - g_loss: -0.5583\n",
      "Epoch 440/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7655 - g_loss: -0.5938\n",
      "Epoch 441/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7409 - g_loss: -0.5493\n",
      "Epoch 442/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8635 - g_loss: -0.5669\n",
      "Epoch 443/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6378 - g_loss: -0.5815\n",
      "Epoch 444/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.4940 - g_loss: -0.6075\n",
      "Epoch 445/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5713 - g_loss: -0.5985\n",
      "Epoch 446/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7348 - g_loss: -0.6059\n",
      "Epoch 447/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8407 - g_loss: -0.6209\n",
      "Epoch 448/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0651 - g_loss: -0.5964\n",
      "Epoch 449/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0659 - g_loss: -0.6029\n",
      "Epoch 450/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.3469 - g_loss: -0.6097\n",
      "Epoch 451/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0707 - g_loss: -0.6576\n",
      "Epoch 452/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9545 - g_loss: -0.6445\n",
      "Epoch 453/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7468 - g_loss: -0.6168\n",
      "Epoch 454/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9109 - g_loss: -0.6166\n",
      "Epoch 455/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9530 - g_loss: -0.6522\n",
      "Epoch 456/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -2.0076 - g_loss: -0.6118\n",
      "Epoch 457/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9897 - g_loss: -0.6240\n",
      "Epoch 458/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5571 - g_loss: -0.6675\n",
      "Epoch 459/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0923 - g_loss: -0.6355\n",
      "Epoch 460/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6133 - g_loss: -0.6709\n",
      "Epoch 461/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6955 - g_loss: -0.6258\n",
      "Epoch 462/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9582 - g_loss: -0.6315\n",
      "Epoch 463/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9867 - g_loss: -0.6841\n",
      "Epoch 464/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7750 - g_loss: -0.6487\n",
      "Epoch 465/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7283 - g_loss: -0.6940\n",
      "Epoch 466/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.9864 - g_loss: -0.6621\n",
      "Epoch 467/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6082 - g_loss: -0.6500\n",
      "Epoch 468/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8209 - g_loss: -0.6626\n",
      "Epoch 469/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1121 - g_loss: -0.6398\n",
      "Epoch 470/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6061 - g_loss: -0.6588\n",
      "Epoch 471/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0738 - g_loss: -0.6633\n",
      "Epoch 472/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9438 - g_loss: -0.7043\n",
      "Epoch 473/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6233 - g_loss: -0.6596\n",
      "Epoch 474/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7285 - g_loss: -0.6256\n",
      "Epoch 475/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8096 - g_loss: -0.6407\n",
      "Epoch 476/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7589 - g_loss: -0.6355\n",
      "Epoch 477/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5939 - g_loss: -0.7089\n",
      "Epoch 478/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6502 - g_loss: -0.6667\n",
      "Epoch 479/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1932 - g_loss: -0.6508\n",
      "Epoch 480/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8943 - g_loss: -0.7046\n",
      "Epoch 481/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3854 - g_loss: -0.6597\n",
      "Epoch 482/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7428 - g_loss: -0.6807\n",
      "Epoch 483/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9357 - g_loss: -0.6730\n",
      "Epoch 484/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8131 - g_loss: -0.6718\n",
      "Epoch 485/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8738 - g_loss: -0.6269\n",
      "Epoch 486/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5671 - g_loss: -0.6887\n",
      "Epoch 487/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8918 - g_loss: -0.6597\n",
      "Epoch 488/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8554 - g_loss: -0.6722\n",
      "Epoch 489/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6125 - g_loss: -0.6882\n",
      "Epoch 490/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5372 - g_loss: -0.6629\n",
      "Epoch 491/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5769 - g_loss: -0.6636\n",
      "Epoch 492/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2466 - g_loss: -0.6774\n",
      "Epoch 493/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7246 - g_loss: -0.6616\n",
      "Epoch 494/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6251 - g_loss: -0.6768\n",
      "Epoch 495/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7155 - g_loss: -0.6884\n",
      "Epoch 496/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5483 - g_loss: -0.6642\n",
      "Epoch 497/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7739 - g_loss: -0.6698\n",
      "Epoch 498/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2330 - g_loss: -0.6630\n",
      "Epoch 499/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4859 - g_loss: -0.6503\n",
      "Epoch 500/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7834 - g_loss: -0.6183\n",
      "Epoch 501/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5608 - g_loss: -0.6672\n",
      "Epoch 502/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8914 - g_loss: -0.6990\n",
      "Epoch 503/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6203 - g_loss: -0.7116\n",
      "Epoch 504/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9908 - g_loss: -0.6815\n",
      "Epoch 505/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8638 - g_loss: -0.6342\n",
      "Epoch 506/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5791 - g_loss: -0.6733\n",
      "Epoch 507/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8681 - g_loss: -0.6733\n",
      "Epoch 508/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.1641 - g_loss: -0.6859\n",
      "Epoch 509/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9992 - g_loss: -0.6931\n",
      "Epoch 510/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7192 - g_loss: -0.7021\n",
      "Epoch 511/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -1.6501 - g_loss: -0.7124\n",
      "Epoch 512/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8662 - g_loss: -0.7091\n",
      "Epoch 513/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8819 - g_loss: -0.7052\n",
      "Epoch 514/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6455 - g_loss: -0.7418\n",
      "Epoch 515/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3003 - g_loss: -0.7019\n",
      "Epoch 516/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6059 - g_loss: -0.7249\n",
      "Epoch 517/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6347 - g_loss: -0.7339\n",
      "Epoch 518/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9237 - g_loss: -0.7120\n",
      "Epoch 519/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8236 - g_loss: -0.6935\n",
      "Epoch 520/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6174 - g_loss: -0.7386\n",
      "Epoch 521/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9134 - g_loss: -0.7009\n",
      "Epoch 522/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5958 - g_loss: -0.7345\n",
      "Epoch 523/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5342 - g_loss: -0.7236\n",
      "Epoch 524/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6544 - g_loss: -0.7119\n",
      "Epoch 525/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3963 - g_loss: -0.7241\n",
      "Epoch 526/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7329 - g_loss: -0.7088\n",
      "Epoch 527/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.6728 - g_loss: -0.6966\n",
      "Epoch 528/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7100 - g_loss: -0.6749\n",
      "Epoch 529/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4818 - g_loss: -0.7670\n",
      "Epoch 530/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3683 - g_loss: -0.7351\n",
      "Epoch 531/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7566 - g_loss: -0.7010\n",
      "Epoch 532/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6454 - g_loss: -0.7605\n",
      "Epoch 533/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.8585 - g_loss: -0.7341\n",
      "Epoch 534/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4066 - g_loss: -0.7047\n",
      "Epoch 535/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6682 - g_loss: -0.7702\n",
      "Epoch 536/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7652 - g_loss: -0.7415\n",
      "Epoch 537/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9675 - g_loss: -0.7332\n",
      "Epoch 538/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6590 - g_loss: -0.7716\n",
      "Epoch 539/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6321 - g_loss: -0.7231\n",
      "Epoch 540/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3728 - g_loss: -0.7705\n",
      "Epoch 541/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4137 - g_loss: -0.7481\n",
      "Epoch 542/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4454 - g_loss: -0.7632\n",
      "Epoch 543/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2325 - g_loss: -0.7307\n",
      "Epoch 544/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3017 - g_loss: -0.7295\n",
      "Epoch 545/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9004 - g_loss: -0.7315\n",
      "Epoch 546/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2342 - g_loss: -0.7222\n",
      "Epoch 547/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5142 - g_loss: -0.7235\n",
      "Epoch 548/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3648 - g_loss: -0.7030\n",
      "Epoch 549/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5629 - g_loss: -0.7374\n",
      "Epoch 550/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3275 - g_loss: -0.7487\n",
      "Epoch 551/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.3232 - g_loss: -0.7207\n",
      "Epoch 552/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -1.4971 - g_loss: -0.7633\n",
      "Epoch 553/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.4120 - g_loss: -0.7292\n",
      "Epoch 554/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -1.6009 - g_loss: -0.7540\n",
      "Epoch 555/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -1.6499 - g_loss: -0.7744\n",
      "Epoch 556/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -1.5234 - g_loss: -0.7524\n",
      "Epoch 557/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.6994 - g_loss: -0.7770\n",
      "Epoch 558/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.3850 - g_loss: -0.7508\n",
      "Epoch 559/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.5585 - g_loss: -0.7475\n",
      "Epoch 560/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1045 - g_loss: -0.7446\n",
      "Epoch 561/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0751 - g_loss: -0.7642\n",
      "Epoch 562/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6127 - g_loss: -0.7214\n",
      "Epoch 563/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4842 - g_loss: -0.7633\n",
      "Epoch 564/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3135 - g_loss: -0.7347\n",
      "Epoch 565/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5348 - g_loss: -0.7414\n",
      "Epoch 566/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6233 - g_loss: -0.7332\n",
      "Epoch 567/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6427 - g_loss: -0.7236\n",
      "Epoch 568/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2081 - g_loss: -0.6894\n",
      "Epoch 569/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4222 - g_loss: -0.7268\n",
      "Epoch 570/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3319 - g_loss: -0.7223\n",
      "Epoch 571/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3144 - g_loss: -0.7011\n",
      "Epoch 572/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3378 - g_loss: -0.7015\n",
      "Epoch 573/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.2879 - g_loss: -0.7146\n",
      "Epoch 574/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4188 - g_loss: -0.6605\n",
      "Epoch 575/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7174 - g_loss: -0.7170\n",
      "Epoch 576/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7559 - g_loss: -0.6975\n",
      "Epoch 577/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5782 - g_loss: -0.7072\n",
      "Epoch 578/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4006 - g_loss: -0.7248\n",
      "Epoch 579/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8669 - g_loss: -0.7230\n",
      "Epoch 580/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2613 - g_loss: -0.7341\n",
      "Epoch 581/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2943 - g_loss: -0.7531\n",
      "Epoch 582/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5950 - g_loss: -0.7124\n",
      "Epoch 583/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6252 - g_loss: -0.7216\n",
      "Epoch 584/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6063 - g_loss: -0.7877\n",
      "Epoch 585/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1329 - g_loss: -0.7459\n",
      "Epoch 586/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3085 - g_loss: -0.7354\n",
      "Epoch 587/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5883 - g_loss: -0.6821\n",
      "Epoch 588/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3525 - g_loss: -0.7339\n",
      "Epoch 589/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6683 - g_loss: -0.7132\n",
      "Epoch 590/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4044 - g_loss: -0.7108\n",
      "Epoch 591/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3409 - g_loss: -0.7084\n",
      "Epoch 592/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8501 - g_loss: -0.7159\n",
      "Epoch 593/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8948 - g_loss: -0.7061\n",
      "Epoch 594/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4396 - g_loss: -0.6934\n",
      "Epoch 595/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4624 - g_loss: -0.7106\n",
      "Epoch 596/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3611 - g_loss: -0.7317\n",
      "Epoch 597/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6187 - g_loss: -0.6886\n",
      "Epoch 598/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6370 - g_loss: -0.7614\n",
      "Epoch 599/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5802 - g_loss: -0.7408\n",
      "Epoch 600/1200\n",
      "4/4 [==============================] - 0s 9ms/step - d_loss: -1.2362 - g_loss: -0.8043\n",
      "Epoch 601/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5610 - g_loss: -0.7265\n",
      "Epoch 602/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7991 - g_loss: -0.7564\n",
      "Epoch 603/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5550 - g_loss: -0.7243\n",
      "Epoch 604/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3306 - g_loss: -0.7103\n",
      "Epoch 605/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2899 - g_loss: -0.7249\n",
      "Epoch 606/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6870 - g_loss: -0.7589\n",
      "Epoch 607/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.8341 - g_loss: -0.7737\n",
      "Epoch 608/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0945 - g_loss: -0.7340\n",
      "Epoch 609/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5102 - g_loss: -0.7376\n",
      "Epoch 610/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4609 - g_loss: -0.7260\n",
      "Epoch 611/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2234 - g_loss: -0.7550\n",
      "Epoch 612/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4004 - g_loss: -0.7535\n",
      "Epoch 613/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3387 - g_loss: -0.7991\n",
      "Epoch 614/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0950 - g_loss: -0.7354\n",
      "Epoch 615/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.4998 - g_loss: -0.7196\n",
      "Epoch 616/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.4350 - g_loss: -0.6676\n",
      "Epoch 617/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7646 - g_loss: -0.6877\n",
      "Epoch 618/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2941 - g_loss: -0.6989\n",
      "Epoch 619/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1976 - g_loss: -0.7223\n",
      "Epoch 620/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6019 - g_loss: -0.7127\n",
      "Epoch 621/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2189 - g_loss: -0.7650\n",
      "Epoch 622/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.1809 - g_loss: -0.7228\n",
      "Epoch 623/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2049 - g_loss: -0.7338\n",
      "Epoch 624/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3441 - g_loss: -0.7628\n",
      "Epoch 625/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4719 - g_loss: -0.7107\n",
      "Epoch 626/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1691 - g_loss: -0.7036\n",
      "Epoch 627/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3879 - g_loss: -0.7071\n",
      "Epoch 628/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6449 - g_loss: -0.7679\n",
      "Epoch 629/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -1.3015 - g_loss: -0.7811\n",
      "Epoch 630/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3731 - g_loss: -0.7275\n",
      "Epoch 631/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9948 - g_loss: -0.7690\n",
      "Epoch 632/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8575 - g_loss: -0.7371\n",
      "Epoch 633/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2570 - g_loss: -0.6865\n",
      "Epoch 634/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1445 - g_loss: -0.6978\n",
      "Epoch 635/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0153 - g_loss: -0.7186\n",
      "Epoch 636/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4400 - g_loss: -0.6705\n",
      "Epoch 637/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4309 - g_loss: -0.6911\n",
      "Epoch 638/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3365 - g_loss: -0.7601\n",
      "Epoch 639/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5446 - g_loss: -0.6726\n",
      "Epoch 640/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4197 - g_loss: -0.6613\n",
      "Epoch 641/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -2.0695 - g_loss: -0.6935\n",
      "Epoch 642/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4467 - g_loss: -0.7038\n",
      "Epoch 643/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5445 - g_loss: -0.7250\n",
      "Epoch 644/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0981 - g_loss: -0.7009\n",
      "Epoch 645/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.4782 - g_loss: -0.6884\n",
      "Epoch 646/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1173 - g_loss: -0.6876\n",
      "Epoch 647/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2641 - g_loss: -0.6879\n",
      "Epoch 648/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3312 - g_loss: -0.7005\n",
      "Epoch 649/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0008 - g_loss: -0.7026\n",
      "Epoch 650/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2327 - g_loss: -0.7292\n",
      "Epoch 651/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1880 - g_loss: -0.7119\n",
      "Epoch 652/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9634 - g_loss: -0.6708\n",
      "Epoch 653/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5502 - g_loss: -0.6732\n",
      "Epoch 654/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6177 - g_loss: -0.6810\n",
      "Epoch 655/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6760 - g_loss: -0.6600\n",
      "Epoch 656/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2330 - g_loss: -0.6457\n",
      "Epoch 657/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.4970 - g_loss: -0.7010\n",
      "Epoch 658/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8152 - g_loss: -0.6916\n",
      "Epoch 659/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.1212 - g_loss: -0.6651\n",
      "Epoch 660/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3184 - g_loss: -0.6932\n",
      "Epoch 661/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2056 - g_loss: -0.6523\n",
      "Epoch 662/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0163 - g_loss: -0.6991\n",
      "Epoch 663/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2044 - g_loss: -0.7030\n",
      "Epoch 664/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0623 - g_loss: -0.6567\n",
      "Epoch 665/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9618 - g_loss: -0.6633\n",
      "Epoch 666/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3252 - g_loss: -0.6606\n",
      "Epoch 667/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1874 - g_loss: -0.6300\n",
      "Epoch 668/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4675 - g_loss: -0.6209\n",
      "Epoch 669/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1568 - g_loss: -0.6421\n",
      "Epoch 670/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4840 - g_loss: -0.6360\n",
      "Epoch 671/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0573 - g_loss: -0.6554\n",
      "Epoch 672/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8977 - g_loss: -0.6455\n",
      "Epoch 673/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.7152 - g_loss: -0.6251\n",
      "Epoch 674/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2047 - g_loss: -0.6350\n",
      "Epoch 675/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3318 - g_loss: -0.6887\n",
      "Epoch 676/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0960 - g_loss: -0.6505\n",
      "Epoch 677/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.9213 - g_loss: -0.6633\n",
      "Epoch 678/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0269 - g_loss: -0.5954\n",
      "Epoch 679/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1031 - g_loss: -0.6704\n",
      "Epoch 680/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0820 - g_loss: -0.6382\n",
      "Epoch 681/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2438 - g_loss: -0.6306\n",
      "Epoch 682/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -0.7500 - g_loss: -0.6621\n",
      "Epoch 683/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.1772 - g_loss: -0.6371\n",
      "Epoch 684/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4703 - g_loss: -0.6515\n",
      "Epoch 685/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5672 - g_loss: -0.6485\n",
      "Epoch 686/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2553 - g_loss: -0.6380\n",
      "Epoch 687/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1829 - g_loss: -0.6152\n",
      "Epoch 688/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5043 - g_loss: -0.6457\n",
      "Epoch 689/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3716 - g_loss: -0.6322\n",
      "Epoch 690/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1522 - g_loss: -0.5898\n",
      "Epoch 691/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1882 - g_loss: -0.6122\n",
      "Epoch 692/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9813 - g_loss: -0.6074\n",
      "Epoch 693/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9884 - g_loss: -0.6457\n",
      "Epoch 694/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.1509 - g_loss: -0.6106\n",
      "Epoch 695/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0764 - g_loss: -0.6453\n",
      "Epoch 696/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1346 - g_loss: -0.6203\n",
      "Epoch 697/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2085 - g_loss: -0.6614\n",
      "Epoch 698/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: -1.0641 - g_loss: -0.6382\n",
      "Epoch 699/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.1805 - g_loss: -0.6098\n",
      "Epoch 700/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.0770 - g_loss: -0.6194\n",
      "Epoch 701/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.1014 - g_loss: -0.6248\n",
      "Epoch 702/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -0.9643 - g_loss: -0.5828\n",
      "Epoch 703/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.0866 - g_loss: -0.5952\n",
      "Epoch 704/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.1954 - g_loss: -0.6055\n",
      "Epoch 705/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.1565 - g_loss: -0.6248\n",
      "Epoch 706/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -0.9434 - g_loss: -0.5880\n",
      "Epoch 707/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.1569 - g_loss: -0.5820\n",
      "Epoch 708/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.2485 - g_loss: -0.6041\n",
      "Epoch 709/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -1.0687 - g_loss: -0.5945\n",
      "Epoch 710/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.4712 - g_loss: -0.5727\n",
      "Epoch 711/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1660 - g_loss: -0.5829\n",
      "Epoch 712/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9735 - g_loss: -0.5929\n",
      "Epoch 713/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5515 - g_loss: -0.5930\n",
      "Epoch 714/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8839 - g_loss: -0.5455\n",
      "Epoch 715/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.3477 - g_loss: -0.6243\n",
      "Epoch 716/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9464 - g_loss: -0.5849\n",
      "Epoch 717/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1293 - g_loss: -0.6026\n",
      "Epoch 718/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3000 - g_loss: -0.5814\n",
      "Epoch 719/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4441 - g_loss: -0.5639\n",
      "Epoch 720/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3594 - g_loss: -0.6001\n",
      "Epoch 721/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9598 - g_loss: -0.6103\n",
      "Epoch 722/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9681 - g_loss: -0.5724\n",
      "Epoch 723/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8471 - g_loss: -0.5343\n",
      "Epoch 724/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3448 - g_loss: -0.5597\n",
      "Epoch 725/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1003 - g_loss: -0.5870\n",
      "Epoch 726/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0418 - g_loss: -0.5488\n",
      "Epoch 727/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5173 - g_loss: -0.5686\n",
      "Epoch 728/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1485 - g_loss: -0.5766\n",
      "Epoch 729/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1919 - g_loss: -0.5503\n",
      "Epoch 730/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1831 - g_loss: -0.5851\n",
      "Epoch 731/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8089 - g_loss: -0.5673\n",
      "Epoch 732/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9667 - g_loss: -0.5385\n",
      "Epoch 733/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -1.1006 - g_loss: -0.4934\n",
      "Epoch 734/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -0.9812 - g_loss: -0.5023\n",
      "Epoch 735/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.7056 - g_loss: -0.5580\n",
      "Epoch 736/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -0.9163 - g_loss: -0.5761\n",
      "Epoch 737/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -0.9465 - g_loss: -0.5401\n",
      "Epoch 738/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -0.8453 - g_loss: -0.5519\n",
      "Epoch 739/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -0.8900 - g_loss: -0.5230\n",
      "Epoch 740/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -1.0386 - g_loss: -0.5235\n",
      "Epoch 741/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -1.2203 - g_loss: -0.5021\n",
      "Epoch 742/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1270 - g_loss: -0.5360\n",
      "Epoch 743/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0500 - g_loss: -0.5160\n",
      "Epoch 744/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0878 - g_loss: -0.5292\n",
      "Epoch 745/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8916 - g_loss: -0.5140\n",
      "Epoch 746/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3926 - g_loss: -0.5258\n",
      "Epoch 747/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9657 - g_loss: -0.5074\n",
      "Epoch 748/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0233 - g_loss: -0.4925\n",
      "Epoch 749/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2397 - g_loss: -0.4993\n",
      "Epoch 750/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0766 - g_loss: -0.5372\n",
      "Epoch 751/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0950 - g_loss: -0.5713\n",
      "Epoch 752/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9672 - g_loss: -0.5201\n",
      "Epoch 753/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3147 - g_loss: -0.5250\n",
      "Epoch 754/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0348 - g_loss: -0.4972\n",
      "Epoch 755/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -0.8051 - g_loss: -0.4937\n",
      "Epoch 756/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7092 - g_loss: -0.5062\n",
      "Epoch 757/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4318 - g_loss: -0.4746\n",
      "Epoch 758/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1140 - g_loss: -0.5135\n",
      "Epoch 759/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3018 - g_loss: -0.4862\n",
      "Epoch 760/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1454 - g_loss: -0.4875\n",
      "Epoch 761/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0357 - g_loss: -0.4898\n",
      "Epoch 762/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9459 - g_loss: -0.4463\n",
      "Epoch 763/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0563 - g_loss: -0.4734\n",
      "Epoch 764/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8765 - g_loss: -0.4661\n",
      "Epoch 765/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2011 - g_loss: -0.4679\n",
      "Epoch 766/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0128 - g_loss: -0.4736\n",
      "Epoch 767/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0257 - g_loss: -0.4718\n",
      "Epoch 768/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9487 - g_loss: -0.4622\n",
      "Epoch 769/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8005 - g_loss: -0.4535\n",
      "Epoch 770/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1742 - g_loss: -0.4479\n",
      "Epoch 771/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7784 - g_loss: -0.4497\n",
      "Epoch 772/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6754 - g_loss: -0.4350\n",
      "Epoch 773/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8093 - g_loss: -0.4689\n",
      "Epoch 774/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0790 - g_loss: -0.4674\n",
      "Epoch 775/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9880 - g_loss: -0.4741\n",
      "Epoch 776/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2940 - g_loss: -0.4366\n",
      "Epoch 777/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7659 - g_loss: -0.4525\n",
      "Epoch 778/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8733 - g_loss: -0.4592\n",
      "Epoch 779/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3848 - g_loss: -0.4555\n",
      "Epoch 780/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8745 - g_loss: -0.4381\n",
      "Epoch 781/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8538 - g_loss: -0.4653\n",
      "Epoch 782/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7796 - g_loss: -0.4069\n",
      "Epoch 783/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9026 - g_loss: -0.4176\n",
      "Epoch 784/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9561 - g_loss: -0.3947\n",
      "Epoch 785/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7743 - g_loss: -0.4355\n",
      "Epoch 786/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9348 - g_loss: -0.4016\n",
      "Epoch 787/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8736 - g_loss: -0.3853\n",
      "Epoch 788/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0822 - g_loss: -0.4270\n",
      "Epoch 789/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9642 - g_loss: -0.4478\n",
      "Epoch 790/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9583 - g_loss: -0.4048\n",
      "Epoch 791/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1105 - g_loss: -0.4259\n",
      "Epoch 792/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1648 - g_loss: -0.4109\n",
      "Epoch 793/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3126 - g_loss: -0.4414\n",
      "Epoch 794/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7102 - g_loss: -0.4591\n",
      "Epoch 795/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9131 - g_loss: -0.4221\n",
      "Epoch 796/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8923 - g_loss: -0.4259\n",
      "Epoch 797/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8343 - g_loss: -0.4327\n",
      "Epoch 798/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7719 - g_loss: -0.4320\n",
      "Epoch 799/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5875 - g_loss: -0.4117\n",
      "Epoch 800/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8853 - g_loss: -0.3595\n",
      "Epoch 801/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9933 - g_loss: -0.3745\n",
      "Epoch 802/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8773 - g_loss: -0.4082\n",
      "Epoch 803/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8467 - g_loss: -0.3918\n",
      "Epoch 804/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8606 - g_loss: -0.4054\n",
      "Epoch 805/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2926 - g_loss: -0.4305\n",
      "Epoch 806/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2423 - g_loss: -0.3839\n",
      "Epoch 807/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2301 - g_loss: -0.4282\n",
      "Epoch 808/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0724 - g_loss: -0.4281\n",
      "Epoch 809/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9940 - g_loss: -0.4003\n",
      "Epoch 810/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8054 - g_loss: -0.3967\n",
      "Epoch 811/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7860 - g_loss: -0.3693\n",
      "Epoch 812/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6785 - g_loss: -0.3810\n",
      "Epoch 813/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0874 - g_loss: -0.3617\n",
      "Epoch 814/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9715 - g_loss: -0.4059\n",
      "Epoch 815/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2030 - g_loss: -0.3705\n",
      "Epoch 816/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0840 - g_loss: -0.3797\n",
      "Epoch 817/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2539 - g_loss: -0.3783\n",
      "Epoch 818/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1238 - g_loss: -0.3852\n",
      "Epoch 819/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9295 - g_loss: -0.3821\n",
      "Epoch 820/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2626 - g_loss: -0.3851\n",
      "Epoch 821/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2872 - g_loss: -0.3938\n",
      "Epoch 822/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.3975 - g_loss: -0.3941\n",
      "Epoch 823/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0478 - g_loss: -0.4481\n",
      "Epoch 824/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7922 - g_loss: -0.4492\n",
      "Epoch 825/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.8350 - g_loss: -0.4046\n",
      "Epoch 826/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6491 - g_loss: -0.3768\n",
      "Epoch 827/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2704 - g_loss: -0.3836\n",
      "Epoch 828/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7385 - g_loss: -0.3780\n",
      "Epoch 829/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8997 - g_loss: -0.3784\n",
      "Epoch 830/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0943 - g_loss: -0.3519\n",
      "Epoch 831/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7402 - g_loss: -0.3741\n",
      "Epoch 832/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0023 - g_loss: -0.3541\n",
      "Epoch 833/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5657 - g_loss: -0.3517\n",
      "Epoch 834/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0469 - g_loss: -0.3229\n",
      "Epoch 835/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3926 - g_loss: -0.3364\n",
      "Epoch 836/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1432 - g_loss: -0.3697\n",
      "Epoch 837/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8834 - g_loss: -0.3627\n",
      "Epoch 838/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9757 - g_loss: -0.3532\n",
      "Epoch 839/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8754 - g_loss: -0.3535\n",
      "Epoch 840/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1414 - g_loss: -0.3582\n",
      "Epoch 841/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9682 - g_loss: -0.3901\n",
      "Epoch 842/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4755 - g_loss: -0.3627\n",
      "Epoch 843/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9832 - g_loss: -0.3351\n",
      "Epoch 844/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5154 - g_loss: -0.3657\n",
      "Epoch 845/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6085 - g_loss: -0.3365\n",
      "Epoch 846/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0884 - g_loss: -0.3360\n",
      "Epoch 847/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4415 - g_loss: -0.3490\n",
      "Epoch 848/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -1.1733 - g_loss: -0.3998\n",
      "Epoch 849/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1875 - g_loss: -0.3430\n",
      "Epoch 850/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7834 - g_loss: -0.3522\n",
      "Epoch 851/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6203 - g_loss: -0.3433\n",
      "Epoch 852/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2341 - g_loss: -0.3195\n",
      "Epoch 853/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8574 - g_loss: -0.3137\n",
      "Epoch 854/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9230 - g_loss: -0.3034\n",
      "Epoch 855/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9130 - g_loss: -0.3140\n",
      "Epoch 856/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7722 - g_loss: -0.3314\n",
      "Epoch 857/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9829 - g_loss: -0.3061\n",
      "Epoch 858/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7604 - g_loss: -0.2880\n",
      "Epoch 859/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0468 - g_loss: -0.2691\n",
      "Epoch 860/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2801 - g_loss: -0.2871\n",
      "Epoch 861/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8273 - g_loss: -0.3032\n",
      "Epoch 862/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8810 - g_loss: -0.3010\n",
      "Epoch 863/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0183 - g_loss: -0.2937\n",
      "Epoch 864/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6435 - g_loss: -0.3197\n",
      "Epoch 865/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9249 - g_loss: -0.3232\n",
      "Epoch 866/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2584 - g_loss: -0.3058\n",
      "Epoch 867/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7923 - g_loss: -0.3187\n",
      "Epoch 868/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2165 - g_loss: -0.3182\n",
      "Epoch 869/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1160 - g_loss: -0.3232\n",
      "Epoch 870/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5352 - g_loss: -0.2824\n",
      "Epoch 871/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7870 - g_loss: -0.2608\n",
      "Epoch 872/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4576 - g_loss: -0.2345\n",
      "Epoch 873/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6703 - g_loss: -0.2829\n",
      "Epoch 874/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0155 - g_loss: -0.3049\n",
      "Epoch 875/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9748 - g_loss: -0.2725\n",
      "Epoch 876/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0219 - g_loss: -0.2839\n",
      "Epoch 877/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8187 - g_loss: -0.2779\n",
      "Epoch 878/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1704 - g_loss: -0.2641\n",
      "Epoch 879/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0051 - g_loss: -0.2576\n",
      "Epoch 880/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1890 - g_loss: -0.2796\n",
      "Epoch 881/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1843 - g_loss: -0.2354\n",
      "Epoch 882/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5203 - g_loss: -0.2340\n",
      "Epoch 883/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6902 - g_loss: -0.2325\n",
      "Epoch 884/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5318 - g_loss: -0.2176\n",
      "Epoch 885/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0978 - g_loss: -0.2083\n",
      "Epoch 886/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6186 - g_loss: -0.2330\n",
      "Epoch 887/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0881 - g_loss: -0.2242\n",
      "Epoch 888/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0418 - g_loss: -0.1961\n",
      "Epoch 889/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8284 - g_loss: -0.1923\n",
      "Epoch 890/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4972 - g_loss: -0.1933\n",
      "Epoch 891/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9675 - g_loss: -0.1954\n",
      "Epoch 892/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5499 - g_loss: -0.1934\n",
      "Epoch 893/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0080 - g_loss: -0.1808\n",
      "Epoch 894/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1508 - g_loss: -0.1730\n",
      "Epoch 895/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0383 - g_loss: -0.1910\n",
      "Epoch 896/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8573 - g_loss: -0.1934\n",
      "Epoch 897/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9295 - g_loss: -0.1865\n",
      "Epoch 898/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9065 - g_loss: -0.1766\n",
      "Epoch 899/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1189 - g_loss: -0.2022\n",
      "Epoch 900/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7290 - g_loss: -0.1982\n",
      "Epoch 901/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5055 - g_loss: -0.1755\n",
      "Epoch 902/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9339 - g_loss: -0.1619\n",
      "Epoch 903/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8931 - g_loss: -0.1751\n",
      "Epoch 904/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7580 - g_loss: -0.1684\n",
      "Epoch 905/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9379 - g_loss: -0.1704\n",
      "Epoch 906/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6497 - g_loss: -0.1640\n",
      "Epoch 907/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0630 - g_loss: -0.1691\n",
      "Epoch 908/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4510 - g_loss: -0.1447\n",
      "Epoch 909/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6171 - g_loss: -0.1602\n",
      "Epoch 910/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7767 - g_loss: -0.1730\n",
      "Epoch 911/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7187 - g_loss: -0.1439\n",
      "Epoch 912/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7291 - g_loss: -0.1312\n",
      "Epoch 913/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8454 - g_loss: -0.1449\n",
      "Epoch 914/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9366 - g_loss: -0.1525\n",
      "Epoch 915/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8198 - g_loss: -0.1612\n",
      "Epoch 916/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7959 - g_loss: -0.1557\n",
      "Epoch 917/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3535 - g_loss: -0.1493\n",
      "Epoch 918/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7674 - g_loss: -0.1556\n",
      "Epoch 919/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8629 - g_loss: -0.1386\n",
      "Epoch 920/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0739 - g_loss: -0.1402\n",
      "Epoch 921/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7255 - g_loss: -0.1259\n",
      "Epoch 922/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9429 - g_loss: -0.1227\n",
      "Epoch 923/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8478 - g_loss: -0.1168\n",
      "Epoch 924/1200\n",
      "4/4 [==============================] - 0s 19ms/step - d_loss: -1.1012 - g_loss: -0.1172\n",
      "Epoch 925/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -1.1912 - g_loss: -0.1085\n",
      "Epoch 926/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -1.3040 - g_loss: -0.1241\n",
      "Epoch 927/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.1319 - g_loss: -0.1078\n",
      "Epoch 928/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.1556 - g_loss: -0.1139\n",
      "Epoch 929/1200\n",
      "4/4 [==============================] - 0s 19ms/step - d_loss: -0.5389 - g_loss: -0.0981\n",
      "Epoch 930/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -0.5444 - g_loss: -0.0838\n",
      "Epoch 931/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4484 - g_loss: -0.0642\n",
      "Epoch 932/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0911 - g_loss: -0.0817\n",
      "Epoch 933/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0490 - g_loss: -0.0942\n",
      "Epoch 934/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8154 - g_loss: -0.0943\n",
      "Epoch 935/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8664 - g_loss: -0.1014\n",
      "Epoch 936/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8224 - g_loss: -0.1042\n",
      "Epoch 937/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7064 - g_loss: -0.0850\n",
      "Epoch 938/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6924 - g_loss: -0.0881\n",
      "Epoch 939/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1090 - g_loss: -0.0669\n",
      "Epoch 940/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7608 - g_loss: -0.0615\n",
      "Epoch 941/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5724 - g_loss: -0.0473\n",
      "Epoch 942/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4863 - g_loss: -0.0686\n",
      "Epoch 943/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3972 - g_loss: -0.0615\n",
      "Epoch 944/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0511 - g_loss: -0.0710\n",
      "Epoch 945/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.9046 - g_loss: -0.0786\n",
      "Epoch 946/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4733 - g_loss: -0.0660\n",
      "Epoch 947/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0878 - g_loss: -0.0560\n",
      "Epoch 948/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6564 - g_loss: -0.0823\n",
      "Epoch 949/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9211 - g_loss: -0.0639\n",
      "Epoch 950/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2540 - g_loss: -0.0888\n",
      "Epoch 951/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8425 - g_loss: -0.0923\n",
      "Epoch 952/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6102 - g_loss: -0.0694\n",
      "Epoch 953/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8453 - g_loss: -0.0815\n",
      "Epoch 954/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7037 - g_loss: -0.0776\n",
      "Epoch 955/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0256 - g_loss: -0.0636\n",
      "Epoch 956/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8230 - g_loss: -0.0663\n",
      "Epoch 957/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8004 - g_loss: -0.0959\n",
      "Epoch 958/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8902 - g_loss: -0.0695\n",
      "Epoch 959/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7926 - g_loss: -0.0620\n",
      "Epoch 960/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1427 - g_loss: -0.1024\n",
      "Epoch 961/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4152 - g_loss: -0.0902\n",
      "Epoch 962/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9057 - g_loss: -0.0850\n",
      "Epoch 963/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4503 - g_loss: -0.0820\n",
      "Epoch 964/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.6139 - g_loss: -0.0927\n",
      "Epoch 965/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9008 - g_loss: -0.0918\n",
      "Epoch 966/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.9284 - g_loss: -0.1003\n",
      "Epoch 967/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8814 - g_loss: -0.1118\n",
      "Epoch 968/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7916 - g_loss: -0.1181\n",
      "Epoch 969/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3623 - g_loss: -0.1134\n",
      "Epoch 970/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9820 - g_loss: -0.0962\n",
      "Epoch 971/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5877 - g_loss: -0.1373\n",
      "Epoch 972/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0102 - g_loss: -0.1002\n",
      "Epoch 973/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7814 - g_loss: -0.0938\n",
      "Epoch 974/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8039 - g_loss: -0.0975\n",
      "Epoch 975/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0364 - g_loss: -0.0769\n",
      "Epoch 976/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4393 - g_loss: -0.0642\n",
      "Epoch 977/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2126 - g_loss: -0.0708\n",
      "Epoch 978/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2735 - g_loss: -0.0496\n",
      "Epoch 979/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0451 - g_loss: -0.0636\n",
      "Epoch 980/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1388 - g_loss: -0.0522\n",
      "Epoch 981/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8165 - g_loss: -0.0618\n",
      "Epoch 982/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7335 - g_loss: -0.0921\n",
      "Epoch 983/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7840 - g_loss: -0.0780\n",
      "Epoch 984/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6780 - g_loss: -0.0862\n",
      "Epoch 985/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3266 - g_loss: -0.0484\n",
      "Epoch 986/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1078 - g_loss: -0.0640\n",
      "Epoch 987/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8893 - g_loss: -0.0848\n",
      "Epoch 988/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2075 - g_loss: -0.0971\n",
      "Epoch 989/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3047 - g_loss: -0.0875\n",
      "Epoch 990/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3243 - g_loss: -0.0882\n",
      "Epoch 991/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3681 - g_loss: -0.0705\n",
      "Epoch 992/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4973 - g_loss: -0.0740\n",
      "Epoch 993/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4530 - g_loss: -0.0521\n",
      "Epoch 994/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4296 - g_loss: -0.0589\n",
      "Epoch 995/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0208 - g_loss: -0.0731\n",
      "Epoch 996/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9667 - g_loss: -0.0621\n",
      "Epoch 997/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4000 - g_loss: -0.0775\n",
      "Epoch 998/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8328 - g_loss: -0.0474\n",
      "Epoch 999/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8980 - g_loss: -0.1106\n",
      "Epoch 1000/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3249 - g_loss: -0.0856\n",
      "Epoch 1001/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1007 - g_loss: -0.0749\n",
      "Epoch 1002/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7875 - g_loss: -0.0674\n",
      "Epoch 1003/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8887 - g_loss: -0.0749\n",
      "Epoch 1004/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0260 - g_loss: -0.1025\n",
      "Epoch 1005/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9054 - g_loss: -0.0769\n",
      "Epoch 1006/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2448 - g_loss: -0.0499\n",
      "Epoch 1007/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0414 - g_loss: -0.0861\n",
      "Epoch 1008/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6222 - g_loss: -0.0920\n",
      "Epoch 1009/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4209 - g_loss: -0.0316\n",
      "Epoch 1010/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3200 - g_loss: -0.0445\n",
      "Epoch 1011/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4494 - g_loss: -0.0623\n",
      "Epoch 1012/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8885 - g_loss: -0.0362\n",
      "Epoch 1013/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4404 - g_loss: -0.0354\n",
      "Epoch 1014/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5728 - g_loss: -0.0335\n",
      "Epoch 1015/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1962 - g_loss: -0.0619\n",
      "Epoch 1016/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8832 - g_loss: -0.0614\n",
      "Epoch 1017/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7170 - g_loss: -0.0685\n",
      "Epoch 1018/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7814 - g_loss: -0.0690\n",
      "Epoch 1019/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2517 - g_loss: -0.1013\n",
      "Epoch 1020/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -1.0256 - g_loss: -0.0914\n",
      "Epoch 1021/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8866 - g_loss: -0.0933\n",
      "Epoch 1022/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8662 - g_loss: -0.0408\n",
      "Epoch 1023/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6517 - g_loss: -0.0705\n",
      "Epoch 1024/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5981 - g_loss: -0.0727\n",
      "Epoch 1025/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7315 - g_loss: -0.0760\n",
      "Epoch 1026/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1164 - g_loss: -0.0911\n",
      "Epoch 1027/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5560 - g_loss: -0.0463\n",
      "Epoch 1028/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7990 - g_loss: -0.0396\n",
      "Epoch 1029/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6704 - g_loss: -0.0634\n",
      "Epoch 1030/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9813 - g_loss: -0.0706\n",
      "Epoch 1031/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9425 - g_loss: -0.0529\n",
      "Epoch 1032/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7222 - g_loss: -0.0390\n",
      "Epoch 1033/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7248 - g_loss: -0.0807\n",
      "Epoch 1034/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9422 - g_loss: -0.0621\n",
      "Epoch 1035/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3145 - g_loss: -0.0689\n",
      "Epoch 1036/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3899 - g_loss: -0.0977\n",
      "Epoch 1037/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8918 - g_loss: -0.1188\n",
      "Epoch 1038/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7221 - g_loss: -0.0646\n",
      "Epoch 1039/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3339 - g_loss: -0.0724\n",
      "Epoch 1040/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8664 - g_loss: -0.0661\n",
      "Epoch 1041/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8925 - g_loss: -0.0998\n",
      "Epoch 1042/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9193 - g_loss: -0.0942\n",
      "Epoch 1043/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9336 - g_loss: -0.1033\n",
      "Epoch 1044/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8614 - g_loss: -0.0895\n",
      "Epoch 1045/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4043 - g_loss: -0.0649\n",
      "Epoch 1046/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0296 - g_loss: -0.0601\n",
      "Epoch 1047/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6683 - g_loss: -0.0530\n",
      "Epoch 1048/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7668 - g_loss: -0.0555\n",
      "Epoch 1049/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6004 - g_loss: -0.0665\n",
      "Epoch 1050/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9047 - g_loss: -0.0780\n",
      "Epoch 1051/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1102 - g_loss: -0.0593\n",
      "Epoch 1052/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5676 - g_loss: -0.0925\n",
      "Epoch 1053/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3697 - g_loss: -0.0796\n",
      "Epoch 1054/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9573 - g_loss: -0.0739\n",
      "Epoch 1055/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7185 - g_loss: -0.1000\n",
      "Epoch 1056/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6643 - g_loss: -0.0730\n",
      "Epoch 1057/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7249 - g_loss: -0.0652\n",
      "Epoch 1058/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9323 - g_loss: -0.0915\n",
      "Epoch 1059/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5099 - g_loss: -0.0852\n",
      "Epoch 1060/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9261 - g_loss: -0.0845\n",
      "Epoch 1061/1200\n",
      "4/4 [==============================] - 0s 34ms/step - d_loss: -0.6963 - g_loss: -0.0923\n",
      "Epoch 1062/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: -0.7911 - g_loss: -0.1253\n",
      "Epoch 1063/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -0.9537 - g_loss: -0.1005\n",
      "Epoch 1064/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.1412 - g_loss: -0.1258\n",
      "Epoch 1065/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.3147 - g_loss: -0.1036\n",
      "Epoch 1066/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.6268 - g_loss: -0.1742\n",
      "Epoch 1067/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -1.3001 - g_loss: -0.1501\n",
      "Epoch 1068/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2142 - g_loss: -0.1453\n",
      "Epoch 1069/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4402 - g_loss: -0.1503\n",
      "Epoch 1070/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.2007 - g_loss: -0.1755\n",
      "Epoch 1071/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7984 - g_loss: -0.1412\n",
      "Epoch 1072/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.9338 - g_loss: -0.1844\n",
      "Epoch 1073/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2440 - g_loss: -0.1885\n",
      "Epoch 1074/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7383 - g_loss: -0.2458\n",
      "Epoch 1075/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4980 - g_loss: -0.1771\n",
      "Epoch 1076/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5806 - g_loss: -0.1135\n",
      "Epoch 1077/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8412 - g_loss: -0.1238\n",
      "Epoch 1078/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4047 - g_loss: -0.1275\n",
      "Epoch 1079/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.2819 - g_loss: -0.1551\n",
      "Epoch 1080/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.7227 - g_loss: -0.1624\n",
      "Epoch 1081/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6318 - g_loss: -0.1914\n",
      "Epoch 1082/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.6549 - g_loss: -0.1557\n",
      "Epoch 1083/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6238 - g_loss: -0.2318\n",
      "Epoch 1084/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9239 - g_loss: -0.1871\n",
      "Epoch 1085/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1539 - g_loss: -0.2158\n",
      "Epoch 1086/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.4587 - g_loss: -0.2398\n",
      "Epoch 1087/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9957 - g_loss: -0.1908\n",
      "Epoch 1088/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3585 - g_loss: -0.2026\n",
      "Epoch 1089/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -0.8624 - g_loss: -0.2455\n",
      "Epoch 1090/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2304 - g_loss: -0.2075\n",
      "Epoch 1091/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.7528 - g_loss: -0.2165\n",
      "Epoch 1092/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9858 - g_loss: -0.1886\n",
      "Epoch 1093/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7743 - g_loss: -0.2222\n",
      "Epoch 1094/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3550 - g_loss: -0.1960\n",
      "Epoch 1095/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9277 - g_loss: -0.1960\n",
      "Epoch 1096/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5554 - g_loss: -0.2464\n",
      "Epoch 1097/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4730 - g_loss: -0.1568\n",
      "Epoch 1098/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0631 - g_loss: -0.1475\n",
      "Epoch 1099/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4329 - g_loss: -0.1984\n",
      "Epoch 1100/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3776 - g_loss: -0.1407\n",
      "Epoch 1101/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9217 - g_loss: -0.1090\n",
      "Epoch 1102/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4118 - g_loss: -0.1453\n",
      "Epoch 1103/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4242 - g_loss: -0.1306\n",
      "Epoch 1104/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4145 - g_loss: -0.1286\n",
      "Epoch 1105/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9971 - g_loss: -0.1052\n",
      "Epoch 1106/1200\n",
      "4/4 [==============================] - 0s 19ms/step - d_loss: -1.1027 - g_loss: -0.1417\n",
      "Epoch 1107/1200\n",
      "4/4 [==============================] - 0s 20ms/step - d_loss: -1.1685 - g_loss: -0.1365\n",
      "Epoch 1108/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - d_loss: -0.8463 - g_loss: -0.1778\n",
      "Epoch 1109/1200\n",
      "4/4 [==============================] - 0s 19ms/step - d_loss: -1.5459 - g_loss: -0.1738\n",
      "Epoch 1110/1200\n",
      "4/4 [==============================] - 0s 24ms/step - d_loss: -0.9187 - g_loss: -0.1603\n",
      "Epoch 1111/1200\n",
      "4/4 [==============================] - 0s 18ms/step - d_loss: -0.9517 - g_loss: -0.1510\n",
      "Epoch 1112/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.2600 - g_loss: -0.1683\n",
      "Epoch 1113/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.1123 - g_loss: -0.1383\n",
      "Epoch 1114/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.6816 - g_loss: -0.1506\n",
      "Epoch 1115/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7617 - g_loss: -0.1753\n",
      "Epoch 1116/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9256 - g_loss: -0.1633\n",
      "Epoch 1117/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7234 - g_loss: -0.1858\n",
      "Epoch 1118/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: -0.7440 - g_loss: -0.1936\n",
      "Epoch 1119/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.6138 - g_loss: -0.1380\n",
      "Epoch 1120/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.7553 - g_loss: -0.1742\n",
      "Epoch 1121/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.3943 - g_loss: -0.1986\n",
      "Epoch 1122/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.9562 - g_loss: -0.1298\n",
      "Epoch 1123/1200\n",
      "4/4 [==============================] - 0s 20ms/step - d_loss: -0.8147 - g_loss: -0.1380\n",
      "Epoch 1124/1200\n",
      "4/4 [==============================] - 0s 17ms/step - d_loss: -0.7470 - g_loss: -0.1363\n",
      "Epoch 1125/1200\n",
      "4/4 [==============================] - 0s 14ms/step - d_loss: -0.9747 - g_loss: -0.2053\n",
      "Epoch 1126/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7940 - g_loss: -0.1562\n",
      "Epoch 1127/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5489 - g_loss: -0.1514\n",
      "Epoch 1128/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7125 - g_loss: -0.1801\n",
      "Epoch 1129/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3937 - g_loss: -0.1587\n",
      "Epoch 1130/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -0.6751 - g_loss: -0.1586\n",
      "Epoch 1131/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9705 - g_loss: -0.2211\n",
      "Epoch 1132/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8600 - g_loss: -0.1621\n",
      "Epoch 1133/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7694 - g_loss: -0.1587\n",
      "Epoch 1134/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.2799 - g_loss: -0.1239\n",
      "Epoch 1135/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6647 - g_loss: -0.1448\n",
      "Epoch 1136/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8466 - g_loss: -0.1174\n",
      "Epoch 1137/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7791 - g_loss: -0.1562\n",
      "Epoch 1138/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6953 - g_loss: -0.1396\n",
      "Epoch 1139/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.8212 - g_loss: -0.1591\n",
      "Epoch 1140/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4300 - g_loss: -0.1445\n",
      "Epoch 1141/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9503 - g_loss: -0.1587\n",
      "Epoch 1142/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5522 - g_loss: -0.1259\n",
      "Epoch 1143/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6604 - g_loss: -0.1190\n",
      "Epoch 1144/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -0.6706 - g_loss: -0.1643\n",
      "Epoch 1145/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3958 - g_loss: -0.1629\n",
      "Epoch 1146/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1092 - g_loss: -0.0983\n",
      "Epoch 1147/1200\n",
      "4/4 [==============================] - 0s 16ms/step - d_loss: -0.5639 - g_loss: -0.2422\n",
      "Epoch 1148/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5553 - g_loss: -0.1754\n",
      "Epoch 1149/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0006 - g_loss: -0.1577\n",
      "Epoch 1150/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7738 - g_loss: -0.1636\n",
      "Epoch 1151/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.3047 - g_loss: -0.1809\n",
      "Epoch 1152/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5711 - g_loss: -0.2111\n",
      "Epoch 1153/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1424 - g_loss: -0.1890\n",
      "Epoch 1154/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.5852 - g_loss: -0.1956\n",
      "Epoch 1155/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5297 - g_loss: -0.1497\n",
      "Epoch 1156/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7180 - g_loss: -0.1820\n",
      "Epoch 1157/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7740 - g_loss: -0.2025\n",
      "Epoch 1158/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5602 - g_loss: -0.1667\n",
      "Epoch 1159/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6962 - g_loss: -0.2370\n",
      "Epoch 1160/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2400 - g_loss: -0.2240\n",
      "Epoch 1161/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6336 - g_loss: -0.1834\n",
      "Epoch 1162/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -1.2023 - g_loss: -0.2525\n",
      "Epoch 1163/1200\n",
      "4/4 [==============================] - 0s 13ms/step - d_loss: -1.3965 - g_loss: -0.2329\n",
      "Epoch 1164/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.2104 - g_loss: -0.2012\n",
      "Epoch 1165/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.3479 - g_loss: -0.2734\n",
      "Epoch 1166/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.5360 - g_loss: -0.1560\n",
      "Epoch 1167/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3549 - g_loss: -0.1608\n",
      "Epoch 1168/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7411 - g_loss: -0.1885\n",
      "Epoch 1169/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4004 - g_loss: -0.1637\n",
      "Epoch 1170/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6871 - g_loss: -0.1595\n",
      "Epoch 1171/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5257 - g_loss: -0.0834\n",
      "Epoch 1172/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6193 - g_loss: -0.1908\n",
      "Epoch 1173/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6286 - g_loss: -0.1666\n",
      "Epoch 1174/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.9529 - g_loss: -0.1974\n",
      "Epoch 1175/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -1.3197 - g_loss: -0.1740\n",
      "Epoch 1176/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3780 - g_loss: -0.1820\n",
      "Epoch 1177/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7973 - g_loss: -0.1686\n",
      "Epoch 1178/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.1724 - g_loss: -0.1635\n",
      "Epoch 1179/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6770 - g_loss: -0.2197\n",
      "Epoch 1180/1200\n",
      "4/4 [==============================] - 0s 15ms/step - d_loss: -0.3739 - g_loss: -0.1781\n",
      "Epoch 1181/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.3305 - g_loss: -0.1932\n",
      "Epoch 1182/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5772 - g_loss: -0.1366\n",
      "Epoch 1183/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5192 - g_loss: -0.1472\n",
      "Epoch 1184/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5872 - g_loss: -0.2292\n",
      "Epoch 1185/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5037 - g_loss: -0.2300\n",
      "Epoch 1186/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7021 - g_loss: -0.1369\n",
      "Epoch 1187/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.5382 - g_loss: -0.1173\n",
      "Epoch 1188/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6161 - g_loss: -0.1372\n",
      "Epoch 1189/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0324 - g_loss: -0.1728\n",
      "Epoch 1190/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.5536 - g_loss: -0.1691\n",
      "Epoch 1191/1200\n",
      "4/4 [==============================] - 0s 11ms/step - d_loss: -0.6359 - g_loss: -0.1362\n",
      "Epoch 1192/1200\n",
      "4/4 [==============================] - 0s 12ms/step - d_loss: -0.7485 - g_loss: -0.1705\n",
      "Epoch 1193/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.5601 - g_loss: -0.2692\n",
      "Epoch 1194/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -1.0411 - g_loss: -0.2612\n",
      "Epoch 1195/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7962 - g_loss: -0.1906\n",
      "Epoch 1196/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.4820 - g_loss: -0.2086\n",
      "Epoch 1197/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.2551 - g_loss: -0.1436\n",
      "Epoch 1198/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7556 - g_loss: -0.1308\n",
      "Epoch 1199/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.6740 - g_loss: -0.2208\n",
      "Epoch 1200/1200\n",
      "4/4 [==============================] - 0s 10ms/step - d_loss: -0.7280 - g_loss: -0.1906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc8302ba510>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim= 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "batch_size=128\n",
    "\n",
    "# %% --------------------------------------- Set Seeds -----------------------------------------------------------------\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "weight_init = glorot_normal(seed=SEED)\n",
    "\n",
    "# %% --------------------------------------- WGAN_GP G D-----------------------------------------------------------------\n",
    "# https://github.com/keras-team/keras-io/blob/master/examples/generative/wgan_gp.py\n",
    "    \n",
    "def generator_wgan_gp():\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(noise)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(256, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # tanh is removed since we are not dealing with normalized image data\n",
    "    out = Dense(data_dim, kernel_initializer=weight_init)(x)\n",
    "    \n",
    "    model = Model(inputs=noise, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_wgan_gp():\n",
    "    data = Input(shape=data_dim)\n",
    "    x = Dense(256, kernel_initializer=weight_init)(data)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    out = Dense(1, kernel_initializer=weight_init)(x)\n",
    "    model = Model(inputs=data, outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# %% ----------------------------------- WGAN_GP ----------------------------------------------------------------------\n",
    "\n",
    "# gp_lambda = 5 # change lambda for gradient penalty\n",
    "\n",
    "class WGAN_GP(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=2,\n",
    "        gp_lambda=5,\n",
    "    ):\n",
    "        super(WGAN_GP, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_lambda = gp_lambda\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN_GP, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_data, fake_data):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated data\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # get the interplated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_data - real_data\n",
    "        interpolated = real_data + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated data.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calcuate the norm of the gradients\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        if isinstance(real_data, tuple):\n",
    "            real_data = real_data[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper.\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add gradient penalty to the discriminator loss\n",
    "        # 6. Return generator and discriminator losses as a loss dictionary.\n",
    "\n",
    "        # Train discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 1 extra steps\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_data = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_data, training=True)\n",
    "                # Get the logits for real images\n",
    "                real_logits = self.discriminator(real_data, training=True)\n",
    "\n",
    "                # Calculate discriminator loss using fake and real logits\n",
    "                d_cost = self.d_loss_fn(real_score=real_logits, fake_score=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_data, fake_data)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_lambda\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator now.\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake data using the generator\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake data\n",
    "            gen_data_logits = self.discriminator(generated_data, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_data_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(zip(gen_gradient, self.generator.trainable_variables))\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "# Optimizer for both the networks\n",
    "\n",
    "# generator_optimizer = Adam(learning_rate=0.00001)\n",
    "# discriminator_optimizer = Adam(learning_rate=0.00001)\n",
    "\n",
    "generator_optimizer = Adam(learning_rate=0.00001, beta_1=0.5, beta_2=0.9)\n",
    "discriminator_optimizer = Adam(learning_rate=0.00001, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "# Define the loss functions to be used for discrimiator\n",
    "# This should be (fake_loss - real_loss)\n",
    "# We will add the gradient penalty later to this loss function\n",
    "def discriminator_loss(real_score, fake_score):\n",
    "    real_loss = tf.reduce_mean(real_score)\n",
    "    fake_loss = tf.reduce_mean(fake_score)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions to be used for generator\n",
    "def generator_loss(fake_score):\n",
    "    return -tf.reduce_mean(fake_score)\n",
    "\n",
    "\n",
    "d_model = discriminator_wgan_gp()\n",
    "g_model = generator_wgan_gp()\n",
    "\n",
    "\n",
    "# Get the wgan model\n",
    "wgan_gp = WGAN_GP(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=latent_dim,\n",
    "    discriminator_extra_steps=2,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "wgan_gp.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Epochs to train\n",
    "epochs = 1200\n",
    "X_train_fraud = x_train_fraud.to_numpy()\n",
    "wgan_gp.fit(X_train_fraud, batch_size=128, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan_gp.generator.save('wgan_gp_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.285898</td>\n",
       "      <td>-3.736352</td>\n",
       "      <td>2.997597</td>\n",
       "      <td>-6.250955</td>\n",
       "      <td>4.386627</td>\n",
       "      <td>-2.690249</td>\n",
       "      <td>-1.363962</td>\n",
       "      <td>-4.612177</td>\n",
       "      <td>0.660612</td>\n",
       "      <td>-2.170760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161842</td>\n",
       "      <td>0.240621</td>\n",
       "      <td>0.151653</td>\n",
       "      <td>0.064407</td>\n",
       "      <td>-0.196004</td>\n",
       "      <td>-0.021371</td>\n",
       "      <td>-0.057928</td>\n",
       "      <td>0.125946</td>\n",
       "      <td>-0.078475</td>\n",
       "      <td>0.935417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.426660</td>\n",
       "      <td>1.026493</td>\n",
       "      <td>1.564539</td>\n",
       "      <td>2.113275</td>\n",
       "      <td>1.406841</td>\n",
       "      <td>1.277270</td>\n",
       "      <td>0.835166</td>\n",
       "      <td>1.663036</td>\n",
       "      <td>1.348746</td>\n",
       "      <td>1.228492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464864</td>\n",
       "      <td>0.807718</td>\n",
       "      <td>0.685435</td>\n",
       "      <td>0.585355</td>\n",
       "      <td>0.514275</td>\n",
       "      <td>0.587829</td>\n",
       "      <td>0.520005</td>\n",
       "      <td>0.846364</td>\n",
       "      <td>0.856760</td>\n",
       "      <td>0.793710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.050900</td>\n",
       "      <td>-8.100610</td>\n",
       "      <td>-0.453291</td>\n",
       "      <td>-13.860250</td>\n",
       "      <td>0.889702</td>\n",
       "      <td>-7.325989</td>\n",
       "      <td>-4.856511</td>\n",
       "      <td>-10.718233</td>\n",
       "      <td>-2.863583</td>\n",
       "      <td>-6.186068</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.509552</td>\n",
       "      <td>-2.064033</td>\n",
       "      <td>-1.781631</td>\n",
       "      <td>-1.671867</td>\n",
       "      <td>-1.546942</td>\n",
       "      <td>-1.756543</td>\n",
       "      <td>-1.656567</td>\n",
       "      <td>-2.476619</td>\n",
       "      <td>-2.983893</td>\n",
       "      <td>-1.248760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.338709</td>\n",
       "      <td>-4.390446</td>\n",
       "      <td>1.864912</td>\n",
       "      <td>-7.514736</td>\n",
       "      <td>3.371391</td>\n",
       "      <td>-3.524099</td>\n",
       "      <td>-1.929833</td>\n",
       "      <td>-5.637193</td>\n",
       "      <td>-0.302777</td>\n",
       "      <td>-2.920360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163455</td>\n",
       "      <td>-0.303049</td>\n",
       "      <td>-0.279954</td>\n",
       "      <td>-0.344180</td>\n",
       "      <td>-0.542387</td>\n",
       "      <td>-0.403878</td>\n",
       "      <td>-0.424682</td>\n",
       "      <td>-0.455132</td>\n",
       "      <td>-0.647239</td>\n",
       "      <td>0.365596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.991888</td>\n",
       "      <td>-3.658928</td>\n",
       "      <td>2.800085</td>\n",
       "      <td>-6.009765</td>\n",
       "      <td>4.278168</td>\n",
       "      <td>-2.549115</td>\n",
       "      <td>-1.293017</td>\n",
       "      <td>-4.384441</td>\n",
       "      <td>0.554528</td>\n",
       "      <td>-2.013049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161121</td>\n",
       "      <td>0.221696</td>\n",
       "      <td>0.154098</td>\n",
       "      <td>0.045523</td>\n",
       "      <td>-0.224998</td>\n",
       "      <td>-0.052052</td>\n",
       "      <td>-0.064129</td>\n",
       "      <td>0.150543</td>\n",
       "      <td>-0.042851</td>\n",
       "      <td>0.912836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>26.578759</td>\n",
       "      <td>-3.006803</td>\n",
       "      <td>3.940542</td>\n",
       "      <td>-4.746693</td>\n",
       "      <td>5.297742</td>\n",
       "      <td>-1.741142</td>\n",
       "      <td>-0.761045</td>\n",
       "      <td>-3.349066</td>\n",
       "      <td>1.551444</td>\n",
       "      <td>-1.325352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440837</td>\n",
       "      <td>0.813910</td>\n",
       "      <td>0.588534</td>\n",
       "      <td>0.428765</td>\n",
       "      <td>0.131469</td>\n",
       "      <td>0.369913</td>\n",
       "      <td>0.322693</td>\n",
       "      <td>0.712349</td>\n",
       "      <td>0.539105</td>\n",
       "      <td>1.453722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.840462</td>\n",
       "      <td>-1.164650</td>\n",
       "      <td>8.482611</td>\n",
       "      <td>-1.707185</td>\n",
       "      <td>9.282091</td>\n",
       "      <td>0.280309</td>\n",
       "      <td>0.806470</td>\n",
       "      <td>-1.021128</td>\n",
       "      <td>5.115191</td>\n",
       "      <td>0.709693</td>\n",
       "      <td>...</td>\n",
       "      <td>1.620332</td>\n",
       "      <td>2.592486</td>\n",
       "      <td>2.288929</td>\n",
       "      <td>1.883580</td>\n",
       "      <td>1.720156</td>\n",
       "      <td>1.901813</td>\n",
       "      <td>1.642066</td>\n",
       "      <td>2.772267</td>\n",
       "      <td>2.604203</td>\n",
       "      <td>3.975319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     23.285898    -3.736352     2.997597    -6.250955     4.386627   \n",
       "std       5.426660     1.026493     1.564539     2.113275     1.406841   \n",
       "min      10.050900    -8.100610    -0.453291   -13.860250     0.889702   \n",
       "25%      19.338709    -4.390446     1.864912    -7.514736     3.371391   \n",
       "50%      22.991888    -3.658928     2.800085    -6.009765     4.278168   \n",
       "75%      26.578759    -3.006803     3.940542    -4.746693     5.297742   \n",
       "max      41.840462    -1.164650     8.482611    -1.707185     9.282091   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -2.690249    -1.363962    -4.612177     0.660612    -2.170760  ...   \n",
       "std       1.277270     0.835166     1.663036     1.348746     1.228492  ...   \n",
       "min      -7.325989    -4.856511   -10.718233    -2.863583    -6.186068  ...   \n",
       "25%      -3.524099    -1.929833    -5.637193    -0.302777    -2.920360  ...   \n",
       "50%      -2.549115    -1.293017    -4.384441     0.554528    -2.013049  ...   \n",
       "75%      -1.741142    -0.761045    -3.349066     1.551444    -1.325352  ...   \n",
       "max       0.280309     0.806470    -1.021128     5.115191     0.709693  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.161842     0.240621     0.151653     0.064407    -0.196004   \n",
       "std       0.464864     0.807718     0.685435     0.585355     0.514275   \n",
       "min      -1.509552    -2.064033    -1.781631    -1.671867    -1.546942   \n",
       "25%      -0.163455    -0.303049    -0.279954    -0.344180    -0.542387   \n",
       "50%       0.161121     0.221696     0.154098     0.045523    -0.224998   \n",
       "75%       0.440837     0.813910     0.588534     0.428765     0.131469   \n",
       "max       1.620332     2.592486     2.288929     1.883580     1.720156   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.021371    -0.057928     0.125946    -0.078475     0.935417  \n",
       "std       0.587829     0.520005     0.846364     0.856760     0.793710  \n",
       "min      -1.756543    -1.656567    -2.476619    -2.983893    -1.248760  \n",
       "25%      -0.403878    -0.424682    -0.455132    -0.647239     0.365596  \n",
       "50%      -0.052052    -0.064129     0.150543    -0.042851     0.912836  \n",
       "75%       0.369913     0.322693     0.712349     0.539105     1.453722  \n",
       "max       1.901813     1.642066     2.772267     2.604203     3.975319  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 1000 more fraud\n",
    "gen_1000_wgan_gp, x_train_gen_1000_wgan_gp, y_train_gen_1000_wgan_gp = gen_data(wgan_gp.generator, 1000)\n",
    "df_gen_1000_wgan_gp = pd.DataFrame(data=gen_1000_wgan_gp, index=None, columns=x_train.columns)\n",
    "df_gen_1000_wgan_gp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995962220427653\n",
      "Precision:  0.8947368421052632\n",
      "Recall:  0.8673469387755102\n",
      "F1 score:  0.8808290155440415\n",
      "ROC AUC score:  0.9335855402937765\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_1000_wgan_gp = XGBC_model_predit(x_train_gen_1000_wgan_gp, y_train_gen_1000_wgan_gp)\n",
    "check_performance(y_test, y_pred_gen_1000_wgan_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_1000_wgan_gp, 'WGAN_GP 1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
