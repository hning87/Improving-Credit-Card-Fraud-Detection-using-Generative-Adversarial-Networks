{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/creditcardfraud/creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "# !pip install scikit-plot\n",
    "import scikitplot as skplt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>2.177883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.567026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000278</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>2.579395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000278</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>2.095169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000556</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>1.851197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  0.000000 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1  0.000000  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2  0.000278 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3  0.000278 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4  0.000556 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  2.177883      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724  0.567026      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  2.579395      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  2.095169      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153  1.851197      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the transformation, time transformed from sec to hour\n",
    "df_raw['Amount'] = np.log10( df_raw['Amount'].values + 1 )\n",
    "df_raw['Time'] = df_raw['Time'].values/3600 \n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test\n",
    "stratify target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "# Divide the training data into training (80%) and test (20%)\n",
    "df_train, df_test = train_test_split(df_raw, train_size=0.8, random_state=42, stratify=df_raw[target])\n",
    "\n",
    "# Reset the index\n",
    "df_train, df_test  = df_train.reset_index(drop=True), df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 31) (56962, 31)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    227451\n",
       "1       394\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.drop(target,axis=1)\n",
    "y_train = df_train[target]\n",
    "x_test = df_test.drop(target,axis=1)\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227845, 30) (227845,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_fraud = x_train[y_train==1]\n",
    "x_train_fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best params obtained from training base model\n",
    "# clf_xgb_os = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#           colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
    "#           gamma=0.2, gpu_id=0, importance_type='gain',\n",
    "#           interaction_constraints='', learning_rate=0.02, max_delta_step=0,\n",
    "#           max_depth=10, min_child_weight=2,\n",
    "#           monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
    "#           n_estimators=800, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
    "#           reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "#           tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model performance using Accuracry, Precision, Recall, F1 and ROC AUC score\n",
    "def check_performance(y_test, y_pred):\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1 score: ', f1_score(y_test, y_pred))\n",
    "    print('ROC AUC score: ',  roc_auc_score(y_test, y_pred))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "def plot_cm(y_test, y_pred, title):\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, y_pred,figsize=(8,8))\n",
    "    plt.title('Confusion Matrix ' + title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, multiply, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras import layers\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 49,153\n",
      "Trainable params: 49,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 51,166\n",
      "Trainable params: 51,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim= 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer = Adam(lr=0.00001)\n",
    "\n",
    "# %% --------------------------------------- Set Seeds -----------------------------------------------------------------\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "weight_init = glorot_normal(seed=SEED)\n",
    "\n",
    "# %% --------------------------------------- G D-----------------------------------------------------------------\n",
    "def generator():\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(noise)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(256, kernel_initializer=weight_init)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # tanh is removed since we are not dealing with normalized image data    \n",
    "    out = Dense(data_dim, kernel_initializer=weight_init)(x)\n",
    "    \n",
    "    model = Model(inputs=noise, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator():\n",
    "    data = Input(shape=data_dim)\n",
    "    x = Dense(256, kernel_initializer=weight_init)(data)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "#    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "#    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "#    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    out = Dense(1, activation='sigmoid', kernel_initializer=weight_init)(x)\n",
    "\n",
    "    model = Model(inputs=data, outputs=out)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_G(generator, discriminator):\n",
    "    # Freeze the discriminator when training generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "# %% ----------------------------------- GAN ----------------------------------------------------------------------\n",
    "# modified from https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self, g_model, d_model):\n",
    "        self.z = latent_dim\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.generator = g_model\n",
    "        self.discriminator = d_model\n",
    "\n",
    "        self.train_G = train_G(self.generator, self.discriminator)\n",
    "        self.loss_D, self.loss_G = [], []\n",
    "\n",
    "    def train(self, data, batch_size=128, steps_per_epoch=50):    \n",
    "\n",
    "        for epoch in range(steps_per_epoch):\n",
    "            # Select a random batch of transactions data \n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            real_data = data[idx]\n",
    "\n",
    "            # generate a batch of new data\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            fake_data= self.generator.predict(noise)\n",
    "\n",
    "            # Train D\n",
    "            loss_real = self.discriminator.train_on_batch(real_data, np.ones(batch_size))\n",
    "            loss_fake = self.discriminator.train_on_batch(fake_data, np.zeros(batch_size))\n",
    "            self.loss_D.append(0.5 * np.add(loss_fake, loss_real))\n",
    "\n",
    "            # Train G\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            loss_G = self.train_G.train_on_batch(noise, np.ones(batch_size))\n",
    "            self.loss_G.append(loss_G)\n",
    "\n",
    "            if (epoch + 1) * 10 % steps_per_epoch == 0:\n",
    "                print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f, acc: %.2f%%] [Loss_G: %f]' %\n",
    "                  (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], 100*self.loss_D[-1][1], loss_G))\n",
    "\n",
    "        return\n",
    "\n",
    "D = discriminator()\n",
    "G = generator()\n",
    "\n",
    "D.summary()\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #  1 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.963686, Loss_D_fake: 0.700344, acc: 45.31%] [Loss_G: 0.691058]\n",
      "Steps (20 / 100): [Loss_D_real: 0.738024, Loss_D_fake: 0.710239, acc: 45.70%] [Loss_G: 0.679139]\n",
      "Steps (30 / 100): [Loss_D_real: 0.498922, Loss_D_fake: 0.719145, acc: 48.05%] [Loss_G: 0.665071]\n",
      "Steps (40 / 100): [Loss_D_real: 0.365797, Loss_D_fake: 0.727036, acc: 48.83%] [Loss_G: 0.660530]\n",
      "Steps (50 / 100): [Loss_D_real: 0.262502, Loss_D_fake: 0.736440, acc: 53.52%] [Loss_G: 0.663346]\n",
      "Steps (60 / 100): [Loss_D_real: 0.189305, Loss_D_fake: 0.755222, acc: 52.34%] [Loss_G: 0.640558]\n",
      "Steps (70 / 100): [Loss_D_real: 0.132081, Loss_D_fake: 0.760665, acc: 52.34%] [Loss_G: 0.635177]\n",
      "Steps (80 / 100): [Loss_D_real: 0.151776, Loss_D_fake: 0.761376, acc: 51.95%] [Loss_G: 0.625603]\n",
      "Steps (90 / 100): [Loss_D_real: 0.146473, Loss_D_fake: 0.774640, acc: 50.78%] [Loss_G: 0.624140]\n",
      "Steps (100 / 100): [Loss_D_real: 0.149267, Loss_D_fake: 0.779554, acc: 50.00%] [Loss_G: 0.615288]\n",
      "EPOCH #  2 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.083585, Loss_D_fake: 0.793124, acc: 50.78%] [Loss_G: 0.613973]\n",
      "Steps (20 / 100): [Loss_D_real: 0.119746, Loss_D_fake: 0.797481, acc: 49.61%] [Loss_G: 0.604923]\n",
      "Steps (30 / 100): [Loss_D_real: 0.107802, Loss_D_fake: 0.800984, acc: 48.44%] [Loss_G: 0.607199]\n",
      "Steps (40 / 100): [Loss_D_real: 0.085968, Loss_D_fake: 0.804355, acc: 50.00%] [Loss_G: 0.587990]\n",
      "Steps (50 / 100): [Loss_D_real: 0.093862, Loss_D_fake: 0.814709, acc: 49.22%] [Loss_G: 0.599507]\n",
      "Steps (60 / 100): [Loss_D_real: 0.055436, Loss_D_fake: 0.815513, acc: 50.00%] [Loss_G: 0.598110]\n",
      "Steps (70 / 100): [Loss_D_real: 0.069467, Loss_D_fake: 0.816274, acc: 50.00%] [Loss_G: 0.592350]\n",
      "Steps (80 / 100): [Loss_D_real: 0.073591, Loss_D_fake: 0.806237, acc: 50.00%] [Loss_G: 0.592077]\n",
      "Steps (90 / 100): [Loss_D_real: 0.076432, Loss_D_fake: 0.808358, acc: 49.61%] [Loss_G: 0.587958]\n",
      "Steps (100 / 100): [Loss_D_real: 0.055764, Loss_D_fake: 0.810743, acc: 49.61%] [Loss_G: 0.594150]\n",
      "EPOCH #  3 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.061961, Loss_D_fake: 0.806066, acc: 48.83%] [Loss_G: 0.598424]\n",
      "Steps (20 / 100): [Loss_D_real: 0.054361, Loss_D_fake: 0.808077, acc: 50.00%] [Loss_G: 0.596331]\n",
      "Steps (30 / 100): [Loss_D_real: 0.066826, Loss_D_fake: 0.788462, acc: 49.61%] [Loss_G: 0.607993]\n",
      "Steps (40 / 100): [Loss_D_real: 0.056748, Loss_D_fake: 0.789532, acc: 48.83%] [Loss_G: 0.607716]\n",
      "Steps (50 / 100): [Loss_D_real: 0.070253, Loss_D_fake: 0.782737, acc: 50.39%] [Loss_G: 0.612540]\n",
      "Steps (60 / 100): [Loss_D_real: 0.047011, Loss_D_fake: 0.777820, acc: 50.00%] [Loss_G: 0.616397]\n",
      "Steps (70 / 100): [Loss_D_real: 0.054161, Loss_D_fake: 0.768523, acc: 50.78%] [Loss_G: 0.621201]\n",
      "Steps (80 / 100): [Loss_D_real: 0.050814, Loss_D_fake: 0.769037, acc: 50.39%] [Loss_G: 0.627629]\n",
      "Steps (90 / 100): [Loss_D_real: 0.048702, Loss_D_fake: 0.762482, acc: 50.00%] [Loss_G: 0.629971]\n",
      "Steps (100 / 100): [Loss_D_real: 0.036555, Loss_D_fake: 0.762994, acc: 50.78%] [Loss_G: 0.637547]\n",
      "EPOCH #  4 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.032506, Loss_D_fake: 0.748849, acc: 51.95%] [Loss_G: 0.637348]\n",
      "Steps (20 / 100): [Loss_D_real: 0.055991, Loss_D_fake: 0.743770, acc: 54.69%] [Loss_G: 0.643154]\n",
      "Steps (30 / 100): [Loss_D_real: 0.041698, Loss_D_fake: 0.737256, acc: 54.69%] [Loss_G: 0.650304]\n",
      "Steps (40 / 100): [Loss_D_real: 0.049204, Loss_D_fake: 0.735001, acc: 56.25%] [Loss_G: 0.652928]\n",
      "Steps (50 / 100): [Loss_D_real: 0.053124, Loss_D_fake: 0.734218, acc: 57.03%] [Loss_G: 0.652811]\n",
      "Steps (60 / 100): [Loss_D_real: 0.055606, Loss_D_fake: 0.735283, acc: 56.25%] [Loss_G: 0.662108]\n",
      "Steps (70 / 100): [Loss_D_real: 0.046075, Loss_D_fake: 0.721259, acc: 62.50%] [Loss_G: 0.662273]\n",
      "Steps (80 / 100): [Loss_D_real: 0.043207, Loss_D_fake: 0.724766, acc: 62.11%] [Loss_G: 0.668364]\n",
      "Steps (90 / 100): [Loss_D_real: 0.042996, Loss_D_fake: 0.725557, acc: 60.16%] [Loss_G: 0.669835]\n",
      "Steps (100 / 100): [Loss_D_real: 0.045392, Loss_D_fake: 0.720482, acc: 62.89%] [Loss_G: 0.672816]\n",
      "EPOCH #  5 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.060718, Loss_D_fake: 0.714371, acc: 67.19%] [Loss_G: 0.667329]\n",
      "Steps (20 / 100): [Loss_D_real: 0.038874, Loss_D_fake: 0.721327, acc: 65.23%] [Loss_G: 0.668679]\n",
      "Steps (30 / 100): [Loss_D_real: 0.047217, Loss_D_fake: 0.716702, acc: 67.58%] [Loss_G: 0.668444]\n",
      "Steps (40 / 100): [Loss_D_real: 0.036157, Loss_D_fake: 0.714917, acc: 70.31%] [Loss_G: 0.669289]\n",
      "Steps (50 / 100): [Loss_D_real: 0.053896, Loss_D_fake: 0.733949, acc: 60.94%] [Loss_G: 0.654766]\n",
      "Steps (60 / 100): [Loss_D_real: 0.056342, Loss_D_fake: 0.738583, acc: 62.11%] [Loss_G: 0.655387]\n",
      "Steps (70 / 100): [Loss_D_real: 0.042123, Loss_D_fake: 0.745470, acc: 57.03%] [Loss_G: 0.656788]\n",
      "Steps (80 / 100): [Loss_D_real: 0.057744, Loss_D_fake: 0.747524, acc: 56.64%] [Loss_G: 0.643571]\n",
      "Steps (90 / 100): [Loss_D_real: 0.056046, Loss_D_fake: 0.745931, acc: 57.42%] [Loss_G: 0.642207]\n",
      "Steps (100 / 100): [Loss_D_real: 0.051613, Loss_D_fake: 0.757433, acc: 57.81%] [Loss_G: 0.637219]\n",
      "EPOCH #  6 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.078171, Loss_D_fake: 0.763199, acc: 56.64%] [Loss_G: 0.637501]\n",
      "Steps (20 / 100): [Loss_D_real: 0.066877, Loss_D_fake: 0.763160, acc: 55.86%] [Loss_G: 0.630699]\n",
      "Steps (30 / 100): [Loss_D_real: 0.052392, Loss_D_fake: 0.763055, acc: 55.47%] [Loss_G: 0.628962]\n",
      "Steps (40 / 100): [Loss_D_real: 0.069162, Loss_D_fake: 0.769995, acc: 52.73%] [Loss_G: 0.622135]\n",
      "Steps (50 / 100): [Loss_D_real: 0.055657, Loss_D_fake: 0.767561, acc: 54.69%] [Loss_G: 0.622130]\n",
      "Steps (60 / 100): [Loss_D_real: 0.073861, Loss_D_fake: 0.775988, acc: 54.69%] [Loss_G: 0.628190]\n",
      "Steps (70 / 100): [Loss_D_real: 0.077196, Loss_D_fake: 0.761915, acc: 55.86%] [Loss_G: 0.625959]\n",
      "Steps (80 / 100): [Loss_D_real: 0.084435, Loss_D_fake: 0.763127, acc: 53.52%] [Loss_G: 0.627931]\n",
      "Steps (90 / 100): [Loss_D_real: 0.093610, Loss_D_fake: 0.771024, acc: 52.73%] [Loss_G: 0.627714]\n",
      "Steps (100 / 100): [Loss_D_real: 0.075962, Loss_D_fake: 0.771660, acc: 53.52%] [Loss_G: 0.635434]\n",
      "EPOCH #  7 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.064365, Loss_D_fake: 0.764819, acc: 53.91%] [Loss_G: 0.624966]\n",
      "Steps (20 / 100): [Loss_D_real: 0.078807, Loss_D_fake: 0.755990, acc: 57.81%] [Loss_G: 0.635049]\n",
      "Steps (30 / 100): [Loss_D_real: 0.065346, Loss_D_fake: 0.756390, acc: 57.03%] [Loss_G: 0.632210]\n",
      "Steps (40 / 100): [Loss_D_real: 0.073426, Loss_D_fake: 0.756650, acc: 56.64%] [Loss_G: 0.640113]\n",
      "Steps (50 / 100): [Loss_D_real: 0.081177, Loss_D_fake: 0.747854, acc: 56.64%] [Loss_G: 0.640915]\n",
      "Steps (60 / 100): [Loss_D_real: 0.083924, Loss_D_fake: 0.745975, acc: 58.59%] [Loss_G: 0.638166]\n",
      "Steps (70 / 100): [Loss_D_real: 0.067528, Loss_D_fake: 0.742007, acc: 59.38%] [Loss_G: 0.643185]\n",
      "Steps (80 / 100): [Loss_D_real: 0.076353, Loss_D_fake: 0.740624, acc: 58.20%] [Loss_G: 0.648463]\n",
      "Steps (90 / 100): [Loss_D_real: 0.069945, Loss_D_fake: 0.734624, acc: 60.16%] [Loss_G: 0.653146]\n",
      "Steps (100 / 100): [Loss_D_real: 0.076523, Loss_D_fake: 0.729659, acc: 60.55%] [Loss_G: 0.651999]\n",
      "EPOCH #  8 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.079597, Loss_D_fake: 0.731658, acc: 61.33%] [Loss_G: 0.657386]\n",
      "Steps (20 / 100): [Loss_D_real: 0.079890, Loss_D_fake: 0.734966, acc: 60.55%] [Loss_G: 0.659329]\n",
      "Steps (30 / 100): [Loss_D_real: 0.081023, Loss_D_fake: 0.733532, acc: 59.77%] [Loss_G: 0.660400]\n",
      "Steps (40 / 100): [Loss_D_real: 0.091859, Loss_D_fake: 0.726823, acc: 60.94%] [Loss_G: 0.661133]\n",
      "Steps (50 / 100): [Loss_D_real: 0.080947, Loss_D_fake: 0.723178, acc: 61.72%] [Loss_G: 0.663809]\n",
      "Steps (60 / 100): [Loss_D_real: 0.085240, Loss_D_fake: 0.717577, acc: 64.45%] [Loss_G: 0.663380]\n",
      "Steps (70 / 100): [Loss_D_real: 0.084100, Loss_D_fake: 0.725825, acc: 63.28%] [Loss_G: 0.669333]\n",
      "Steps (80 / 100): [Loss_D_real: 0.089135, Loss_D_fake: 0.723417, acc: 62.11%] [Loss_G: 0.671249]\n",
      "Steps (90 / 100): [Loss_D_real: 0.081278, Loss_D_fake: 0.717894, acc: 64.45%] [Loss_G: 0.666681]\n",
      "Steps (100 / 100): [Loss_D_real: 0.075132, Loss_D_fake: 0.720598, acc: 62.89%] [Loss_G: 0.673323]\n",
      "EPOCH #  9 --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (10 / 100): [Loss_D_real: 0.101657, Loss_D_fake: 0.707293, acc: 67.58%] [Loss_G: 0.678805]\n",
      "Steps (20 / 100): [Loss_D_real: 0.094967, Loss_D_fake: 0.707195, acc: 68.75%] [Loss_G: 0.681281]\n",
      "Steps (30 / 100): [Loss_D_real: 0.093957, Loss_D_fake: 0.710665, acc: 65.62%] [Loss_G: 0.684685]\n",
      "Steps (40 / 100): [Loss_D_real: 0.081320, Loss_D_fake: 0.700718, acc: 71.88%] [Loss_G: 0.687961]\n",
      "Steps (50 / 100): [Loss_D_real: 0.082580, Loss_D_fake: 0.701316, acc: 71.09%] [Loss_G: 0.688516]\n",
      "Steps (60 / 100): [Loss_D_real: 0.080174, Loss_D_fake: 0.702266, acc: 70.70%] [Loss_G: 0.694299]\n",
      "Steps (70 / 100): [Loss_D_real: 0.071277, Loss_D_fake: 0.693728, acc: 75.78%] [Loss_G: 0.693609]\n",
      "Steps (80 / 100): [Loss_D_real: 0.095912, Loss_D_fake: 0.690134, acc: 78.52%] [Loss_G: 0.692794]\n",
      "Steps (90 / 100): [Loss_D_real: 0.095385, Loss_D_fake: 0.680741, acc: 82.03%] [Loss_G: 0.702665]\n",
      "Steps (100 / 100): [Loss_D_real: 0.098336, Loss_D_fake: 0.683814, acc: 82.42%] [Loss_G: 0.705765]\n",
      "EPOCH #  10 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: 0.111382, Loss_D_fake: 0.684764, acc: 81.25%] [Loss_G: 0.712679]\n",
      "Steps (20 / 100): [Loss_D_real: 0.109255, Loss_D_fake: 0.677153, acc: 84.77%] [Loss_G: 0.713664]\n",
      "Steps (30 / 100): [Loss_D_real: 0.101266, Loss_D_fake: 0.678099, acc: 86.33%] [Loss_G: 0.716080]\n",
      "Steps (40 / 100): [Loss_D_real: 0.101724, Loss_D_fake: 0.666773, acc: 87.89%] [Loss_G: 0.719870]\n",
      "Steps (50 / 100): [Loss_D_real: 0.083318, Loss_D_fake: 0.669179, acc: 89.06%] [Loss_G: 0.717473]\n",
      "Steps (60 / 100): [Loss_D_real: 0.090148, Loss_D_fake: 0.669791, acc: 86.72%] [Loss_G: 0.718432]\n",
      "Steps (70 / 100): [Loss_D_real: 0.107258, Loss_D_fake: 0.669048, acc: 85.94%] [Loss_G: 0.723410]\n",
      "Steps (80 / 100): [Loss_D_real: 0.103611, Loss_D_fake: 0.665883, acc: 88.28%] [Loss_G: 0.721091]\n",
      "Steps (90 / 100): [Loss_D_real: 0.084980, Loss_D_fake: 0.663872, acc: 89.84%] [Loss_G: 0.727577]\n",
      "Steps (100 / 100): [Loss_D_real: 0.082441, Loss_D_fake: 0.666728, acc: 88.67%] [Loss_G: 0.716970]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(g_model=G, d_model=D)\n",
    "EPOCHS = 10\n",
    "X_train_fraud = x_train_fraud.to_numpy()\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH # ', epoch + 1, '-' * 50)\n",
    "    gan.train(X_train_fraud, batch_size=128, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.generator.save('gan_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAN ROS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(generator, n_data):\n",
    "    noise = np.random.normal(0, 1, size=(n_data, latent_dim))\n",
    "    gen = generator.predict(noise)\n",
    "    x_train_gen = np.concatenate((x_train, gen))\n",
    "    y_gen = np.array(gen.shape[0] * [1])\n",
    "    y_train_gen = np.concatenate((y_train, y_gen))\n",
    "    return gen, x_train_gen, y_train_gen    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.198706</td>\n",
       "      <td>-0.762683</td>\n",
       "      <td>0.550463</td>\n",
       "      <td>-0.873680</td>\n",
       "      <td>1.195872</td>\n",
       "      <td>-0.272598</td>\n",
       "      <td>-0.131035</td>\n",
       "      <td>-0.885020</td>\n",
       "      <td>0.172107</td>\n",
       "      <td>-0.630421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172998</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>-0.442863</td>\n",
       "      <td>-0.008667</td>\n",
       "      <td>-0.134734</td>\n",
       "      <td>-0.074124</td>\n",
       "      <td>-0.308090</td>\n",
       "      <td>-0.086204</td>\n",
       "      <td>0.298442</td>\n",
       "      <td>0.090632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.809531</td>\n",
       "      <td>0.242220</td>\n",
       "      <td>0.253717</td>\n",
       "      <td>0.331004</td>\n",
       "      <td>0.368732</td>\n",
       "      <td>0.255431</td>\n",
       "      <td>0.269883</td>\n",
       "      <td>0.360909</td>\n",
       "      <td>0.249147</td>\n",
       "      <td>0.311389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194187</td>\n",
       "      <td>0.187618</td>\n",
       "      <td>0.290937</td>\n",
       "      <td>0.266189</td>\n",
       "      <td>0.256705</td>\n",
       "      <td>0.269221</td>\n",
       "      <td>0.204048</td>\n",
       "      <td>0.238961</td>\n",
       "      <td>0.284064</td>\n",
       "      <td>0.226025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.008746</td>\n",
       "      <td>-1.553882</td>\n",
       "      <td>-0.155522</td>\n",
       "      <td>-2.179274</td>\n",
       "      <td>0.302949</td>\n",
       "      <td>-1.204299</td>\n",
       "      <td>-1.027184</td>\n",
       "      <td>-2.258254</td>\n",
       "      <td>-0.672015</td>\n",
       "      <td>-1.820887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420414</td>\n",
       "      <td>-0.507902</td>\n",
       "      <td>-1.583942</td>\n",
       "      <td>-1.109340</td>\n",
       "      <td>-0.947352</td>\n",
       "      <td>-0.928648</td>\n",
       "      <td>-1.170368</td>\n",
       "      <td>-0.948985</td>\n",
       "      <td>-0.615322</td>\n",
       "      <td>-0.521733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.607534</td>\n",
       "      <td>-0.912969</td>\n",
       "      <td>0.371841</td>\n",
       "      <td>-1.077091</td>\n",
       "      <td>0.923528</td>\n",
       "      <td>-0.423944</td>\n",
       "      <td>-0.310951</td>\n",
       "      <td>-1.102189</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>-0.832885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039617</td>\n",
       "      <td>-0.047567</td>\n",
       "      <td>-0.608654</td>\n",
       "      <td>-0.181488</td>\n",
       "      <td>-0.317593</td>\n",
       "      <td>-0.259135</td>\n",
       "      <td>-0.439663</td>\n",
       "      <td>-0.243062</td>\n",
       "      <td>0.102301</td>\n",
       "      <td>-0.066395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.141662</td>\n",
       "      <td>-0.749413</td>\n",
       "      <td>0.537844</td>\n",
       "      <td>-0.843805</td>\n",
       "      <td>1.163631</td>\n",
       "      <td>-0.256563</td>\n",
       "      <td>-0.115851</td>\n",
       "      <td>-0.863069</td>\n",
       "      <td>0.161042</td>\n",
       "      <td>-0.602991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164542</td>\n",
       "      <td>0.074092</td>\n",
       "      <td>-0.430965</td>\n",
       "      <td>-0.007723</td>\n",
       "      <td>-0.136500</td>\n",
       "      <td>-0.090567</td>\n",
       "      <td>-0.297199</td>\n",
       "      <td>-0.068053</td>\n",
       "      <td>0.297197</td>\n",
       "      <td>0.076631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.701789</td>\n",
       "      <td>-0.591090</td>\n",
       "      <td>0.721799</td>\n",
       "      <td>-0.630324</td>\n",
       "      <td>1.437741</td>\n",
       "      <td>-0.110520</td>\n",
       "      <td>0.049075</td>\n",
       "      <td>-0.620011</td>\n",
       "      <td>0.346040</td>\n",
       "      <td>-0.405448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301933</td>\n",
       "      <td>0.200008</td>\n",
       "      <td>-0.255343</td>\n",
       "      <td>0.164115</td>\n",
       "      <td>0.041329</td>\n",
       "      <td>0.104701</td>\n",
       "      <td>-0.168827</td>\n",
       "      <td>0.088158</td>\n",
       "      <td>0.498284</td>\n",
       "      <td>0.239294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.568624</td>\n",
       "      <td>-0.130545</td>\n",
       "      <td>1.558313</td>\n",
       "      <td>-0.094780</td>\n",
       "      <td>2.794879</td>\n",
       "      <td>0.407427</td>\n",
       "      <td>0.780318</td>\n",
       "      <td>-0.010885</td>\n",
       "      <td>1.309904</td>\n",
       "      <td>0.156086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951111</td>\n",
       "      <td>0.736460</td>\n",
       "      <td>0.386309</td>\n",
       "      <td>0.891412</td>\n",
       "      <td>0.678998</td>\n",
       "      <td>0.836135</td>\n",
       "      <td>0.251568</td>\n",
       "      <td>0.741321</td>\n",
       "      <td>1.327040</td>\n",
       "      <td>0.853007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      3.198706    -0.762683     0.550463    -0.873680     1.195872   \n",
       "std       0.809531     0.242220     0.253717     0.331004     0.368732   \n",
       "min       1.008746    -1.553882    -0.155522    -2.179274     0.302949   \n",
       "25%       2.607534    -0.912969     0.371841    -1.077091     0.923528   \n",
       "50%       3.141662    -0.749413     0.537844    -0.843805     1.163631   \n",
       "75%       3.701789    -0.591090     0.721799    -0.630324     1.437741   \n",
       "max       6.568624    -0.130545     1.558313    -0.094780     2.794879   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -0.272598    -0.131035    -0.885020     0.172107    -0.630421  ...   \n",
       "std       0.255431     0.269883     0.360909     0.249147     0.311389  ...   \n",
       "min      -1.204299    -1.027184    -2.258254    -0.672015    -1.820887  ...   \n",
       "25%      -0.423944    -0.310951    -1.102189     0.004328    -0.832885  ...   \n",
       "50%      -0.256563    -0.115851    -0.863069     0.161042    -0.602991  ...   \n",
       "75%      -0.110520     0.049075    -0.620011     0.346040    -0.405448  ...   \n",
       "max       0.407427     0.780318    -0.010885     1.309904     0.156086  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.172998     0.081598    -0.442863    -0.008667    -0.134734   \n",
       "std       0.194187     0.187618     0.290937     0.266189     0.256705   \n",
       "min      -0.420414    -0.507902    -1.583942    -1.109340    -0.947352   \n",
       "25%       0.039617    -0.047567    -0.608654    -0.181488    -0.317593   \n",
       "50%       0.164542     0.074092    -0.430965    -0.007723    -0.136500   \n",
       "75%       0.301933     0.200008    -0.255343     0.164115     0.041329   \n",
       "max       0.951111     0.736460     0.386309     0.891412     0.678998   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.074124    -0.308090    -0.086204     0.298442     0.090632  \n",
       "std       0.269221     0.204048     0.238961     0.284064     0.226025  \n",
       "min      -0.928648    -1.170368    -0.948985    -0.615322    -0.521733  \n",
       "25%      -0.259135    -0.439663    -0.243062     0.102301    -0.066395  \n",
       "50%      -0.090567    -0.297199    -0.068053     0.297197     0.076631  \n",
       "75%       0.104701    -0.168827     0.088158     0.498284     0.239294  \n",
       "max       0.836135     0.251568     0.741321     1.327040     0.853007  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 1000 more fraud\n",
    "gen_1000, x_train_gen_1000, y_train_gen_1000 = gen_data(gan.generator, 1000)\n",
    "df_gen_1000 = pd.DataFrame(data=gen_1000, index=None, columns=x_train.columns)\n",
    "df_gen_1000.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "      <td>227057.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.218229</td>\n",
       "      <td>-0.756737</td>\n",
       "      <td>0.545450</td>\n",
       "      <td>-0.865985</td>\n",
       "      <td>1.202481</td>\n",
       "      <td>-0.291968</td>\n",
       "      <td>-0.135377</td>\n",
       "      <td>-0.896216</td>\n",
       "      <td>0.172936</td>\n",
       "      <td>-0.649727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163267</td>\n",
       "      <td>0.070233</td>\n",
       "      <td>-0.446318</td>\n",
       "      <td>-0.006336</td>\n",
       "      <td>-0.142786</td>\n",
       "      <td>-0.066574</td>\n",
       "      <td>-0.303814</td>\n",
       "      <td>-0.095933</td>\n",
       "      <td>0.288435</td>\n",
       "      <td>0.085132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811030</td>\n",
       "      <td>0.237068</td>\n",
       "      <td>0.248480</td>\n",
       "      <td>0.324748</td>\n",
       "      <td>0.375655</td>\n",
       "      <td>0.256967</td>\n",
       "      <td>0.262603</td>\n",
       "      <td>0.354987</td>\n",
       "      <td>0.260421</td>\n",
       "      <td>0.328550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202289</td>\n",
       "      <td>0.185099</td>\n",
       "      <td>0.297259</td>\n",
       "      <td>0.268653</td>\n",
       "      <td>0.245904</td>\n",
       "      <td>0.274539</td>\n",
       "      <td>0.203102</td>\n",
       "      <td>0.238672</td>\n",
       "      <td>0.294179</td>\n",
       "      <td>0.221833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.864514</td>\n",
       "      <td>-2.054588</td>\n",
       "      <td>-0.467439</td>\n",
       "      <td>-3.083127</td>\n",
       "      <td>-0.161780</td>\n",
       "      <td>-1.679737</td>\n",
       "      <td>-1.472482</td>\n",
       "      <td>-2.770850</td>\n",
       "      <td>-0.876556</td>\n",
       "      <td>-2.878052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.744244</td>\n",
       "      <td>-0.724654</td>\n",
       "      <td>-2.054292</td>\n",
       "      <td>-1.390064</td>\n",
       "      <td>-1.226690</td>\n",
       "      <td>-1.385246</td>\n",
       "      <td>-1.398025</td>\n",
       "      <td>-1.388916</td>\n",
       "      <td>-1.065479</td>\n",
       "      <td>-0.907872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.637174</td>\n",
       "      <td>-0.907591</td>\n",
       "      <td>0.373981</td>\n",
       "      <td>-1.067487</td>\n",
       "      <td>0.936968</td>\n",
       "      <td>-0.454711</td>\n",
       "      <td>-0.305363</td>\n",
       "      <td>-1.119759</td>\n",
       "      <td>-0.006194</td>\n",
       "      <td>-0.856450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027138</td>\n",
       "      <td>-0.055728</td>\n",
       "      <td>-0.638298</td>\n",
       "      <td>-0.184396</td>\n",
       "      <td>-0.307600</td>\n",
       "      <td>-0.252043</td>\n",
       "      <td>-0.436240</td>\n",
       "      <td>-0.249986</td>\n",
       "      <td>0.092895</td>\n",
       "      <td>-0.066339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.155191</td>\n",
       "      <td>-0.741627</td>\n",
       "      <td>0.535681</td>\n",
       "      <td>-0.835274</td>\n",
       "      <td>1.177739</td>\n",
       "      <td>-0.278182</td>\n",
       "      <td>-0.129652</td>\n",
       "      <td>-0.866632</td>\n",
       "      <td>0.163114</td>\n",
       "      <td>-0.624668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156409</td>\n",
       "      <td>0.063012</td>\n",
       "      <td>-0.433781</td>\n",
       "      <td>-0.006708</td>\n",
       "      <td>-0.146187</td>\n",
       "      <td>-0.074882</td>\n",
       "      <td>-0.300090</td>\n",
       "      <td>-0.087382</td>\n",
       "      <td>0.291074</td>\n",
       "      <td>0.075964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.728535</td>\n",
       "      <td>-0.590100</td>\n",
       "      <td>0.706414</td>\n",
       "      <td>-0.633683</td>\n",
       "      <td>1.443086</td>\n",
       "      <td>-0.115086</td>\n",
       "      <td>0.040646</td>\n",
       "      <td>-0.641440</td>\n",
       "      <td>0.341009</td>\n",
       "      <td>-0.415283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292845</td>\n",
       "      <td>0.188909</td>\n",
       "      <td>-0.241708</td>\n",
       "      <td>0.172219</td>\n",
       "      <td>0.017624</td>\n",
       "      <td>0.110001</td>\n",
       "      <td>-0.167320</td>\n",
       "      <td>0.066672</td>\n",
       "      <td>0.486651</td>\n",
       "      <td>0.226646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.098598</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>1.781558</td>\n",
       "      <td>0.096638</td>\n",
       "      <td>3.173489</td>\n",
       "      <td>0.693634</td>\n",
       "      <td>1.145547</td>\n",
       "      <td>0.370712</td>\n",
       "      <td>1.762260</td>\n",
       "      <td>0.374252</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221887</td>\n",
       "      <td>1.129017</td>\n",
       "      <td>0.788170</td>\n",
       "      <td>1.249569</td>\n",
       "      <td>1.158705</td>\n",
       "      <td>1.394942</td>\n",
       "      <td>0.672848</td>\n",
       "      <td>0.942883</td>\n",
       "      <td>1.733728</td>\n",
       "      <td>1.194972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean        3.218229      -0.756737       0.545450      -0.865985   \n",
       "std         0.811030       0.237068       0.248480       0.324748   \n",
       "min         0.864514      -2.054588      -0.467439      -3.083127   \n",
       "25%         2.637174      -0.907591       0.373981      -1.067487   \n",
       "50%         3.155191      -0.741627       0.535681      -0.835274   \n",
       "75%         3.728535      -0.590100       0.706414      -0.633683   \n",
       "max         8.098598       0.093388       1.781558       0.096638   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean        1.202481      -0.291968      -0.135377      -0.896216   \n",
       "std         0.375655       0.256967       0.262603       0.354987   \n",
       "min        -0.161780      -1.679737      -1.472482      -2.770850   \n",
       "25%         0.936968      -0.454711      -0.305363      -1.119759   \n",
       "50%         1.177739      -0.278182      -0.129652      -0.866632   \n",
       "75%         1.443086      -0.115086       0.040646      -0.641440   \n",
       "max         3.173489       0.693634       1.145547       0.370712   \n",
       "\n",
       "                  V8             V9  ...            V20            V21  \\\n",
       "count  227057.000000  227057.000000  ...  227057.000000  227057.000000   \n",
       "mean        0.172936      -0.649727  ...       0.163267       0.070233   \n",
       "std         0.260421       0.328550  ...       0.202289       0.185099   \n",
       "min        -0.876556      -2.878052  ...      -0.744244      -0.724654   \n",
       "25%        -0.006194      -0.856450  ...       0.027138      -0.055728   \n",
       "50%         0.163114      -0.624668  ...       0.156409       0.063012   \n",
       "75%         0.341009      -0.415283  ...       0.292845       0.188909   \n",
       "max         1.762260       0.374252  ...       1.221887       1.129017   \n",
       "\n",
       "                 V22            V23            V24            V25  \\\n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000   \n",
       "mean       -0.446318      -0.006336      -0.142786      -0.066574   \n",
       "std         0.297259       0.268653       0.245904       0.274539   \n",
       "min        -2.054292      -1.390064      -1.226690      -1.385246   \n",
       "25%        -0.638298      -0.184396      -0.307600      -0.252043   \n",
       "50%        -0.433781      -0.006708      -0.146187      -0.074882   \n",
       "75%        -0.241708       0.172219       0.017624       0.110001   \n",
       "max         0.788170       1.249569       1.158705       1.394942   \n",
       "\n",
       "                 V26            V27            V28         Amount  \n",
       "count  227057.000000  227057.000000  227057.000000  227057.000000  \n",
       "mean       -0.303814      -0.095933       0.288435       0.085132  \n",
       "std         0.203102       0.238672       0.294179       0.221833  \n",
       "min        -1.398025      -1.388916      -1.065479      -0.907872  \n",
       "25%        -0.436240      -0.249986       0.092895      -0.066339  \n",
       "50%        -0.300090      -0.087382       0.291074       0.075964  \n",
       "75%        -0.167320       0.066672       0.486651       0.226646  \n",
       "max         0.672848       0.942883       1.733728       1.194972  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 227451 -394 = 227057  (total of x_train - fraud in x_train)\n",
    "gen_227057, x_train_gen_227057, y_train_gen_227057 = gen_data(gan.generator, 227057)\n",
    "\n",
    "df_gen_227057 = pd.DataFrame(data=gen_227057, index=None, columns=x_train.columns)\n",
    "df_gen_227057.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.008938</td>\n",
       "      <td>-4.707808</td>\n",
       "      <td>3.588729</td>\n",
       "      <td>-7.068378</td>\n",
       "      <td>4.592975</td>\n",
       "      <td>-3.101629</td>\n",
       "      <td>-1.387192</td>\n",
       "      <td>-5.539909</td>\n",
       "      <td>0.587920</td>\n",
       "      <td>-2.589654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358018</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.051318</td>\n",
       "      <td>-0.062790</td>\n",
       "      <td>-0.109108</td>\n",
       "      <td>0.019602</td>\n",
       "      <td>0.047827</td>\n",
       "      <td>0.155933</td>\n",
       "      <td>0.077212</td>\n",
       "      <td>1.228674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.347935</td>\n",
       "      <td>6.841390</td>\n",
       "      <td>4.309436</td>\n",
       "      <td>7.166449</td>\n",
       "      <td>2.883467</td>\n",
       "      <td>5.406586</td>\n",
       "      <td>1.864770</td>\n",
       "      <td>7.316745</td>\n",
       "      <td>6.676697</td>\n",
       "      <td>2.495584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384017</td>\n",
       "      <td>3.750615</td>\n",
       "      <td>1.457801</td>\n",
       "      <td>1.681228</td>\n",
       "      <td>0.509477</td>\n",
       "      <td>0.826820</td>\n",
       "      <td>0.467046</td>\n",
       "      <td>1.358987</td>\n",
       "      <td>0.555106</td>\n",
       "      <td>0.965996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.239444</td>\n",
       "      <td>-30.552380</td>\n",
       "      <td>-8.402154</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>-1.313275</td>\n",
       "      <td>-22.105532</td>\n",
       "      <td>-5.773192</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.128186</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-19.254328</td>\n",
       "      <td>-2.028024</td>\n",
       "      <td>-4.781606</td>\n",
       "      <td>-1.152671</td>\n",
       "      <td>-7.263482</td>\n",
       "      <td>-1.869290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.500278</td>\n",
       "      <td>-5.996596</td>\n",
       "      <td>1.229209</td>\n",
       "      <td>-8.436924</td>\n",
       "      <td>2.419178</td>\n",
       "      <td>-4.741036</td>\n",
       "      <td>-2.504633</td>\n",
       "      <td>-7.765017</td>\n",
       "      <td>-0.135707</td>\n",
       "      <td>-3.828323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181718</td>\n",
       "      <td>0.040122</td>\n",
       "      <td>-0.515338</td>\n",
       "      <td>-0.330293</td>\n",
       "      <td>-0.445282</td>\n",
       "      <td>-0.312004</td>\n",
       "      <td>-0.253693</td>\n",
       "      <td>-0.025894</td>\n",
       "      <td>-0.096541</td>\n",
       "      <td>0.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.393056</td>\n",
       "      <td>-2.272114</td>\n",
       "      <td>2.662472</td>\n",
       "      <td>-5.133485</td>\n",
       "      <td>4.258196</td>\n",
       "      <td>-1.522962</td>\n",
       "      <td>-1.421577</td>\n",
       "      <td>-2.926216</td>\n",
       "      <td>0.642565</td>\n",
       "      <td>-2.230097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280862</td>\n",
       "      <td>0.576441</td>\n",
       "      <td>0.073696</td>\n",
       "      <td>-0.057241</td>\n",
       "      <td>-0.060269</td>\n",
       "      <td>0.088371</td>\n",
       "      <td>-0.003464</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>0.147380</td>\n",
       "      <td>1.007318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.912917</td>\n",
       "      <td>-0.410418</td>\n",
       "      <td>4.737900</td>\n",
       "      <td>-2.302626</td>\n",
       "      <td>6.390866</td>\n",
       "      <td>0.240184</td>\n",
       "      <td>-0.361122</td>\n",
       "      <td>-0.900824</td>\n",
       "      <td>1.743587</td>\n",
       "      <td>-0.825345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783528</td>\n",
       "      <td>1.204214</td>\n",
       "      <td>0.615344</td>\n",
       "      <td>0.307132</td>\n",
       "      <td>0.274014</td>\n",
       "      <td>0.441670</td>\n",
       "      <td>0.393148</td>\n",
       "      <td>0.779267</td>\n",
       "      <td>0.372389</td>\n",
       "      <td>2.028937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.318889</td>\n",
       "      <td>2.132386</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>2.250210</td>\n",
       "      <td>12.114672</td>\n",
       "      <td>11.095089</td>\n",
       "      <td>6.474115</td>\n",
       "      <td>5.802537</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>...</td>\n",
       "      <td>11.059004</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>5.466230</td>\n",
       "      <td>0.994110</td>\n",
       "      <td>2.208209</td>\n",
       "      <td>2.745261</td>\n",
       "      <td>3.052358</td>\n",
       "      <td>1.779364</td>\n",
       "      <td>3.327741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time          V1          V2          V3          V4          V5  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean    23.008938   -4.707808    3.588729   -7.068378    4.592975   -3.101629   \n",
       "std     13.347935    6.841390    4.309436    7.166449    2.883467    5.406586   \n",
       "min      1.239444  -30.552380   -8.402154  -31.103685   -1.313275  -22.105532   \n",
       "25%     11.500278   -5.996596    1.229209   -8.436924    2.419178   -4.741036   \n",
       "50%     21.393056   -2.272114    2.662472   -5.133485    4.258196   -1.522962   \n",
       "75%     35.912917   -0.410418    4.737900   -2.302626    6.390866    0.240184   \n",
       "max     47.318889    2.132386   22.057729    2.250210   12.114672   11.095089   \n",
       "\n",
       "               V6          V7          V8          V9  ...         V20  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  ...  394.000000   \n",
       "mean    -1.387192   -5.539909    0.587920   -2.589654  ...    0.358018   \n",
       "std      1.864770    7.316745    6.676697    2.495584  ...    1.384017   \n",
       "min     -5.773192  -43.557242  -41.044261  -13.434066  ...   -4.128186   \n",
       "25%     -2.504633   -7.765017   -0.135707   -3.828323  ...   -0.181718   \n",
       "50%     -1.421577   -2.926216    0.642565   -2.230097  ...    0.280862   \n",
       "75%     -0.361122   -0.900824    1.743587   -0.825345  ...    0.783528   \n",
       "max      6.474115    5.802537   20.007208    3.353525  ...   11.059004   \n",
       "\n",
       "              V21         V22         V23         V24         V25         V26  \\\n",
       "count  394.000000  394.000000  394.000000  394.000000  394.000000  394.000000   \n",
       "mean     0.628814    0.051318   -0.062790   -0.109108    0.019602    0.047827   \n",
       "std      3.750615    1.457801    1.681228    0.509477    0.826820    0.467046   \n",
       "min    -22.797604   -8.887017  -19.254328   -2.028024   -4.781606   -1.152671   \n",
       "25%      0.040122   -0.515338   -0.330293   -0.445282   -0.312004   -0.253693   \n",
       "50%      0.576441    0.073696   -0.057241   -0.060269    0.088371   -0.003464   \n",
       "75%      1.204214    0.615344    0.307132    0.274014    0.441670    0.393148   \n",
       "max     27.202839    8.361985    5.466230    0.994110    2.208209    2.745261   \n",
       "\n",
       "              V27         V28      Amount  \n",
       "count  394.000000  394.000000  394.000000  \n",
       "mean     0.155933    0.077212    1.228674  \n",
       "std      1.358987    0.555106    0.965996  \n",
       "min     -7.263482   -1.869290    0.000000  \n",
       "25%     -0.025894   -0.096541    0.301030  \n",
       "50%      0.394926    0.147380    1.007318  \n",
       "75%      0.779267    0.372389    2.028937  \n",
       "max      3.052358    1.779364    3.327741  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with GPU\n",
    "ros = RandomOverSampler()\n",
    "def XGBC_model_predit(x, y):   \n",
    "    x, y = ros.fit_sample(x, y)\n",
    "    clf_xgb_os = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, eval_metric='auc',\n",
    "              gamma=0.2, gpu_id=0, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.02, max_delta_step=0,\n",
    "              max_depth=10, min_child_weight=2,\n",
    "              monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
    "              n_estimators=800, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
    "              tree_method='gpu_hist', validate_parameters=1, verbosity=None)\n",
    "    \n",
    "    clf_xgb_os.fit(x, y)  \n",
    "    y_pred_gen_os = clf_xgb_os.predict(x_test.to_numpy())\n",
    "    return y_pred_gen_os       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995084442259752\n",
      "Precision:  0.8645833333333334\n",
      "Recall:  0.8469387755102041\n",
      "F1 score:  0.8556701030927835\n",
      "ROC AUC score:  0.9233550799329299\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_1000 = XGBC_model_predit(x_train_gen_1000, y_train_gen_1000)\n",
    "check_performance(y_test, y_pred_gen_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_1000, 'Adding GAN 1000 Fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995435553526912\n",
      "Precision:  0.9\n",
      "Recall:  0.826530612244898\n",
      "F1 score:  0.8617021276595744\n",
      "ROC AUC score:  0.9131861699378682\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_227057 = XGBC_model_predit(x_train_gen_227057, y_train_gen_227057)\n",
    "check_performance(y_test, y_pred_gen_227057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_227057, 'Adding GAN 227057 Fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 30)]              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 49,153\n",
      "Trainable params: 49,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 51,166\n",
      "Trainable params: 51,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "latent_dim= 32\n",
    "data_dim = len(x_train.columns)\n",
    "n_classes = len(np.unique(y_train))\n",
    "optimizer_wgan = RMSprop(lr=0.00001)\n",
    "\n",
    "# %% --------------------------------------- Set Seeds -----------------------------------------------------------------\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "weight_init = glorot_normal(seed=SEED)\n",
    "\n",
    "\n",
    "# %% --------------------------------------- G D-----------------------------------------------------------------\n",
    "def generator_wgan():\n",
    "\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(noise)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(256, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # tanh is removed since we are not dealing with normalized image data\n",
    "    out = Dense(data_dim, kernel_initializer=weight_init)(x)\n",
    "    \n",
    "    model = Model(inputs=noise, outputs=out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_wgan():\n",
    "    data = Input(shape=data_dim)\n",
    "    x = Dense(256, kernel_initializer=weight_init)(data)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(128, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = Dense(64, kernel_initializer=weight_init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # remove sigmoid for D in WGAN\n",
    "    out = Dense(1, kernel_initializer=weight_init)(x)\n",
    "\n",
    "    model = Model(inputs=data, outputs=out)\n",
    "    \n",
    "    # RMSprop optimizer, w_loss\n",
    "    model.compile(optimizer=optimizer_wgan, loss=w_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "def w_loss(y, y_pred):\n",
    "    return tf.reduce_mean(tf.multiply(y, y_pred))\n",
    "\n",
    "# def w_loss(y, y_pred):\n",
    "#     return K.mean(y* y_pred)\n",
    "\n",
    "def train_G_wgan(generator, discriminator):\n",
    "    # Freeze the discriminator when training generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(optimizer=optimizer_wgan, loss=w_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "# %% ----------------------------------- WGAN ----------------------------------------------------------------------\n",
    "# modified from https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan/wgan.py\n",
    "clip_value = 0.01\n",
    "train_D = 2 # train D more than G\n",
    "\n",
    "class WGAN:\n",
    "    def __init__(self, g_model, d_model):\n",
    "        self.z = latent_dim\n",
    "        self.optimizer = optimizer_wgan\n",
    "\n",
    "        self.generator = g_model\n",
    "        self.discriminator = d_model\n",
    "\n",
    "        self.train_G = train_G_wgan(self.generator, self.discriminator)\n",
    "        self.loss_D, self.loss_G = [], []\n",
    "\n",
    "    def train(self, data, batch_size=128, steps_per_epoch=50):    \n",
    "\n",
    "        for epoch in range(steps_per_epoch):\n",
    "            # Select a random batch of transactions data \n",
    "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "            real_data = data[idx]\n",
    "\n",
    "            # generate a batch of new data\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            fake_data= self.generator.predict(noise)\n",
    "            \n",
    "            for _ in range(train_D):\n",
    "\n",
    "                # Train D                \n",
    "                loss_real = self.discriminator.train_on_batch(real_data, -np.ones(batch_size))\n",
    "                loss_fake = self.discriminator.train_on_batch(fake_data, np.ones(batch_size))\n",
    "                # loss_d = loss fake -  loss real: the wasserstein loss\n",
    "                loss_d = 0.5 * np.add(loss_real, loss_fake)\n",
    "                \n",
    "                self.loss_D.append(loss_d)\n",
    "\n",
    "\n",
    "                # clip the weight for D\n",
    "                for l in self.discriminator.layers:\n",
    "                        weights = l.get_weights()\n",
    "                        weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
    "                        l.set_weights(weights)\n",
    "\n",
    "            # Train G\n",
    "            noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "            loss_G = self.train_G.train_on_batch(noise, -np.ones(batch_size))\n",
    "            self.loss_G.append(loss_G)\n",
    "\n",
    "            if (epoch + 1) * 10 % steps_per_epoch == 0:\n",
    "                print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f] [loss_D: %f] [Loss_G: %f]' % \n",
    "                      (epoch+1, steps_per_epoch, loss_real, loss_fake, loss_d, loss_G))\n",
    "#                   (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], loss_d[0], loss_G[0]))\n",
    "\n",
    "#                 print('Steps (%d / %d): [Loss_D_real: %f, Loss_D_fake: %f, acc: %.2f%%] [Loss_G: %f]' %\n",
    "#                   (epoch+1, steps_per_epoch, loss_real[0], loss_fake[0], loss_d, loss_G[0]))\n",
    "#                 print('Steps (%d / %d): [Loss_D %f] [Loss_G: %f]' %\n",
    "#                   (epoch+1, steps_per_epoch, self.loss_D, loss_G))\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "# # %% -----------------------------------set up G D for WGAN ------------------------------------------------------------------\n",
    "D_wgan = discriminator_wgan()\n",
    "G_wgan  = generator_wgan()\n",
    "\n",
    "D_wgan.summary()\n",
    "G_wgan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #  1 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.000276, Loss_D_fake: 0.000029] [loss_D: -0.000123] [Loss_G: -0.000016]\n",
      "Steps (20 / 100): [Loss_D_real: -0.000335, Loss_D_fake: 0.000023] [loss_D: -0.000156] [Loss_G: -0.000009]\n",
      "Steps (30 / 100): [Loss_D_real: -0.000618, Loss_D_fake: 0.000009] [loss_D: -0.000305] [Loss_G: 0.000004]\n",
      "Steps (40 / 100): [Loss_D_real: -0.000969, Loss_D_fake: -0.000009] [loss_D: -0.000489] [Loss_G: 0.000023]\n",
      "Steps (50 / 100): [Loss_D_real: -0.001407, Loss_D_fake: -0.000038] [loss_D: -0.000723] [Loss_G: 0.000049]\n",
      "Steps (60 / 100): [Loss_D_real: -0.002074, Loss_D_fake: -0.000060] [loss_D: -0.001067] [Loss_G: 0.000076]\n",
      "Steps (70 / 100): [Loss_D_real: -0.002538, Loss_D_fake: -0.000089] [loss_D: -0.001313] [Loss_G: 0.000109]\n",
      "Steps (80 / 100): [Loss_D_real: -0.002984, Loss_D_fake: -0.000119] [loss_D: -0.001552] [Loss_G: 0.000139]\n",
      "Steps (90 / 100): [Loss_D_real: -0.003541, Loss_D_fake: -0.000153] [loss_D: -0.001847] [Loss_G: 0.000166]\n",
      "Steps (100 / 100): [Loss_D_real: -0.004270, Loss_D_fake: -0.000194] [loss_D: -0.002232] [Loss_G: 0.000199]\n",
      "EPOCH #  2 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.005338, Loss_D_fake: -0.000212] [loss_D: -0.002775] [Loss_G: 0.000218]\n",
      "Steps (20 / 100): [Loss_D_real: -0.006151, Loss_D_fake: -0.000237] [loss_D: -0.003194] [Loss_G: 0.000256]\n",
      "Steps (30 / 100): [Loss_D_real: -0.007367, Loss_D_fake: -0.000264] [loss_D: -0.003816] [Loss_G: 0.000284]\n",
      "Steps (40 / 100): [Loss_D_real: -0.008889, Loss_D_fake: -0.000286] [loss_D: -0.004588] [Loss_G: 0.000272]\n",
      "Steps (50 / 100): [Loss_D_real: -0.009922, Loss_D_fake: -0.000282] [loss_D: -0.005102] [Loss_G: 0.000321]\n",
      "Steps (60 / 100): [Loss_D_real: -0.012040, Loss_D_fake: -0.000258] [loss_D: -0.006149] [Loss_G: 0.000316]\n",
      "Steps (70 / 100): [Loss_D_real: -0.013406, Loss_D_fake: -0.000257] [loss_D: -0.006831] [Loss_G: 0.000279]\n",
      "Steps (80 / 100): [Loss_D_real: -0.013948, Loss_D_fake: -0.000248] [loss_D: -0.007098] [Loss_G: 0.000279]\n",
      "Steps (90 / 100): [Loss_D_real: -0.016645, Loss_D_fake: -0.000230] [loss_D: -0.008437] [Loss_G: 0.000219]\n",
      "Steps (100 / 100): [Loss_D_real: -0.019932, Loss_D_fake: -0.000109] [loss_D: -0.010021] [Loss_G: 0.000259]\n",
      "EPOCH #  3 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.020337, Loss_D_fake: -0.000093] [loss_D: -0.010215] [Loss_G: 0.000129]\n",
      "Steps (20 / 100): [Loss_D_real: -0.023879, Loss_D_fake: 0.000039] [loss_D: -0.011920] [Loss_G: 0.000066]\n",
      "Steps (30 / 100): [Loss_D_real: -0.026069, Loss_D_fake: 0.000045] [loss_D: -0.013012] [Loss_G: -0.000010]\n",
      "Steps (40 / 100): [Loss_D_real: -0.029662, Loss_D_fake: 0.000193] [loss_D: -0.014734] [Loss_G: -0.000224]\n",
      "Steps (50 / 100): [Loss_D_real: -0.029693, Loss_D_fake: 0.000312] [loss_D: -0.014691] [Loss_G: -0.000382]\n",
      "Steps (60 / 100): [Loss_D_real: -0.035835, Loss_D_fake: 0.000656] [loss_D: -0.017590] [Loss_G: -0.000643]\n",
      "Steps (70 / 100): [Loss_D_real: -0.039640, Loss_D_fake: 0.000728] [loss_D: -0.019456] [Loss_G: -0.000730]\n",
      "Steps (80 / 100): [Loss_D_real: -0.041055, Loss_D_fake: 0.000998] [loss_D: -0.020029] [Loss_G: -0.000951]\n",
      "Steps (90 / 100): [Loss_D_real: -0.045918, Loss_D_fake: 0.001252] [loss_D: -0.022333] [Loss_G: -0.001277]\n",
      "Steps (100 / 100): [Loss_D_real: -0.051906, Loss_D_fake: 0.001741] [loss_D: -0.025083] [Loss_G: -0.001510]\n",
      "EPOCH #  4 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.057075, Loss_D_fake: 0.001870] [loss_D: -0.027603] [Loss_G: -0.001873]\n",
      "Steps (20 / 100): [Loss_D_real: -0.054083, Loss_D_fake: 0.002203] [loss_D: -0.025940] [Loss_G: -0.002246]\n",
      "Steps (30 / 100): [Loss_D_real: -0.064776, Loss_D_fake: 0.002483] [loss_D: -0.031147] [Loss_G: -0.002531]\n",
      "Steps (40 / 100): [Loss_D_real: -0.067247, Loss_D_fake: 0.003058] [loss_D: -0.032094] [Loss_G: -0.002860]\n",
      "Steps (50 / 100): [Loss_D_real: -0.069548, Loss_D_fake: 0.003511] [loss_D: -0.033019] [Loss_G: -0.003557]\n",
      "Steps (60 / 100): [Loss_D_real: -0.062561, Loss_D_fake: 0.004238] [loss_D: -0.029162] [Loss_G: -0.003909]\n",
      "Steps (70 / 100): [Loss_D_real: -0.082488, Loss_D_fake: 0.004254] [loss_D: -0.039117] [Loss_G: -0.004457]\n",
      "Steps (80 / 100): [Loss_D_real: -0.087636, Loss_D_fake: 0.005003] [loss_D: -0.041317] [Loss_G: -0.005083]\n",
      "Steps (90 / 100): [Loss_D_real: -0.097291, Loss_D_fake: 0.005631] [loss_D: -0.045830] [Loss_G: -0.005439]\n",
      "Steps (100 / 100): [Loss_D_real: -0.102638, Loss_D_fake: 0.006235] [loss_D: -0.048202] [Loss_G: -0.006135]\n",
      "EPOCH #  5 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.101867, Loss_D_fake: 0.006496] [loss_D: -0.047685] [Loss_G: -0.007067]\n",
      "Steps (20 / 100): [Loss_D_real: -0.113421, Loss_D_fake: 0.007684] [loss_D: -0.052868] [Loss_G: -0.007597]\n",
      "Steps (30 / 100): [Loss_D_real: -0.119756, Loss_D_fake: 0.008309] [loss_D: -0.055723] [Loss_G: -0.008262]\n",
      "Steps (40 / 100): [Loss_D_real: -0.127219, Loss_D_fake: 0.008830] [loss_D: -0.059194] [Loss_G: -0.009143]\n",
      "Steps (50 / 100): [Loss_D_real: -0.123341, Loss_D_fake: 0.010378] [loss_D: -0.056482] [Loss_G: -0.010291]\n",
      "Steps (60 / 100): [Loss_D_real: -0.131050, Loss_D_fake: 0.011134] [loss_D: -0.059958] [Loss_G: -0.011446]\n",
      "Steps (70 / 100): [Loss_D_real: -0.152415, Loss_D_fake: 0.012751] [loss_D: -0.069832] [Loss_G: -0.011982]\n",
      "Steps (80 / 100): [Loss_D_real: -0.145079, Loss_D_fake: 0.013383] [loss_D: -0.065848] [Loss_G: -0.013088]\n",
      "Steps (90 / 100): [Loss_D_real: -0.165176, Loss_D_fake: 0.014209] [loss_D: -0.075484] [Loss_G: -0.014361]\n",
      "Steps (100 / 100): [Loss_D_real: -0.179324, Loss_D_fake: 0.015784] [loss_D: -0.081770] [Loss_G: -0.015704]\n",
      "EPOCH #  6 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.171945, Loss_D_fake: 0.017434] [loss_D: -0.077255] [Loss_G: -0.016096]\n",
      "Steps (20 / 100): [Loss_D_real: -0.191882, Loss_D_fake: 0.018118] [loss_D: -0.086882] [Loss_G: -0.017688]\n",
      "Steps (30 / 100): [Loss_D_real: -0.211475, Loss_D_fake: 0.019837] [loss_D: -0.095819] [Loss_G: -0.019467]\n",
      "Steps (40 / 100): [Loss_D_real: -0.181645, Loss_D_fake: 0.020888] [loss_D: -0.080379] [Loss_G: -0.020949]\n",
      "Steps (50 / 100): [Loss_D_real: -0.210724, Loss_D_fake: 0.022952] [loss_D: -0.093886] [Loss_G: -0.022705]\n",
      "Steps (60 / 100): [Loss_D_real: -0.226911, Loss_D_fake: 0.024379] [loss_D: -0.101266] [Loss_G: -0.023953]\n",
      "Steps (70 / 100): [Loss_D_real: -0.225953, Loss_D_fake: 0.025098] [loss_D: -0.100427] [Loss_G: -0.025524]\n",
      "Steps (80 / 100): [Loss_D_real: -0.218038, Loss_D_fake: 0.026813] [loss_D: -0.095612] [Loss_G: -0.026895]\n",
      "Steps (90 / 100): [Loss_D_real: -0.256111, Loss_D_fake: 0.029801] [loss_D: -0.113155] [Loss_G: -0.029955]\n",
      "Steps (100 / 100): [Loss_D_real: -0.245461, Loss_D_fake: 0.031495] [loss_D: -0.106983] [Loss_G: -0.030878]\n",
      "EPOCH #  7 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.266148, Loss_D_fake: 0.032802] [loss_D: -0.116673] [Loss_G: -0.033772]\n",
      "Steps (20 / 100): [Loss_D_real: -0.270568, Loss_D_fake: 0.035275] [loss_D: -0.117646] [Loss_G: -0.035745]\n",
      "Steps (30 / 100): [Loss_D_real: -0.319608, Loss_D_fake: 0.037045] [loss_D: -0.141281] [Loss_G: -0.038686]\n",
      "Steps (40 / 100): [Loss_D_real: -0.314377, Loss_D_fake: 0.040924] [loss_D: -0.136727] [Loss_G: -0.039371]\n",
      "Steps (50 / 100): [Loss_D_real: -0.317577, Loss_D_fake: 0.041285] [loss_D: -0.138146] [Loss_G: -0.041688]\n",
      "Steps (60 / 100): [Loss_D_real: -0.336953, Loss_D_fake: 0.045923] [loss_D: -0.145515] [Loss_G: -0.046945]\n",
      "Steps (70 / 100): [Loss_D_real: -0.346438, Loss_D_fake: 0.047015] [loss_D: -0.149712] [Loss_G: -0.049497]\n",
      "Steps (80 / 100): [Loss_D_real: -0.333088, Loss_D_fake: 0.050560] [loss_D: -0.141264] [Loss_G: -0.051704]\n",
      "Steps (90 / 100): [Loss_D_real: -0.358367, Loss_D_fake: 0.053312] [loss_D: -0.152527] [Loss_G: -0.052575]\n",
      "Steps (100 / 100): [Loss_D_real: -0.348376, Loss_D_fake: 0.055370] [loss_D: -0.146503] [Loss_G: -0.057571]\n",
      "EPOCH #  8 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.396233, Loss_D_fake: 0.058508] [loss_D: -0.168863] [Loss_G: -0.059225]\n",
      "Steps (20 / 100): [Loss_D_real: -0.391368, Loss_D_fake: 0.064713] [loss_D: -0.163328] [Loss_G: -0.064049]\n",
      "Steps (30 / 100): [Loss_D_real: -0.413179, Loss_D_fake: 0.067727] [loss_D: -0.172726] [Loss_G: -0.064282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (40 / 100): [Loss_D_real: -0.429590, Loss_D_fake: 0.070169] [loss_D: -0.179711] [Loss_G: -0.070635]\n",
      "Steps (50 / 100): [Loss_D_real: -0.434347, Loss_D_fake: 0.074431] [loss_D: -0.179958] [Loss_G: -0.073998]\n",
      "Steps (60 / 100): [Loss_D_real: -0.433043, Loss_D_fake: 0.075103] [loss_D: -0.178970] [Loss_G: -0.076999]\n",
      "Steps (70 / 100): [Loss_D_real: -0.473358, Loss_D_fake: 0.081093] [loss_D: -0.196132] [Loss_G: -0.081815]\n",
      "Steps (80 / 100): [Loss_D_real: -0.472709, Loss_D_fake: 0.087090] [loss_D: -0.192810] [Loss_G: -0.086016]\n",
      "Steps (90 / 100): [Loss_D_real: -0.493896, Loss_D_fake: 0.089168] [loss_D: -0.202364] [Loss_G: -0.089716]\n",
      "Steps (100 / 100): [Loss_D_real: -0.522904, Loss_D_fake: 0.094729] [loss_D: -0.214088] [Loss_G: -0.091661]\n",
      "EPOCH #  9 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.525642, Loss_D_fake: 0.092483] [loss_D: -0.216580] [Loss_G: -0.095804]\n",
      "Steps (20 / 100): [Loss_D_real: -0.429198, Loss_D_fake: 0.100233] [loss_D: -0.164482] [Loss_G: -0.098222]\n",
      "Steps (30 / 100): [Loss_D_real: -0.487414, Loss_D_fake: 0.106436] [loss_D: -0.190489] [Loss_G: -0.101610]\n",
      "Steps (40 / 100): [Loss_D_real: -0.484107, Loss_D_fake: 0.106268] [loss_D: -0.188919] [Loss_G: -0.103768]\n",
      "Steps (50 / 100): [Loss_D_real: -0.497223, Loss_D_fake: 0.109623] [loss_D: -0.193800] [Loss_G: -0.113261]\n",
      "Steps (60 / 100): [Loss_D_real: -0.545467, Loss_D_fake: 0.115752] [loss_D: -0.214858] [Loss_G: -0.106671]\n",
      "Steps (70 / 100): [Loss_D_real: -0.563333, Loss_D_fake: 0.113579] [loss_D: -0.224877] [Loss_G: -0.114950]\n",
      "Steps (80 / 100): [Loss_D_real: -0.544284, Loss_D_fake: 0.116732] [loss_D: -0.213776] [Loss_G: -0.119914]\n",
      "Steps (90 / 100): [Loss_D_real: -0.481054, Loss_D_fake: 0.117860] [loss_D: -0.181597] [Loss_G: -0.121102]\n",
      "Steps (100 / 100): [Loss_D_real: -0.501014, Loss_D_fake: 0.124595] [loss_D: -0.188209] [Loss_G: -0.123702]\n",
      "EPOCH #  10 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.531215, Loss_D_fake: 0.125829] [loss_D: -0.202693] [Loss_G: -0.120304]\n",
      "Steps (20 / 100): [Loss_D_real: -0.518339, Loss_D_fake: 0.129108] [loss_D: -0.194615] [Loss_G: -0.123875]\n",
      "Steps (30 / 100): [Loss_D_real: -0.521627, Loss_D_fake: 0.133052] [loss_D: -0.194287] [Loss_G: -0.126348]\n",
      "Steps (40 / 100): [Loss_D_real: -0.508550, Loss_D_fake: 0.129526] [loss_D: -0.189512] [Loss_G: -0.131111]\n",
      "Steps (50 / 100): [Loss_D_real: -0.535571, Loss_D_fake: 0.135120] [loss_D: -0.200225] [Loss_G: -0.139758]\n",
      "Steps (60 / 100): [Loss_D_real: -0.558246, Loss_D_fake: 0.136261] [loss_D: -0.210992] [Loss_G: -0.139744]\n",
      "Steps (70 / 100): [Loss_D_real: -0.558147, Loss_D_fake: 0.147174] [loss_D: -0.205486] [Loss_G: -0.140078]\n",
      "Steps (80 / 100): [Loss_D_real: -0.551334, Loss_D_fake: 0.144721] [loss_D: -0.203306] [Loss_G: -0.142464]\n",
      "Steps (90 / 100): [Loss_D_real: -0.571100, Loss_D_fake: 0.147404] [loss_D: -0.211848] [Loss_G: -0.146362]\n",
      "Steps (100 / 100): [Loss_D_real: -0.638089, Loss_D_fake: 0.149464] [loss_D: -0.244312] [Loss_G: -0.151280]\n",
      "EPOCH #  11 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.535906, Loss_D_fake: 0.151689] [loss_D: -0.192109] [Loss_G: -0.146075]\n",
      "Steps (20 / 100): [Loss_D_real: -0.537568, Loss_D_fake: 0.155962] [loss_D: -0.190803] [Loss_G: -0.151801]\n",
      "Steps (30 / 100): [Loss_D_real: -0.531261, Loss_D_fake: 0.157402] [loss_D: -0.186930] [Loss_G: -0.161834]\n",
      "Steps (40 / 100): [Loss_D_real: -0.503820, Loss_D_fake: 0.147626] [loss_D: -0.178097] [Loss_G: -0.157026]\n",
      "Steps (50 / 100): [Loss_D_real: -0.518981, Loss_D_fake: 0.160900] [loss_D: -0.179041] [Loss_G: -0.155530]\n",
      "Steps (60 / 100): [Loss_D_real: -0.572013, Loss_D_fake: 0.161452] [loss_D: -0.205281] [Loss_G: -0.162644]\n",
      "Steps (70 / 100): [Loss_D_real: -0.577589, Loss_D_fake: 0.165217] [loss_D: -0.206186] [Loss_G: -0.168860]\n",
      "Steps (80 / 100): [Loss_D_real: -0.531552, Loss_D_fake: 0.173563] [loss_D: -0.178994] [Loss_G: -0.165631]\n",
      "Steps (90 / 100): [Loss_D_real: -0.558005, Loss_D_fake: 0.173368] [loss_D: -0.192319] [Loss_G: -0.172271]\n",
      "Steps (100 / 100): [Loss_D_real: -0.532218, Loss_D_fake: 0.172494] [loss_D: -0.179862] [Loss_G: -0.173585]\n",
      "EPOCH #  12 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.566832, Loss_D_fake: 0.177666] [loss_D: -0.194583] [Loss_G: -0.179802]\n",
      "Steps (20 / 100): [Loss_D_real: -0.550532, Loss_D_fake: 0.180856] [loss_D: -0.184838] [Loss_G: -0.182508]\n",
      "Steps (30 / 100): [Loss_D_real: -0.521498, Loss_D_fake: 0.182713] [loss_D: -0.169392] [Loss_G: -0.185243]\n",
      "Steps (40 / 100): [Loss_D_real: -0.510366, Loss_D_fake: 0.193356] [loss_D: -0.158505] [Loss_G: -0.188769]\n",
      "Steps (50 / 100): [Loss_D_real: -0.514269, Loss_D_fake: 0.183629] [loss_D: -0.165320] [Loss_G: -0.189077]\n",
      "Steps (60 / 100): [Loss_D_real: -0.564200, Loss_D_fake: 0.195320] [loss_D: -0.184440] [Loss_G: -0.192738]\n",
      "Steps (70 / 100): [Loss_D_real: -0.498379, Loss_D_fake: 0.191600] [loss_D: -0.153389] [Loss_G: -0.190593]\n",
      "Steps (80 / 100): [Loss_D_real: -0.590978, Loss_D_fake: 0.197377] [loss_D: -0.196801] [Loss_G: -0.192515]\n",
      "Steps (90 / 100): [Loss_D_real: -0.537146, Loss_D_fake: 0.205234] [loss_D: -0.165956] [Loss_G: -0.193669]\n",
      "Steps (100 / 100): [Loss_D_real: -0.509562, Loss_D_fake: 0.208766] [loss_D: -0.150398] [Loss_G: -0.205949]\n",
      "EPOCH #  13 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.495700, Loss_D_fake: 0.201069] [loss_D: -0.147316] [Loss_G: -0.204124]\n",
      "Steps (20 / 100): [Loss_D_real: -0.541610, Loss_D_fake: 0.204997] [loss_D: -0.168307] [Loss_G: -0.208706]\n",
      "Steps (30 / 100): [Loss_D_real: -0.532596, Loss_D_fake: 0.215146] [loss_D: -0.158725] [Loss_G: -0.213791]\n",
      "Steps (40 / 100): [Loss_D_real: -0.538026, Loss_D_fake: 0.217912] [loss_D: -0.160057] [Loss_G: -0.207039]\n",
      "Steps (50 / 100): [Loss_D_real: -0.548578, Loss_D_fake: 0.223492] [loss_D: -0.162543] [Loss_G: -0.215331]\n",
      "Steps (60 / 100): [Loss_D_real: -0.527969, Loss_D_fake: 0.216049] [loss_D: -0.155960] [Loss_G: -0.218073]\n",
      "Steps (70 / 100): [Loss_D_real: -0.572269, Loss_D_fake: 0.229640] [loss_D: -0.171315] [Loss_G: -0.222423]\n",
      "Steps (80 / 100): [Loss_D_real: -0.551529, Loss_D_fake: 0.228058] [loss_D: -0.161736] [Loss_G: -0.220574]\n",
      "Steps (90 / 100): [Loss_D_real: -0.521217, Loss_D_fake: 0.231528] [loss_D: -0.144845] [Loss_G: -0.221830]\n",
      "Steps (100 / 100): [Loss_D_real: -0.543505, Loss_D_fake: 0.231664] [loss_D: -0.155920] [Loss_G: -0.222605]\n",
      "EPOCH #  14 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.499309, Loss_D_fake: 0.234540] [loss_D: -0.132385] [Loss_G: -0.224566]\n",
      "Steps (20 / 100): [Loss_D_real: -0.496875, Loss_D_fake: 0.238103] [loss_D: -0.129386] [Loss_G: -0.229263]\n",
      "Steps (30 / 100): [Loss_D_real: -0.517726, Loss_D_fake: 0.236247] [loss_D: -0.140739] [Loss_G: -0.240290]\n",
      "Steps (40 / 100): [Loss_D_real: -0.526819, Loss_D_fake: 0.242881] [loss_D: -0.141969] [Loss_G: -0.236429]\n",
      "Steps (50 / 100): [Loss_D_real: -0.591180, Loss_D_fake: 0.239075] [loss_D: -0.176052] [Loss_G: -0.235638]\n",
      "Steps (60 / 100): [Loss_D_real: -0.521654, Loss_D_fake: 0.244061] [loss_D: -0.138797] [Loss_G: -0.247040]\n",
      "Steps (70 / 100): [Loss_D_real: -0.555371, Loss_D_fake: 0.253013] [loss_D: -0.151179] [Loss_G: -0.247029]\n",
      "Steps (80 / 100): [Loss_D_real: -0.512760, Loss_D_fake: 0.257709] [loss_D: -0.127525] [Loss_G: -0.249032]\n",
      "Steps (90 / 100): [Loss_D_real: -0.490547, Loss_D_fake: 0.254345] [loss_D: -0.118101] [Loss_G: -0.261405]\n",
      "Steps (100 / 100): [Loss_D_real: -0.508516, Loss_D_fake: 0.256096] [loss_D: -0.126210] [Loss_G: -0.265919]\n",
      "EPOCH #  15 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.626931, Loss_D_fake: 0.261558] [loss_D: -0.182687] [Loss_G: -0.264789]\n",
      "Steps (20 / 100): [Loss_D_real: -0.530560, Loss_D_fake: 0.266452] [loss_D: -0.132054] [Loss_G: -0.249670]\n",
      "Steps (30 / 100): [Loss_D_real: -0.526016, Loss_D_fake: 0.275560] [loss_D: -0.125228] [Loss_G: -0.274844]\n",
      "Steps (40 / 100): [Loss_D_real: -0.502531, Loss_D_fake: 0.277871] [loss_D: -0.112330] [Loss_G: -0.264657]\n",
      "Steps (50 / 100): [Loss_D_real: -0.493584, Loss_D_fake: 0.270748] [loss_D: -0.111418] [Loss_G: -0.258825]\n",
      "Steps (60 / 100): [Loss_D_real: -0.577266, Loss_D_fake: 0.279857] [loss_D: -0.148705] [Loss_G: -0.265575]\n",
      "Steps (70 / 100): [Loss_D_real: -0.530847, Loss_D_fake: 0.267859] [loss_D: -0.131494] [Loss_G: -0.279167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (80 / 100): [Loss_D_real: -0.533130, Loss_D_fake: 0.283724] [loss_D: -0.124703] [Loss_G: -0.279952]\n",
      "Steps (90 / 100): [Loss_D_real: -0.532214, Loss_D_fake: 0.290887] [loss_D: -0.120663] [Loss_G: -0.283596]\n",
      "Steps (100 / 100): [Loss_D_real: -0.533592, Loss_D_fake: 0.290791] [loss_D: -0.121401] [Loss_G: -0.278192]\n",
      "EPOCH #  16 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.495205, Loss_D_fake: 0.289416] [loss_D: -0.102894] [Loss_G: -0.282818]\n",
      "Steps (20 / 100): [Loss_D_real: -0.522162, Loss_D_fake: 0.287018] [loss_D: -0.117572] [Loss_G: -0.282321]\n",
      "Steps (30 / 100): [Loss_D_real: -0.532921, Loss_D_fake: 0.308699] [loss_D: -0.112111] [Loss_G: -0.288539]\n",
      "Steps (40 / 100): [Loss_D_real: -0.503958, Loss_D_fake: 0.308547] [loss_D: -0.097706] [Loss_G: -0.298276]\n",
      "Steps (50 / 100): [Loss_D_real: -0.525074, Loss_D_fake: 0.303890] [loss_D: -0.110592] [Loss_G: -0.293342]\n",
      "Steps (60 / 100): [Loss_D_real: -0.506355, Loss_D_fake: 0.317411] [loss_D: -0.094472] [Loss_G: -0.302998]\n",
      "Steps (70 / 100): [Loss_D_real: -0.525423, Loss_D_fake: 0.307226] [loss_D: -0.109098] [Loss_G: -0.299805]\n",
      "Steps (80 / 100): [Loss_D_real: -0.523432, Loss_D_fake: 0.320666] [loss_D: -0.101383] [Loss_G: -0.308460]\n",
      "Steps (90 / 100): [Loss_D_real: -0.527136, Loss_D_fake: 0.321269] [loss_D: -0.102933] [Loss_G: -0.302370]\n",
      "Steps (100 / 100): [Loss_D_real: -0.533013, Loss_D_fake: 0.331567] [loss_D: -0.100723] [Loss_G: -0.313807]\n",
      "EPOCH #  17 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.550452, Loss_D_fake: 0.316670] [loss_D: -0.116891] [Loss_G: -0.315906]\n",
      "Steps (20 / 100): [Loss_D_real: -0.498997, Loss_D_fake: 0.320663] [loss_D: -0.089167] [Loss_G: -0.319154]\n",
      "Steps (30 / 100): [Loss_D_real: -0.517627, Loss_D_fake: 0.317480] [loss_D: -0.100074] [Loss_G: -0.316841]\n",
      "Steps (40 / 100): [Loss_D_real: -0.501198, Loss_D_fake: 0.321516] [loss_D: -0.089841] [Loss_G: -0.323480]\n",
      "Steps (50 / 100): [Loss_D_real: -0.478522, Loss_D_fake: 0.335467] [loss_D: -0.071528] [Loss_G: -0.329108]\n",
      "Steps (60 / 100): [Loss_D_real: -0.505167, Loss_D_fake: 0.328563] [loss_D: -0.088302] [Loss_G: -0.327686]\n",
      "Steps (70 / 100): [Loss_D_real: -0.556810, Loss_D_fake: 0.331374] [loss_D: -0.112718] [Loss_G: -0.337476]\n",
      "Steps (80 / 100): [Loss_D_real: -0.488652, Loss_D_fake: 0.340312] [loss_D: -0.074170] [Loss_G: -0.336463]\n",
      "Steps (90 / 100): [Loss_D_real: -0.515267, Loss_D_fake: 0.344788] [loss_D: -0.085239] [Loss_G: -0.339352]\n",
      "Steps (100 / 100): [Loss_D_real: -0.544216, Loss_D_fake: 0.342472] [loss_D: -0.100872] [Loss_G: -0.339542]\n",
      "EPOCH #  18 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.488940, Loss_D_fake: 0.348142] [loss_D: -0.070399] [Loss_G: -0.348631]\n",
      "Steps (20 / 100): [Loss_D_real: -0.513494, Loss_D_fake: 0.356611] [loss_D: -0.078442] [Loss_G: -0.344795]\n",
      "Steps (30 / 100): [Loss_D_real: -0.518604, Loss_D_fake: 0.343384] [loss_D: -0.087610] [Loss_G: -0.356565]\n",
      "Steps (40 / 100): [Loss_D_real: -0.556748, Loss_D_fake: 0.347622] [loss_D: -0.104563] [Loss_G: -0.355677]\n",
      "Steps (50 / 100): [Loss_D_real: -0.516212, Loss_D_fake: 0.361465] [loss_D: -0.077373] [Loss_G: -0.369151]\n",
      "Steps (60 / 100): [Loss_D_real: -0.520164, Loss_D_fake: 0.365796] [loss_D: -0.077184] [Loss_G: -0.359203]\n",
      "Steps (70 / 100): [Loss_D_real: -0.478772, Loss_D_fake: 0.369069] [loss_D: -0.054851] [Loss_G: -0.357012]\n",
      "Steps (80 / 100): [Loss_D_real: -0.497173, Loss_D_fake: 0.352767] [loss_D: -0.072203] [Loss_G: -0.361839]\n",
      "Steps (90 / 100): [Loss_D_real: -0.536976, Loss_D_fake: 0.367660] [loss_D: -0.084658] [Loss_G: -0.362247]\n",
      "Steps (100 / 100): [Loss_D_real: -0.486851, Loss_D_fake: 0.362757] [loss_D: -0.062047] [Loss_G: -0.361580]\n",
      "EPOCH #  19 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.482288, Loss_D_fake: 0.372480] [loss_D: -0.054904] [Loss_G: -0.364963]\n",
      "Steps (20 / 100): [Loss_D_real: -0.548189, Loss_D_fake: 0.379472] [loss_D: -0.084359] [Loss_G: -0.359932]\n",
      "Steps (30 / 100): [Loss_D_real: -0.494762, Loss_D_fake: 0.372197] [loss_D: -0.061282] [Loss_G: -0.374768]\n",
      "Steps (40 / 100): [Loss_D_real: -0.521850, Loss_D_fake: 0.368361] [loss_D: -0.076745] [Loss_G: -0.368385]\n",
      "Steps (50 / 100): [Loss_D_real: -0.463223, Loss_D_fake: 0.378575] [loss_D: -0.042324] [Loss_G: -0.381251]\n",
      "Steps (60 / 100): [Loss_D_real: -0.500462, Loss_D_fake: 0.375942] [loss_D: -0.062260] [Loss_G: -0.373699]\n",
      "Steps (70 / 100): [Loss_D_real: -0.479801, Loss_D_fake: 0.384275] [loss_D: -0.047763] [Loss_G: -0.389126]\n",
      "Steps (80 / 100): [Loss_D_real: -0.486542, Loss_D_fake: 0.382847] [loss_D: -0.051847] [Loss_G: -0.372131]\n",
      "Steps (90 / 100): [Loss_D_real: -0.495300, Loss_D_fake: 0.385033] [loss_D: -0.055134] [Loss_G: -0.377594]\n",
      "Steps (100 / 100): [Loss_D_real: -0.496257, Loss_D_fake: 0.392393] [loss_D: -0.051932] [Loss_G: -0.389488]\n",
      "EPOCH #  20 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.494366, Loss_D_fake: 0.387558] [loss_D: -0.053404] [Loss_G: -0.389853]\n",
      "Steps (20 / 100): [Loss_D_real: -0.452661, Loss_D_fake: 0.400125] [loss_D: -0.026268] [Loss_G: -0.384335]\n",
      "Steps (30 / 100): [Loss_D_real: -0.493970, Loss_D_fake: 0.389575] [loss_D: -0.052197] [Loss_G: -0.404711]\n",
      "Steps (40 / 100): [Loss_D_real: -0.467702, Loss_D_fake: 0.416180] [loss_D: -0.025761] [Loss_G: -0.379995]\n",
      "Steps (50 / 100): [Loss_D_real: -0.465818, Loss_D_fake: 0.405849] [loss_D: -0.029984] [Loss_G: -0.389340]\n",
      "Steps (60 / 100): [Loss_D_real: -0.480280, Loss_D_fake: 0.401465] [loss_D: -0.039407] [Loss_G: -0.387807]\n",
      "Steps (70 / 100): [Loss_D_real: -0.454519, Loss_D_fake: 0.390782] [loss_D: -0.031868] [Loss_G: -0.404876]\n",
      "Steps (80 / 100): [Loss_D_real: -0.469890, Loss_D_fake: 0.405267] [loss_D: -0.032311] [Loss_G: -0.392758]\n",
      "Steps (90 / 100): [Loss_D_real: -0.468460, Loss_D_fake: 0.393147] [loss_D: -0.037657] [Loss_G: -0.401485]\n",
      "Steps (100 / 100): [Loss_D_real: -0.499510, Loss_D_fake: 0.407305] [loss_D: -0.046103] [Loss_G: -0.407437]\n",
      "EPOCH #  21 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.507586, Loss_D_fake: 0.405380] [loss_D: -0.051103] [Loss_G: -0.393991]\n",
      "Steps (20 / 100): [Loss_D_real: -0.499017, Loss_D_fake: 0.400726] [loss_D: -0.049146] [Loss_G: -0.415324]\n",
      "Steps (30 / 100): [Loss_D_real: -0.496506, Loss_D_fake: 0.401358] [loss_D: -0.047574] [Loss_G: -0.414067]\n",
      "Steps (40 / 100): [Loss_D_real: -0.455122, Loss_D_fake: 0.405508] [loss_D: -0.024807] [Loss_G: -0.388770]\n",
      "Steps (50 / 100): [Loss_D_real: -0.457637, Loss_D_fake: 0.415593] [loss_D: -0.021022] [Loss_G: -0.404662]\n",
      "Steps (60 / 100): [Loss_D_real: -0.454427, Loss_D_fake: 0.401794] [loss_D: -0.026317] [Loss_G: -0.400584]\n",
      "Steps (70 / 100): [Loss_D_real: -0.478284, Loss_D_fake: 0.416539] [loss_D: -0.030872] [Loss_G: -0.397805]\n",
      "Steps (80 / 100): [Loss_D_real: -0.454005, Loss_D_fake: 0.431265] [loss_D: -0.011370] [Loss_G: -0.393306]\n",
      "Steps (90 / 100): [Loss_D_real: -0.433589, Loss_D_fake: 0.400619] [loss_D: -0.016485] [Loss_G: -0.402525]\n",
      "Steps (100 / 100): [Loss_D_real: -0.460730, Loss_D_fake: 0.395306] [loss_D: -0.032712] [Loss_G: -0.413565]\n",
      "EPOCH #  22 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.444187, Loss_D_fake: 0.413876] [loss_D: -0.015156] [Loss_G: -0.417699]\n",
      "Steps (20 / 100): [Loss_D_real: -0.437226, Loss_D_fake: 0.404819] [loss_D: -0.016204] [Loss_G: -0.405601]\n",
      "Steps (30 / 100): [Loss_D_real: -0.470244, Loss_D_fake: 0.402926] [loss_D: -0.033659] [Loss_G: -0.403018]\n",
      "Steps (40 / 100): [Loss_D_real: -0.442405, Loss_D_fake: 0.421251] [loss_D: -0.010577] [Loss_G: -0.399710]\n",
      "Steps (50 / 100): [Loss_D_real: -0.440199, Loss_D_fake: 0.425645] [loss_D: -0.007277] [Loss_G: -0.416689]\n",
      "Steps (60 / 100): [Loss_D_real: -0.473465, Loss_D_fake: 0.416112] [loss_D: -0.028677] [Loss_G: -0.414736]\n",
      "Steps (70 / 100): [Loss_D_real: -0.441259, Loss_D_fake: 0.408238] [loss_D: -0.016511] [Loss_G: -0.400464]\n",
      "Steps (80 / 100): [Loss_D_real: -0.478643, Loss_D_fake: 0.399875] [loss_D: -0.039384] [Loss_G: -0.392927]\n",
      "Steps (90 / 100): [Loss_D_real: -0.406708, Loss_D_fake: 0.400804] [loss_D: -0.002952] [Loss_G: -0.407857]\n",
      "Steps (100 / 100): [Loss_D_real: -0.402429, Loss_D_fake: 0.409682] [loss_D: 0.003627] [Loss_G: -0.422844]\n",
      "EPOCH #  23 --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps (10 / 100): [Loss_D_real: -0.383561, Loss_D_fake: 0.405564] [loss_D: 0.011002] [Loss_G: -0.407586]\n",
      "Steps (20 / 100): [Loss_D_real: -0.387783, Loss_D_fake: 0.400810] [loss_D: 0.006514] [Loss_G: -0.403273]\n",
      "Steps (30 / 100): [Loss_D_real: -0.448515, Loss_D_fake: 0.402118] [loss_D: -0.023199] [Loss_G: -0.411359]\n",
      "Steps (40 / 100): [Loss_D_real: -0.416860, Loss_D_fake: 0.412109] [loss_D: -0.002375] [Loss_G: -0.397570]\n",
      "Steps (50 / 100): [Loss_D_real: -0.438488, Loss_D_fake: 0.403005] [loss_D: -0.017742] [Loss_G: -0.412979]\n",
      "Steps (60 / 100): [Loss_D_real: -0.399133, Loss_D_fake: 0.394874] [loss_D: -0.002130] [Loss_G: -0.404954]\n",
      "Steps (70 / 100): [Loss_D_real: -0.410238, Loss_D_fake: 0.397681] [loss_D: -0.006278] [Loss_G: -0.404110]\n",
      "Steps (80 / 100): [Loss_D_real: -0.411013, Loss_D_fake: 0.407505] [loss_D: -0.001754] [Loss_G: -0.398273]\n",
      "Steps (90 / 100): [Loss_D_real: -0.380009, Loss_D_fake: 0.413161] [loss_D: 0.016576] [Loss_G: -0.397908]\n",
      "Steps (100 / 100): [Loss_D_real: -0.386745, Loss_D_fake: 0.398151] [loss_D: 0.005703] [Loss_G: -0.394332]\n",
      "EPOCH #  24 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.383693, Loss_D_fake: 0.381946] [loss_D: -0.000874] [Loss_G: -0.402227]\n",
      "Steps (20 / 100): [Loss_D_real: -0.372156, Loss_D_fake: 0.381438] [loss_D: 0.004641] [Loss_G: -0.387583]\n",
      "Steps (30 / 100): [Loss_D_real: -0.393963, Loss_D_fake: 0.397050] [loss_D: 0.001544] [Loss_G: -0.385365]\n",
      "Steps (40 / 100): [Loss_D_real: -0.364291, Loss_D_fake: 0.393196] [loss_D: 0.014453] [Loss_G: -0.382662]\n",
      "Steps (50 / 100): [Loss_D_real: -0.394991, Loss_D_fake: 0.370554] [loss_D: -0.012219] [Loss_G: -0.379108]\n",
      "Steps (60 / 100): [Loss_D_real: -0.394907, Loss_D_fake: 0.387784] [loss_D: -0.003561] [Loss_G: -0.381537]\n",
      "Steps (70 / 100): [Loss_D_real: -0.360311, Loss_D_fake: 0.377335] [loss_D: 0.008512] [Loss_G: -0.356720]\n",
      "Steps (80 / 100): [Loss_D_real: -0.349043, Loss_D_fake: 0.371596] [loss_D: 0.011276] [Loss_G: -0.360944]\n",
      "Steps (90 / 100): [Loss_D_real: -0.349122, Loss_D_fake: 0.370581] [loss_D: 0.010729] [Loss_G: -0.356007]\n",
      "Steps (100 / 100): [Loss_D_real: -0.366148, Loss_D_fake: 0.353446] [loss_D: -0.006351] [Loss_G: -0.366471]\n",
      "EPOCH #  25 --------------------------------------------------\n",
      "Steps (10 / 100): [Loss_D_real: -0.341343, Loss_D_fake: 0.354391] [loss_D: 0.006524] [Loss_G: -0.354556]\n",
      "Steps (20 / 100): [Loss_D_real: -0.338171, Loss_D_fake: 0.359762] [loss_D: 0.010795] [Loss_G: -0.356082]\n",
      "Steps (30 / 100): [Loss_D_real: -0.341883, Loss_D_fake: 0.355991] [loss_D: 0.007054] [Loss_G: -0.344766]\n",
      "Steps (40 / 100): [Loss_D_real: -0.376390, Loss_D_fake: 0.338737] [loss_D: -0.018827] [Loss_G: -0.344352]\n",
      "Steps (50 / 100): [Loss_D_real: -0.324772, Loss_D_fake: 0.337368] [loss_D: 0.006298] [Loss_G: -0.339929]\n",
      "Steps (60 / 100): [Loss_D_real: -0.342895, Loss_D_fake: 0.336646] [loss_D: -0.003124] [Loss_G: -0.331621]\n",
      "Steps (70 / 100): [Loss_D_real: -0.308424, Loss_D_fake: 0.326584] [loss_D: 0.009080] [Loss_G: -0.316403]\n",
      "Steps (80 / 100): [Loss_D_real: -0.312868, Loss_D_fake: 0.321104] [loss_D: 0.004118] [Loss_G: -0.327871]\n",
      "Steps (90 / 100): [Loss_D_real: -0.316464, Loss_D_fake: 0.325571] [loss_D: 0.004554] [Loss_G: -0.323458]\n",
      "Steps (100 / 100): [Loss_D_real: -0.304579, Loss_D_fake: 0.320970] [loss_D: 0.008195] [Loss_G: -0.320510]\n"
     ]
    }
   ],
   "source": [
    "wgan = WGAN(g_model=G_wgan, d_model=D_wgan)\n",
    "\n",
    "EPOCHS = 25\n",
    "X_train_fraud = x_train_fraud.to_numpy()\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH # ', epoch + 1, '-' * 50)\n",
    "    wgan.train(X_train_fraud, batch_size=128, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.generator.save('wgan_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.014839</td>\n",
       "      <td>-6.305023</td>\n",
       "      <td>10.341335</td>\n",
       "      <td>-8.574209</td>\n",
       "      <td>8.299900</td>\n",
       "      <td>-9.044720</td>\n",
       "      <td>-4.633235</td>\n",
       "      <td>-11.389522</td>\n",
       "      <td>0.204499</td>\n",
       "      <td>-7.789335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>-0.014080</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>-0.073361</td>\n",
       "      <td>-0.092199</td>\n",
       "      <td>-0.017477</td>\n",
       "      <td>0.106953</td>\n",
       "      <td>0.167389</td>\n",
       "      <td>0.107494</td>\n",
       "      <td>3.427012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.801548</td>\n",
       "      <td>1.402232</td>\n",
       "      <td>2.401234</td>\n",
       "      <td>2.029324</td>\n",
       "      <td>1.945500</td>\n",
       "      <td>2.128428</td>\n",
       "      <td>1.141382</td>\n",
       "      <td>2.698648</td>\n",
       "      <td>0.297286</td>\n",
       "      <td>1.878873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359798</td>\n",
       "      <td>0.309235</td>\n",
       "      <td>0.372441</td>\n",
       "      <td>0.339191</td>\n",
       "      <td>0.301301</td>\n",
       "      <td>0.367543</td>\n",
       "      <td>0.265344</td>\n",
       "      <td>0.338120</td>\n",
       "      <td>0.396265</td>\n",
       "      <td>0.842888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.049931</td>\n",
       "      <td>-11.222009</td>\n",
       "      <td>3.486689</td>\n",
       "      <td>-15.068597</td>\n",
       "      <td>2.739768</td>\n",
       "      <td>-16.394028</td>\n",
       "      <td>-8.617829</td>\n",
       "      <td>-20.925287</td>\n",
       "      <td>-1.078363</td>\n",
       "      <td>-14.348653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084793</td>\n",
       "      <td>-0.914717</td>\n",
       "      <td>-1.099021</td>\n",
       "      <td>-1.171458</td>\n",
       "      <td>-0.924914</td>\n",
       "      <td>-1.043308</td>\n",
       "      <td>-0.764414</td>\n",
       "      <td>-1.107698</td>\n",
       "      <td>-1.326581</td>\n",
       "      <td>1.401648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.999061</td>\n",
       "      <td>-7.196817</td>\n",
       "      <td>8.626605</td>\n",
       "      <td>-9.757539</td>\n",
       "      <td>6.917021</td>\n",
       "      <td>-10.372511</td>\n",
       "      <td>-5.322743</td>\n",
       "      <td>-13.013300</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>-8.923686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726711</td>\n",
       "      <td>-0.233750</td>\n",
       "      <td>-0.253383</td>\n",
       "      <td>-0.290629</td>\n",
       "      <td>-0.300965</td>\n",
       "      <td>-0.269029</td>\n",
       "      <td>-0.079615</td>\n",
       "      <td>-0.061865</td>\n",
       "      <td>-0.155170</td>\n",
       "      <td>2.834129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.807904</td>\n",
       "      <td>-6.215917</td>\n",
       "      <td>10.205181</td>\n",
       "      <td>-8.455754</td>\n",
       "      <td>8.195798</td>\n",
       "      <td>-8.812211</td>\n",
       "      <td>-4.535729</td>\n",
       "      <td>-11.154415</td>\n",
       "      <td>0.200554</td>\n",
       "      <td>-7.562764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960777</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>-0.005370</td>\n",
       "      <td>-0.069215</td>\n",
       "      <td>-0.107100</td>\n",
       "      <td>-0.052621</td>\n",
       "      <td>0.096420</td>\n",
       "      <td>0.168851</td>\n",
       "      <td>0.125779</td>\n",
       "      <td>3.344017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.763011</td>\n",
       "      <td>-5.354254</td>\n",
       "      <td>11.860113</td>\n",
       "      <td>-7.109643</td>\n",
       "      <td>9.497823</td>\n",
       "      <td>-7.527036</td>\n",
       "      <td>-3.848781</td>\n",
       "      <td>-9.431325</td>\n",
       "      <td>0.409282</td>\n",
       "      <td>-6.445704</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200804</td>\n",
       "      <td>0.187754</td>\n",
       "      <td>0.240286</td>\n",
       "      <td>0.136887</td>\n",
       "      <td>0.120294</td>\n",
       "      <td>0.216406</td>\n",
       "      <td>0.272310</td>\n",
       "      <td>0.394001</td>\n",
       "      <td>0.376652</td>\n",
       "      <td>3.947572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.611126</td>\n",
       "      <td>-2.239507</td>\n",
       "      <td>18.201218</td>\n",
       "      <td>-2.895845</td>\n",
       "      <td>14.578374</td>\n",
       "      <td>-3.071097</td>\n",
       "      <td>-1.530158</td>\n",
       "      <td>-3.991794</td>\n",
       "      <td>1.102077</td>\n",
       "      <td>-2.510722</td>\n",
       "      <td>...</td>\n",
       "      <td>2.296494</td>\n",
       "      <td>1.050034</td>\n",
       "      <td>1.329247</td>\n",
       "      <td>0.914874</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>1.582470</td>\n",
       "      <td>0.948782</td>\n",
       "      <td>1.246364</td>\n",
       "      <td>1.279843</td>\n",
       "      <td>6.344531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     12.014839    -6.305023    10.341335    -8.574209     8.299900   \n",
       "std       2.801548     1.402232     2.401234     2.029324     1.945500   \n",
       "min       4.049931   -11.222009     3.486689   -15.068597     2.739768   \n",
       "25%       9.999061    -7.196817     8.626605    -9.757539     6.917021   \n",
       "50%      11.807904    -6.215917    10.205181    -8.455754     8.195798   \n",
       "75%      13.763011    -5.354254    11.860113    -7.109643     9.497823   \n",
       "max      21.611126    -2.239507    18.201218    -2.895845    14.578374   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -9.044720    -4.633235   -11.389522     0.204499    -7.789335  ...   \n",
       "std       2.128428     1.141382     2.698648     0.297286     1.878873  ...   \n",
       "min     -16.394028    -8.617829   -20.925287    -1.078363   -14.348653  ...   \n",
       "25%     -10.372511    -5.322743   -13.013300     0.008828    -8.923686  ...   \n",
       "50%      -8.812211    -4.535729   -11.154415     0.200554    -7.562764  ...   \n",
       "75%      -7.527036    -3.848781    -9.431325     0.409282    -6.445704  ...   \n",
       "max      -3.071097    -1.530158    -3.991794     1.102077    -2.510722  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.975866    -0.014080     0.002197    -0.073361    -0.092199   \n",
       "std       0.359798     0.309235     0.372441     0.339191     0.301301   \n",
       "min      -0.084793    -0.914717    -1.099021    -1.171458    -0.924914   \n",
       "25%       0.726711    -0.233750    -0.253383    -0.290629    -0.300965   \n",
       "50%       0.960777    -0.014733    -0.005370    -0.069215    -0.107100   \n",
       "75%       1.200804     0.187754     0.240286     0.136887     0.120294   \n",
       "max       2.296494     1.050034     1.329247     0.914874     0.953445   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.017477     0.106953     0.167389     0.107494     3.427012  \n",
       "std       0.367543     0.265344     0.338120     0.396265     0.842888  \n",
       "min      -1.043308    -0.764414    -1.107698    -1.326581     1.401648  \n",
       "25%      -0.269029    -0.079615    -0.061865    -0.155170     2.834129  \n",
       "50%      -0.052621     0.096420     0.168851     0.125779     3.344017  \n",
       "75%       0.216406     0.272310     0.394001     0.376652     3.947572  \n",
       "max       1.582470     0.948782     1.246364     1.279843     6.344531  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_1000wgan, x_train_gen_1000wgan, y_train_gen_1000wgan = gen_data(wgan.generator, 1000)\n",
    "df_gen_1000wgan = pd.DataFrame(data=gen_1000wgan, index=None, columns=x_train.columns)\n",
    "df_gen_1000wgan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9995435553526912\n",
      "Precision:  0.8673469387755102\n",
      "Recall:  0.8673469387755102\n",
      "F1 score:  0.8673469387755102\n",
      "ROC AUC score:  0.9335591615655828\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_1000wgan = XGBC_model_predit(x_train_gen_1000wgan, y_train_gen_1000wgan)\n",
    "check_performance(y_test, y_pred_gen_1000wgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_1000wgan, 'WGAN 1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.014839</td>\n",
       "      <td>-6.305023</td>\n",
       "      <td>10.341335</td>\n",
       "      <td>-8.574209</td>\n",
       "      <td>8.299900</td>\n",
       "      <td>-9.044720</td>\n",
       "      <td>-4.633235</td>\n",
       "      <td>-11.389522</td>\n",
       "      <td>0.204499</td>\n",
       "      <td>-7.789335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>-0.014080</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>-0.073361</td>\n",
       "      <td>-0.092199</td>\n",
       "      <td>-0.017477</td>\n",
       "      <td>0.106953</td>\n",
       "      <td>0.167389</td>\n",
       "      <td>0.107494</td>\n",
       "      <td>3.427012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.801548</td>\n",
       "      <td>1.402232</td>\n",
       "      <td>2.401234</td>\n",
       "      <td>2.029324</td>\n",
       "      <td>1.945500</td>\n",
       "      <td>2.128428</td>\n",
       "      <td>1.141382</td>\n",
       "      <td>2.698648</td>\n",
       "      <td>0.297286</td>\n",
       "      <td>1.878873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359798</td>\n",
       "      <td>0.309235</td>\n",
       "      <td>0.372441</td>\n",
       "      <td>0.339191</td>\n",
       "      <td>0.301301</td>\n",
       "      <td>0.367543</td>\n",
       "      <td>0.265344</td>\n",
       "      <td>0.338120</td>\n",
       "      <td>0.396265</td>\n",
       "      <td>0.842888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.049931</td>\n",
       "      <td>-11.222009</td>\n",
       "      <td>3.486689</td>\n",
       "      <td>-15.068597</td>\n",
       "      <td>2.739768</td>\n",
       "      <td>-16.394028</td>\n",
       "      <td>-8.617829</td>\n",
       "      <td>-20.925287</td>\n",
       "      <td>-1.078363</td>\n",
       "      <td>-14.348653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084793</td>\n",
       "      <td>-0.914717</td>\n",
       "      <td>-1.099021</td>\n",
       "      <td>-1.171458</td>\n",
       "      <td>-0.924914</td>\n",
       "      <td>-1.043308</td>\n",
       "      <td>-0.764414</td>\n",
       "      <td>-1.107698</td>\n",
       "      <td>-1.326581</td>\n",
       "      <td>1.401648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.999061</td>\n",
       "      <td>-7.196817</td>\n",
       "      <td>8.626605</td>\n",
       "      <td>-9.757539</td>\n",
       "      <td>6.917021</td>\n",
       "      <td>-10.372511</td>\n",
       "      <td>-5.322743</td>\n",
       "      <td>-13.013300</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>-8.923686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726711</td>\n",
       "      <td>-0.233750</td>\n",
       "      <td>-0.253383</td>\n",
       "      <td>-0.290629</td>\n",
       "      <td>-0.300965</td>\n",
       "      <td>-0.269029</td>\n",
       "      <td>-0.079615</td>\n",
       "      <td>-0.061865</td>\n",
       "      <td>-0.155170</td>\n",
       "      <td>2.834129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.807904</td>\n",
       "      <td>-6.215917</td>\n",
       "      <td>10.205181</td>\n",
       "      <td>-8.455754</td>\n",
       "      <td>8.195798</td>\n",
       "      <td>-8.812211</td>\n",
       "      <td>-4.535729</td>\n",
       "      <td>-11.154415</td>\n",
       "      <td>0.200554</td>\n",
       "      <td>-7.562764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960777</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>-0.005370</td>\n",
       "      <td>-0.069215</td>\n",
       "      <td>-0.107100</td>\n",
       "      <td>-0.052621</td>\n",
       "      <td>0.096420</td>\n",
       "      <td>0.168851</td>\n",
       "      <td>0.125779</td>\n",
       "      <td>3.344017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.763011</td>\n",
       "      <td>-5.354254</td>\n",
       "      <td>11.860113</td>\n",
       "      <td>-7.109643</td>\n",
       "      <td>9.497823</td>\n",
       "      <td>-7.527036</td>\n",
       "      <td>-3.848781</td>\n",
       "      <td>-9.431325</td>\n",
       "      <td>0.409282</td>\n",
       "      <td>-6.445704</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200804</td>\n",
       "      <td>0.187754</td>\n",
       "      <td>0.240286</td>\n",
       "      <td>0.136887</td>\n",
       "      <td>0.120294</td>\n",
       "      <td>0.216406</td>\n",
       "      <td>0.272310</td>\n",
       "      <td>0.394001</td>\n",
       "      <td>0.376652</td>\n",
       "      <td>3.947572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.611126</td>\n",
       "      <td>-2.239507</td>\n",
       "      <td>18.201218</td>\n",
       "      <td>-2.895845</td>\n",
       "      <td>14.578374</td>\n",
       "      <td>-3.071097</td>\n",
       "      <td>-1.530158</td>\n",
       "      <td>-3.991794</td>\n",
       "      <td>1.102077</td>\n",
       "      <td>-2.510722</td>\n",
       "      <td>...</td>\n",
       "      <td>2.296494</td>\n",
       "      <td>1.050034</td>\n",
       "      <td>1.329247</td>\n",
       "      <td>0.914874</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>1.582470</td>\n",
       "      <td>0.948782</td>\n",
       "      <td>1.246364</td>\n",
       "      <td>1.279843</td>\n",
       "      <td>6.344531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Time           V1           V2           V3           V4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean     12.014839    -6.305023    10.341335    -8.574209     8.299900   \n",
       "std       2.801548     1.402232     2.401234     2.029324     1.945500   \n",
       "min       4.049931   -11.222009     3.486689   -15.068597     2.739768   \n",
       "25%       9.999061    -7.196817     8.626605    -9.757539     6.917021   \n",
       "50%      11.807904    -6.215917    10.205181    -8.455754     8.195798   \n",
       "75%      13.763011    -5.354254    11.860113    -7.109643     9.497823   \n",
       "max      21.611126    -2.239507    18.201218    -2.895845    14.578374   \n",
       "\n",
       "                V5           V6           V7           V8           V9  ...  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  ...   \n",
       "mean     -9.044720    -4.633235   -11.389522     0.204499    -7.789335  ...   \n",
       "std       2.128428     1.141382     2.698648     0.297286     1.878873  ...   \n",
       "min     -16.394028    -8.617829   -20.925287    -1.078363   -14.348653  ...   \n",
       "25%     -10.372511    -5.322743   -13.013300     0.008828    -8.923686  ...   \n",
       "50%      -8.812211    -4.535729   -11.154415     0.200554    -7.562764  ...   \n",
       "75%      -7.527036    -3.848781    -9.431325     0.409282    -6.445704  ...   \n",
       "max      -3.071097    -1.530158    -3.991794     1.102077    -2.510722  ...   \n",
       "\n",
       "               V20          V21          V22          V23          V24  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.975866    -0.014080     0.002197    -0.073361    -0.092199   \n",
       "std       0.359798     0.309235     0.372441     0.339191     0.301301   \n",
       "min      -0.084793    -0.914717    -1.099021    -1.171458    -0.924914   \n",
       "25%       0.726711    -0.233750    -0.253383    -0.290629    -0.300965   \n",
       "50%       0.960777    -0.014733    -0.005370    -0.069215    -0.107100   \n",
       "75%       1.200804     0.187754     0.240286     0.136887     0.120294   \n",
       "max       2.296494     1.050034     1.329247     0.914874     0.953445   \n",
       "\n",
       "               V25          V26          V27          V28       Amount  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     -0.017477     0.106953     0.167389     0.107494     3.427012  \n",
       "std       0.367543     0.265344     0.338120     0.396265     0.842888  \n",
       "min      -1.043308    -0.764414    -1.107698    -1.326581     1.401648  \n",
       "25%      -0.269029    -0.079615    -0.061865    -0.155170     2.834129  \n",
       "50%      -0.052621     0.096420     0.168851     0.125779     3.344017  \n",
       "75%       0.216406     0.272310     0.394001     0.376652     3.947572  \n",
       "max       1.582470     0.948782     1.246364     1.279843     6.344531  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_227057wgan, x_train_gen_227057wgan, y_train_gen_227057wgan = gen_data(wgan.generator, 227057)\n",
    "df_gen_227057wgan = pd.DataFrame(data=gen_1000wgan, index=None, columns=x_train.columns)\n",
    "df_gen_227057wgan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9996137776061234\n",
      "Precision:  0.9318181818181818\n",
      "Recall:  0.8367346938775511\n",
      "F1 score:  0.8817204301075268\n",
      "ROC AUC score:  0.9183145894823884\n"
     ]
    }
   ],
   "source": [
    "y_pred_gen_227057wgan = XGBC_model_predit(x_train_gen_227057wgan, y_train_gen_227057wgan)\n",
    "check_performance(y_test, y_pred_gen_227057wgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(y_test, y_pred_gen_227057wgan, 'WGAN 227057')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
